"""
Batch Post-Filter v7.9 - FUNDAMENTAL FIX: GEO DATABASE PRIORITY
Based on Gemini's recommendations for 187 countries support
"""

import re
import logging
import time
from typing import List, Dict, Set, Tuple, Optional
from collections import Counter

logger = logging.getLogger("BatchPostFilter")


class BatchPostFilter:
    def __init__(self, 
                 all_cities_global: Dict[str, str], 
                 forbidden_geo: Set[str], 
                 districts: Optional[Dict[str, str]] = None,
                 population_threshold: int = 5000):
        self.forbidden_geo = forbidden_geo
        self.districts = districts or {}
        self.population_threshold = population_threshold
        
        self.city_abbreviations = self._get_city_abbreviations()
        self.regions = self._get_regions()
        self.countries = self._get_countries()
        self.manual_small_cities = self._get_manual_small_cities()
        
        self.ignored_words = {
            "–¥–æ–º", "–º–∏—Ä", "–±–æ—Ä", "–Ω–∏–≤–∞", "–±–∞–ª–∫–∞", "–ª—É—á", "—Å–ø—É—Ç–Ω–∏–∫", "—Ä–∞–±–æ—Ç–∞", "—Ü–µ–Ω–∞", "–≤—ã–µ–∑–¥",
        }
        
        # –°–ø–∏—Å–æ–∫ –∫—Ä—É–ø–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤/—Å—Ç–æ–ª–∏—Ü –∫–æ—Ç–æ—Ä—ã–µ –í–°–ï–ì–î–ê –±–ª–æ–∫–∏—Ä—É—é—Ç—Å—è (–Ω–µ –±—Ä–µ–Ω–¥—ã)
        self.forbidden_major_cities = {
            # –†–æ—Å—Å–∏—è
            "–º–æ—Å–∫–≤–∞", "moscow", "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "petersburg", "–ø–∏—Ç–µ—Ä", "spb",
            "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "–∫–∞–∑–∞–Ω—å", "–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥",
            "—á–µ–ª—è–±–∏–Ω—Å–∫", "—Å–∞–º–∞—Ä–∞", "–æ–º—Å–∫", "—Ä–æ—Å—Ç–æ–≤", "—É—Ñ–∞", "–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫",
            # –ë–µ–ª–∞—Ä—É—Å—å (–µ—Å–ª–∏ —Ç–∞—Ä–≥–µ—Ç –Ω–µ BY)
            "–º–∏–Ω—Å–∫", "minsk", "–≥–æ–º–µ–ª—å", "–º–æ–≥–∏–ª–µ–≤", "–≤–∏—Ç–µ–±—Å–∫", "–≥—Ä–æ–¥–Ω–æ", "–±—Ä–µ—Å—Ç",
            # –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω (–µ—Å–ª–∏ —Ç–∞—Ä–≥–µ—Ç –Ω–µ KZ)
            "–∞–ª–º–∞—Ç—ã", "almaty", "–∞—Å—Ç–∞–Ω–∞", "nur-sultan", "—à—ã–º–∫–µ–Ω—Ç",
            # –î—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω—ã
            "–∫–∏–µ–≤", "kiev", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–¥–Ω–µ–ø—Ä", "–ª—å–≤–æ–≤", "lviv", # UA
            "–≤–∞—Ä—à–∞–≤–∞", "warsaw", "–∫—Ä–∞–∫–æ–≤", "krakow",  # PL
            "–±–µ—Ä–ª–∏–Ω", "berlin", "–º—é–Ω—Ö–µ–Ω", "munich",  # DE
            "–ø–∞—Ä–∏–∂", "paris", "–ª–æ–Ω–¥–æ–Ω", "london",  # FR, GB
            "—Ä–∏–º", "rome", "–º–∏–ª–∞–Ω", "milan",  # IT
            "–º–∞–¥—Ä–∏–¥", "madrid", "–±–∞—Ä—Å–µ–ª–æ–Ω–∞", "barcelona",  # ES
        }

        
        base_index = {k.lower().strip(): v for k, v in (all_cities_global or {}).items()}
        geo_index = self._build_filtered_geo_index()
        
        for k, v in geo_index.items():
            if k not in base_index:
                base_index[k] = v
        
        self.all_cities_global = base_index
        
        forced_by_cities = {
            "–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏": "by",
            "baranovichi": "by",
            "–∂–¥–∞–Ω–æ–≤–∏—á–∏": "by",
            "zhdanovichi": "by",
            "–ª–æ—à–∏—Ü–∞": "by",
        }
        
        for name, code in forced_by_cities.items():
            if name not in self.all_cities_global:
                self.all_cities_global[name] = code
        
        # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –≥–æ—Ä–æ–¥–∞ (–ö–†–ò–¢–ò–ß–ù–û: –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ geonamescache –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω)
        forced_ua_cities = {
            "–ª—å–≤–æ–≤": "ua",
            "–ª—å–≤—ñ–≤": "ua", 
            "lviv": "ua",
            "lvov": "ua",
            "lemberg": "ua",
            "–∫–∏–µ–≤": "ua",
            "–∫–∏—ó–≤": "ua",
            "kyiv": "ua",
            "kiev": "ua",
            "—Ö–∞—Ä—å–∫–æ–≤": "ua",
            "—Ö–∞—Ä–∫—ñ–≤": "ua",
            "kharkiv": "ua",
            "–æ–¥–µ—Å—Å–∞": "ua",
            "–æ–¥–µ—Å–∞": "ua",
            "odessa": "ua",
            "–¥–Ω–µ–ø—Ä": "ua",
            "–¥–Ω—ñ–ø—Ä–æ": "ua",
            "dnipro": "ua",
            "–∑–∞–ø–æ—Ä–æ–∂—å–µ": "ua",
            "–∑–∞–ø–æ—Ä—ñ–∂–∂—è": "ua",
            "zaporizhzhia": "ua",
        }
        
        for name, code in forced_ua_cities.items():
            self.all_cities_global[name] = code
        
        try:
            import pymorphy3
            self.morph_ru = pymorphy3.MorphAnalyzer(lang='ru')
            self.morph_uk = pymorphy3.MorphAnalyzer(lang='uk')
            self._has_morph = True
        except ImportError:
            self._has_morph = False
    
    def _get_city_abbreviations(self) -> Dict[str, str]:
        return {
            '–µ–∫–±': 'ru', '–µ–∫–∞—Ç': 'ru', '—Å–ø–±': 'ru', '–ø–∏—Ç–µ—Ä': 'ru', '–º—Å–∫': 'ru',
            '–Ω—Å–∫': 'ru', '–Ω–Ω': 'ru', '–Ω–Ω–æ–≤': 'ru', '–≤–ª–∞–¥': 'ru', '—Ä–æ—Å—Ç–æ–≤': 'ru',
            '–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä': 'ru', '–º–Ω': 'by', '–∞–ª–º–∞—Ç—ã': 'kz', '–∞—Å—Ç–∞–Ω–∞': 'kz', '—Ç–∞—à–∫–µ–Ω—Ç': 'uz',
        }
    
    def _get_regions(self) -> Dict[str, str]:
        return {
            '–∏–Ω–≥—É—à–µ—Ç–∏—è': 'ru', '—á–µ—á–Ω—è': 'ru', '—á–µ—á–µ–Ω—Å–∫–∞—è —Ä–µ—Å–ø—É–±–ª–∏–∫–∞': 'ru',
            '–¥–∞–≥–µ—Å—Ç–∞–Ω': 'ru', '—Ç–∞—Ç–∞—Ä—Å—Ç–∞–Ω': 'ru', '–±–∞—à–∫–æ—Ä—Ç–æ—Å—Ç–∞–Ω': 'ru',
            '—É–¥–º—É—Ä—Ç–∏—è': 'ru', '–º–æ—Ä–¥–æ–≤–∏—è': 'ru', '–º–∞—Ä–∏–π —ç–ª': 'ru',
            '—á—É–≤–∞—à–∏—è': 'ru', '—è–∫—É—Ç–∏—è': 'ru', '—Å–∞—Ö–∞': 'ru', '–±—É—Ä—è—Ç–∏—è': 'ru',
            '—Ç—ã–≤–∞': 'ru', '—Ö–∞–∫–∞—Å–∏—è': 'ru', '–∞–ª—Ç–∞–π': 'ru', '–∫–∞—Ä–µ–ª–∏—è': 'ru',
            '–∫–æ–º–∏': 'ru', '–∫–∞–ª–º—ã–∫–∏—è': 'ru', '–∞–¥—ã–≥–µ—è': 'ru', '–∫–∞–±–∞—Ä–¥–∏–Ω–æ-–±–∞–ª–∫–∞—Ä–∏—è': 'ru',
            '–∫–∞—Ä–∞—á–∞–µ–≤–æ-—á–µ—Ä–∫–µ—Å–∏—è': 'ru', '—Å–µ–≤–µ—Ä–Ω–∞—è –æ—Å–µ—Ç–∏—è': 'ru', '–∫—Ä—ã–º': 'ru',
            '–º–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru', '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru',
            '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru', '—Å–≤–µ—Ä–¥–ª–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru',
            '–º–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by', '–≥–æ–º–µ–ª—å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–º–æ–≥–∏–ª–µ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by', '–≤–∏—Ç–µ–±—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–≥—Ä–æ–¥–Ω–µ–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by', '–±—Ä–µ—Å—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–∞–ª–º–∞—Ç–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'kz', '—é–∂–Ω–æ-–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'kz',
            '—Ç–∞—à–∫–µ–Ω—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'uz', '—Å–∞–º–∞—Ä–∫–∞–Ω–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'uz',
        }
    
    def _get_countries(self) -> Dict[str, str]:
        return {
            '—Ä–æ—Å—Å–∏—è': 'ru', '—Ä–æ—Å—Å–∏–∏': 'ru', '—Ä—Ñ': 'ru',
            '–±–µ–ª–∞—Ä—É—Å—å': 'by', '–±–µ–ª–æ—Ä—É—Å—Å–∏—è': 'by',
            '–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω': 'kz', '–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ': 'kz',
            '—É–∑–±–µ–∫–∏—Å—Ç–∞–Ω': 'uz', '—É–∑–±–µ–∫–∏—Å—Ç–∞–Ω–µ': 'uz',
            '—É–∫—Ä–∞–∏–Ω–∞': 'ua', '—É–∫—Ä–∞–∏–Ω–µ': 'ua',
            '–∏–∑—Ä–∞–∏–ª—å': 'il', '–∏–∑—Ä–∞–∏–ª–µ': 'il',
            '–ø–æ–ª—å—à–∞': 'pl', '–ø–æ–ª—å—à–µ': 'pl',
            '–≥–µ—Ä–º–∞–Ω–∏—è': 'de', '–≥–µ—Ä–º–∞–Ω–∏–∏': 'de',
            '—Å—à–∞': 'us', '–∞–º–µ—Ä–∏–∫–∞': 'us', '–∞–º–µ—Ä–∏–∫–µ': 'us',
        }
    
    def _get_manual_small_cities(self) -> Dict[str, str]:
        return {
            '–æ—à': 'kg',
            '—É–∑—ã–Ω–∞–≥–∞—à': 'kz',
            '—â–µ–ª–∫–∏–Ω–æ': 'ru',
            '—â—ë–ª–∫ino': 'ru',
            '–π–æ—Ç–∞': 'unknown',
        }
    
    def _build_filtered_geo_index(self) -> Dict[str, str]:
        try:
            import geonamescache
            gc = geonamescache.GeonamesCache()
            
            # –ö–†–ò–¢–ò–ß–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ 5000 –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ cities5000.json (65k –≥–æ—Ä–æ–¥–æ–≤)
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è cities15000.json (32k –≥–æ—Ä–æ–¥–æ–≤)
            gc.min_city_population = self.population_threshold  # 5000
            
            cities = gc.get_cities()
            
            filtered_index = {}
            
            for city_id, city_data in cities.items():
                country = city_data['countrycode'].lower()
                population = city_data.get('population', 0)
                
                if population < self.population_threshold:
                    continue
                
                name = city_data['name'].lower().strip()
                filtered_index[name] = country
                
                for alt in city_data.get('alternatenames', []):
                    alt = alt.strip()
                    if not (3 <= len(alt) <= 50):
                        continue
                    if not any(c.isalpha() for c in alt):
                        continue
                    
                    is_latin_cyrillic = all(
                        ('\u0000' <= c <= '\u007F') or
                        ('\u0400' <= c <= '\u04FF') or
                        c in ['-', "'", ' ']
                        for c in alt
                    )
                    if not is_latin_cyrillic:
                        continue
                    
                    alt_lower = alt.lower()
                    
                    has_cyr = any('\u0400' <= c <= '\u04FF' for c in alt_lower)
                    has_lat = any('a' <= c <= 'z' for c in alt_lower)
                    
                    if has_cyr and not has_lat:
                        if alt_lower not in filtered_index:
                            filtered_index[alt_lower] = country
                    
                    if alt_lower not in filtered_index:
                        filtered_index[alt_lower] = country
                    
                    alt_dash = alt_lower.replace(' ', '-')
                    if alt_dash != alt_lower and alt_dash not in filtered_index:
                        filtered_index[alt_dash] = country
            
            return filtered_index
            
        except ImportError:
            return {
                '–º–æ—Å–∫–≤–∞': 'ru', '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥': 'ru', 
                '–∫–∏–µ–≤': 'ua', '—Ö–∞—Ä—å–∫–æ–≤': 'ua', '–æ–¥–µ—Å—Å–∞': 'ua',
                '–º–∏–Ω—Å–∫': 'by', '–∞–ª–º–∞—Ç—ã': 'kz', '—Ç–∞—à–∫–µ–Ω—Ç': 'uz'
            }

    def _find_in_country(self, word: str, target_country: str) -> bool:
        """
        PRIORITY 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ - —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –≥–æ—Ä–æ–¥–æ–º –¶–ï–õ–ï–í–û–ô —Å—Ç—Ä–∞–Ω—ã
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∏–º–µ–Ω–Ω–æ –∫–∞–∫ –≥–æ—Ä–æ–¥ target_country
        """
        word_lower = word.lower()
        
        # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ
        found_country = self.all_cities_global.get(word_lower)
        if found_country and found_country == target_country.lower():
            return True
        
        # –ü–æ–∏—Å–∫ —Å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π (–Ω–∞ —Å–ª—É—á–∞–π –ø–∞–¥–µ–∂–µ–π)
        if self._has_morph:
            lemma_ru = self._get_lemma(word_lower, 'ru')
            lemma_uk = self._get_lemma(word_lower, 'uk')
            
            for lemma in [lemma_ru, lemma_uk]:
                if lemma != word_lower:
                    found_country = self.all_cities_global.get(lemma)
                    if found_country and found_country == target_country.lower():
                        return True
        
        return False
    
    def _is_real_city_not_brand(self, word: str, found_country: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –†–ï–ê–õ–¨–ù–´–ú –≥–æ—Ä–æ–¥–æ–º (–∞ –Ω–µ –±—Ä–µ–Ω–¥–æ–º)
        
        –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê –±–µ–∑ —Ö–∞—Ä–¥–∫–æ–¥ —Å–ø–∏—Å–∫–æ–≤:
        - –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ ‚Üí –ì–û–†–û–î
        - –õ–∞—Ç–∏–Ω–∏—Ü–∞ ‚Üí –≤–æ–∑–º–æ–∂–Ω—ã–π –ë–†–ï–ù–î
        - –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –±—Ä–µ–Ω–¥—ã ‚Üí –ë–†–ï–ù–î
        """
        word_lower = word.lower()
        
        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –±—Ä–µ–Ω–¥—ã –ù–ï —Å—á–∏—Ç–∞—é—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–º–∏ –≥–æ—Ä–æ–¥–∞–º–∏
        known_brands = {
            "—Ä–µ–¥–º–æ–Ω–¥", "redmond", "–≥–æ—Ä–µ–Ω—å–µ", "gorenje", "–±–æ—à", "bosch",
            "—Å–∞–º—Å—É–Ω–≥", "samsung", "—Ñ–∏–ª–∏–ø—Å", "philips", "–±—Ä–∞—É–Ω", "braun",
            "–ø–∞–Ω–∞—Å–æ–Ω–∏–∫", "panasonic", "—Å–∏–º–µ–Ω—Å", "siemens", "–º–∏–ª–µ", "miele",
            "—ç–ª–µ–∫—Ç—Ä–æ–ª—é–∫—Å", "electrolux", "–∞–µ–≥", "aeg", "–∑–∞–Ω—É—Å—Å–∏", "zanussi",
            "–∏–Ω–¥–µ–∑–∏—Ç", "indesit", "–∞—Ä–∏—Å—Ç–æ–Ω", "ariston", "–∫–∞–Ω–¥–∏", "candy",
            "–±–µ–∫–æ", "beko", "—Ö–æ—Ç–ø–æ–∏–Ω—Ç", "hotpoint", "–≤–∏—Ä–ø—É–ª", "whirlpool",
            "–¥–∞–π—Å–æ–Ω", "dyson", "–∫–µ—Ä—Ö–µ—Ä", "karcher", "–≤–∏—Ç–µ–∫", "vitek",
            "–ø–æ–ª–∞—Ä–∏—Å", "polaris", "—Å–∫–∞—Ä–ª–µ—Ç", "scarlett", "—Ç–µ—Ñ–∞–ª—å", "tefal",
            "–º—É–ª–∏–Ω–µ–∫—Å", "moulinex", "–∫—Ä—É–ø—Å", "krups", "–¥–µ–ª–æ–Ω–≥–∏", "delonghi",
            "—Ñ–∏–ª–∫–æ", "philco", "—Ç–æ–º–∞—Å", "thomas", "–∑–µ–ª–º–µ—Ä", "zelmer",
        }
        
        if word_lower in known_brands:
            return False
        
        # –õ–∞—Ç–∏–Ω—Å–∫–∏–µ —Å–ª–æ–≤–∞ —Å–∫–æ—Ä–µ–µ –±—Ä–µ–Ω–¥—ã, —á–µ–º –≥–æ—Ä–æ–¥–∞
        if word_lower.isascii() and word_lower.isalpha():
            # –ö–æ—Ä–æ—Ç–∫–∏–µ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ —Å–ª–æ–≤–∞ (‚â§4) - —Ç–æ—á–Ω–æ –±—Ä–µ–Ω–¥—ã
            if len(word_lower) <= 4:
                return False
        
        # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (1-2 –±—É–∫–≤—ã) - —Å–∫–æ—Ä–µ–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã/–±—Ä–µ–Ω–¥—ã
        if len(word_lower) <= 2:
            return False
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ - —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ì–û–†–û–î
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º - —ç—Ç–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞?
        if not word_lower.isascii():
            # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ 3+ –±—É–∫–≤ - —ç—Ç–æ –ì–û–†–û–î
            # –ü—Ä–∏–º–µ—Ä—ã: —É—Ñ–∞(3), –æ–º—Å–∫(4), —Ä–∏–≥–∞(4), —Ç—É–ª–∞(4), –µ–π—Å–∫(4), –∫—É—Ä—Å–∫(5)
            if len(word_lower) >= 3:
                return True
        
        # –õ–∞—Ç–∏–Ω—Å–∫–∏–µ –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (5+) –º–æ–≥—É—Ç –±—ã—Ç—å –≥–æ—Ä–æ–¥–∞–º–∏
        # –ü—Ä–∏–º–µ—Ä—ã: Paris, London, Berlin
        if len(word_lower) >= 5:
            return True
        
        # –í –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö - –≤–æ–∑–º–æ–∂–Ω—ã–π –±—Ä–µ–Ω–¥
        return False

    def _is_brand_like(self, word: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ —Å–ª–æ–≤–æ –±—ã—Ç—å –±—Ä–µ–Ω–¥–æ–º (—Å–ø–æ—Ä–Ω–æ–µ —Å–ª–æ–≤–æ)"""
        word_lower = word.lower()
        
        # –°–ª–æ–≤–∞ –≤ ignored_words —Å—á–∏—Ç–∞—é—Ç—Å—è –Ω–µ-–≥–æ—Ä–æ–¥–∞–º–∏
        if word_lower in self.ignored_words:
            return True
        
        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –±—Ä–µ–Ω–¥—ã —Ç–µ—Ö–Ω–∏–∫–∏ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏ –ª–∞—Ç–∏–Ω–∏—Ü–∞)
        known_brands = {
            # –ë—Ä–µ–Ω–¥—ã —Ç–µ—Ö–Ω–∏–∫–∏ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
            "—Ä–µ–¥–º–æ–Ω–¥", "redmond", "–≥–æ—Ä–µ–Ω—å–µ", "gorenje", "–±–æ—à", "bosch",
            "—Å–∞–º—Å—É–Ω–≥", "samsung", "—Ñ–∏–ª–∏–ø—Å", "philips", "–±—Ä–∞—É–Ω", "braun",
            "–ø–∞–Ω–∞—Å–æ–Ω–∏–∫", "panasonic", "—Å–∏–º–µ–Ω—Å", "siemens", "–º–∏–ª–µ", "miele",
            "—ç–ª–µ–∫—Ç—Ä–æ–ª—é–∫—Å", "electrolux", "–∞–µ–≥", "aeg", "–∑–∞–Ω—É—Å—Å–∏", "zanussi",
            "–∏–Ω–¥–µ–∑–∏—Ç", "indesit", "–∞—Ä–∏—Å—Ç–æ–Ω", "ariston", "–∫–∞–Ω–¥–∏", "candy",
            "–±–µ–∫–æ", "beko", "—Ö–æ—Ç–ø–æ–∏–Ω—Ç", "hotpoint", "–≤–∏—Ä–ø—É–ª", "whirlpool",
            "–¥–∞–π—Å–æ–Ω", "dyson", "–∫–µ—Ä—Ö–µ—Ä", "karcher", "–≤–∏—Ç–µ–∫", "vitek",
            "–ø–æ–ª–∞—Ä–∏—Å", "polaris", "—Å–∫–∞—Ä–ª–µ—Ç", "scarlett", "—Ç–µ—Ñ–∞–ª—å", "tefal",
            "–º—É–ª–∏–Ω–µ–∫—Å", "moulinex", "–∫—Ä—É–ø—Å", "krups", "–¥–µ–ª–æ–Ω–≥–∏", "delonghi",
            "—Ñ–∏–ª–∫–æ", "philco", "—Ç–æ–º–∞—Å", "thomas", "–∑–µ–ª–º–µ—Ä", "zelmer",
            # –î–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        }
        
        if word_lower in known_brands:
            return True
        
        # –õ–∞—Ç–∏–Ω—Å–∫–∏–µ —Å–ª–æ–≤–∞ —Å–∫–æ—Ä–µ–µ –±—Ä–µ–Ω–¥—ã —á–µ–º –≥–æ—Ä–æ–¥–∞
        if word.isascii() and word.isalpha():
            return True
        
        # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (3-4 –±—É–∫–≤—ã) –º–æ–≥—É—Ç –±—ã—Ç—å –±—Ä–µ–Ω–¥–∞–º–∏
        if len(word) <= 4:
            return True
        
        return False

    def _has_seed_cores(self, keyword: str, seed: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–æ—Ä–Ω–µ–π –∏–∑ —Å–∏–¥–∞ –≤ –∫–ª—é—á–µ (–ø–µ—Ä–≤—ã–µ 5 –±—É–∫–≤)"""
        seed_roots = [w.lower()[:5] for w in re.findall(r'[–∞-—è—ëa-z]+', seed) if len(w) > 3]
        keyword_lower = keyword.lower()
        return any(root in keyword_lower for root in seed_roots)

    def filter_batch(self, keywords: List[str], seed: str, country: str, 
                     language: str = 'ru') -> Dict:
        start_time = time.time()
        
        logger.info(f"[BPF] START filter_batch | country={country} | lang={language}")
        logger.info(f"[BPF] RAW keywords ({len(keywords)}): {keywords}")
        
        unique_raw = sorted(list(set([k.lower().strip() for k in keywords if k.strip()])))
        logger.info(f"[BPF] UNIQUE_RAW ({len(unique_raw)}): {unique_raw}")
        
        seed_cities = self._extract_cities_from_seed(seed, country, language)
        logger.info(f"[BPF] SEED='{seed}' | seed_cities={seed_cities}")
        
        all_words = set()
        for kw in unique_raw:
            all_words.update(re.findall(r'[–∞-—è—ëa-z0-9-]+', kw))
        
        lemmas_map = self._batch_lemmatize(all_words, language)
        
        final_keywords = []
        final_anchors = []
        stats = {
            'total': len(unique_raw),
            'allowed': 0,
            'blocked': 0,
            'reasons': Counter()
        }

        for kw in unique_raw:
            is_allowed, reason, category = self._check_geo_conflicts_v75(
                kw, country, lemmas_map, seed_cities, language, seed
            )
            
            if is_allowed:
                final_keywords.append(kw)
                stats['allowed'] += 1
            else:
                final_anchors.append(kw)
                stats['blocked'] += 1
                stats['reasons'][category] += 1

        elapsed = time.time() - start_time
        logger.info(f"[BPF] FINISH {elapsed:.2f}s | "
                    f"allowed={len(final_keywords)} | anchors={len(final_anchors)} | "
                    f"reasons={dict(stats['reasons'])}")

        return {
            'keywords': final_keywords,
            'anchors': final_anchors,
            'stats': {
                'total': stats['total'],
                'allowed': stats['allowed'],
                'blocked': stats['blocked'],
                'reasons': dict(stats['reasons']),
                'elapsed_time': round(elapsed, 2)
            }
        }

    def _check_geo_conflicts_v75(self, keyword: str, country: str, 
                                  lemmas_map: Dict[str, str], seed_cities: Set[str],
                                  language: str, seed: str = "") -> Tuple[bool, str, str]:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ—Ä–Ω–µ–π —Å–∏–¥–∞ –≤ –∫–ª—é—á–µ (–ü–†–ò–û–†–ò–¢–ï–¢ –°–ò–î–ê)
        has_seed = self._has_seed_cores(keyword, seed) if seed else False
        
        logger.debug(f"[BPF] CHECK keyword='{keyword}' | country={country} | "
                     f"has_seed={has_seed} | seed_cities={seed_cities}")
        
        words = re.findall(r'[–∞-—è—ëa-z0-9-]+', keyword)
        if not words:
            return True, "", ""

        keyword_lemmas = [lemmas_map.get(w, w) for w in words]
        
        words_set = set(words + keyword_lemmas)
        if any(city in words_set for city in seed_cities):
            logger.debug(f"[BPF] ALLOW by seed_cities | keyword='{keyword}'")
            return True, "", ""
        
        for check_val in words + keyword_lemmas:
            if check_val in self.forbidden_geo:
                return False, f"Hard-Blacklist '{check_val}'", "hard_blacklist"

        for w in words:
            if w in self.districts:
                dist_country = self.districts[w]
                if dist_country != country.lower():
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å—Ç—å –ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ –≥–æ—Ä–æ–¥ –¶–ï–õ–ï–í–û–ô —Å—Ç—Ä–∞–Ω—ã?
                    # "—Ö–∞—Ä—å–∫–æ–≤ –∞–ª–µ–∫—Å–µ–µ–≤–∫–∞" ‚Üí "—Ö–∞—Ä—å–∫–æ–≤" = UA ‚Üí –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º "–∞–ª–µ–∫—Å–µ–µ–≤–∫–∞" (RU)
                    has_target_city = any(
                        self.all_cities_global.get(other_w) == country.lower()
                        for other_w in set(words + keyword_lemmas) - {w}
                    )
                    if has_target_city:
                        logger.debug(f"[BPF] ALLOW district '{w}' ({dist_country}) ‚Äî "
                                     f"keyword has target city ({country})")
                        continue
                    return False, f"—Ä–∞–π–æ–Ω '{w}' ({dist_country})", "districts"
        
        for w in words + keyword_lemmas:
            if w in self.city_abbreviations:
                abbr_country = self.city_abbreviations[w]
                if abbr_country != country.lower():
                    return False, f"—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ '{w}' ({abbr_country})", f"{abbr_country}_abbreviations"
        
        check_regions = words + keyword_lemmas + self._extract_ngrams(words, 2)
        for item in check_regions:
            if item in self.regions:
                region_country = self.regions[item]
                if region_country != country.lower():
                    return False, f"—Ä–µ–≥–∏–æ–Ω '{item}' ({region_country})", f"{region_country}_regions"
        
        for w in words + keyword_lemmas:
            if w in self.countries:
                ctry_code = self.countries[w]
                if ctry_code != country.lower():
                    return False, f"—Å—Ç—Ä–∞–Ω–∞ '{w}' ({ctry_code})", f"{ctry_code}_countries"
        
        for w in words + keyword_lemmas:
            if w in self.manual_small_cities:
                city_country = self.manual_small_cities[w]
                if city_country == 'unknown':
                    return False, f"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ–±—ä–µ–∫—Ç '{w}'", "unknown"
                if city_country != country.lower():
                    return False, f"–º–∞–ª—ã–π –≥–æ—Ä–æ–¥ '{w}' ({city_country})", f"{city_country}_small_cities"

        search_items = []
        search_items.extend(words)
        search_items.extend(keyword_lemmas)
        
        bigrams = self._extract_ngrams(words, 2)
        search_items.extend(bigrams)
        search_items.extend([bg.replace(' ', '-') for bg in bigrams])
        
        lemma_bigrams = self._extract_ngrams(keyword_lemmas, 2)
        search_items.extend(lemma_bigrams)
        search_items.extend([bg.replace(' ', '-') for bg in lemma_bigrams])
        
        trigrams = self._extract_ngrams(words, 3)
        search_items.extend(trigrams)
        search_items.extend([tg.replace(' ', '-') for tg in trigrams])

        # FIX: –°–æ–±–∏—Ä–∞–µ–º –ª–µ–º–º—ã —Å–ª–æ–≤ –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –≥–æ—Ä–æ–¥–∞–º–∏ –ù–ê–®–ï–ô —Å—Ç—Ä–∞–Ω—ã
        # "–ª—å–≤–æ–≤" ‚Üí UA ‚Üí –ª–µ–º–º–∞ "–ª–µ–≤" ‚Üí –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å "–ª–µ–≤" –∫–∞–∫ –≥–æ—Ä–æ–¥ BF
        our_city_lemmas = set()
        for w in words:
            if self._find_in_country(w, country):
                lemma = self._get_lemma(w, language)
                if lemma != w:
                    our_city_lemmas.add(lemma)
                    logger.debug(f"[BPF] our_city_lemma: '{w}' ‚Üí '{lemma}'")

        for item in search_items:
            # –®–ê–ì 0: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ ignored_words
            if len(item) < 3 or item in self.ignored_words:
                if item in self.ignored_words:
                    logger.info(f"[GEO_SKIP] –°–ª–æ–≤–æ '{item}' –≤ ignored_words")
                continue
            
            item_normalized = self._get_lemma(item, language)
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # –ñ–ï–°–¢–ö–ê–Ø –ò–ï–†–ê–†–•–ò–Ø –ü–†–ò–û–†–ò–¢–ï–¢–û–í (v11)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
            # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            # ‚îÇ PRIORITY 1: –°–í–û–ô –ì–û–†–û–î (—Ü–µ–ª–µ–≤–∞—è —Å—Ç—Ä–∞–Ω–∞)                 ‚îÇ
            # ‚îÇ –ü—Ä–æ–≤–µ—Ä—è–µ–º –ü–ï–†–í–´–ú –¥–µ–ª–æ–º - —ç—Ç–æ –Ω–∞—à –≥–æ—Ä–æ–¥?                 ‚îÇ
            # ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            
            is_our_city = self._find_in_country(item, country)
            if not is_our_city:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–∫–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É
                is_our_city = self._find_in_country(item_normalized, country)
            
            if is_our_city:
                logger.info(f"[GEO_ALLOW] ‚úì PRIORITY 1: –ì–æ—Ä–æ–¥ '{item}' –Ω–∞–π–¥–µ–Ω –≤ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–µ {country.upper()}")
                continue  # –õ—å–≤–æ–≤ —Å–ø–∞—Å–µ–Ω!
            
            # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            # ‚îÇ PRIORITY 2: –ß–£–ñ–û–ô –ì–û–†–û–î (–¥—Ä—É–≥–∞—è —Å—Ç—Ä–∞–Ω–∞)                 ‚îÇ
            # ‚îÇ –ï—Å–ª–∏ –≥–æ—Ä–æ–¥ –Ω–∞–π–¥–µ–Ω –≤ –¥—Ä—É–≥–æ–π —Å—Ç—Ä–∞–Ω–µ - –±–ª–æ–∫–∏—Ä—É–µ–º           ‚îÇ
            # ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            
            found_country = self.all_cities_global.get(item_normalized) or self.all_cities_global.get(item)
            
            if found_country:
                logger.info(f"[GEO_DEBUG] –°–ª–æ–≤–æ '{item}' –æ–ø–æ–∑–Ω–∞–Ω–æ –∫–∞–∫ –≥–æ—Ä–æ–¥ —Å—Ç—Ä–∞–Ω—ã: {found_country.upper()}")
                
                # –≠—Ç–æ –≥–æ—Ä–æ–¥ –î–†–£–ì–û–ô —Å—Ç—Ä–∞–Ω—ã (–º—ã —É–∂–µ –ø—Ä–æ–≤–µ—Ä–∏–ª–∏ –Ω–∞—à—É –≤—ã—à–µ)
                if found_country != country.lower():
                    
                    # FIX: –≠—Ç–æ –ª–µ–º–º–∞ –Ω–∞—à–µ–≥–æ –≥–æ—Ä–æ–¥–∞? "–ª–µ–≤" ‚Üê "–ª—å–≤–æ–≤" (UA)
                    if item in our_city_lemmas or item_normalized in our_city_lemmas:
                        logger.info(f"[GEO_ALLOW] ‚úì –°–ª–æ–≤–æ '{item}' ‚Äî –ª–µ–º–º–∞ –≥–æ—Ä–æ–¥–∞ –Ω–∞—à–µ–π —Å—Ç—Ä–∞–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                        continue
                    
                    # FIX: –≠—Ç–æ –æ–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ —è–∑—ã–∫–∞? "–¥–æ–º", "–±–µ–ª–∞—è", "–≥–æ—Ä–∞"
                    if self._is_common_noun(item_normalized, language) or self._is_common_noun(item, language):
                        logger.info(f"[GEO_ALLOW] ‚úì –°–ª–æ–≤–æ '{item}' ‚Äî –æ–±—ã—á–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ, –Ω–µ –≥–æ—Ä–æ–¥")
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ seed_cities (–≥–æ—Ä–æ–¥–∞ –∏–∑ —Å–∏–¥–∞ –≤—Å–µ–≥–¥–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã)
                    if item_normalized in seed_cities or item in seed_cities:
                        logger.info(f"[GEO_ALLOW] –ì–æ—Ä–æ–¥ '{item}' —Ä–∞–∑—Ä–µ—à–µ–Ω (–µ—Å—Ç—å –≤ —Å–∏–¥–µ)")
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º - —ç—Ç–æ –†–ï–ê–õ–¨–ù–´–ô –≥–æ—Ä–æ–¥ –∏–ª–∏ –≤–æ–∑–º–æ–∂–Ω—ã–π –±—Ä–µ–Ω–¥?
                    is_real_city = self._is_real_city_not_brand(item, found_country)
                    
                    if is_real_city:
                        # –≠—Ç–æ —è–≤–Ω–æ –†–ï–ê–õ–¨–ù–´–ô –≥–æ—Ä–æ–¥ (–†–∏–≥–∞, –ï–π—Å–∫, –ò—à–∏–º)
                        # –ë–õ–û–ö–ò–†–£–ï–ú –¥–∞–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å seed!
                        reason = f"–°–ª–æ–≤–æ '{item}' ‚Äî —ç—Ç–æ –≥–æ—Ä–æ–¥ –≤ {found_country.upper()}, –∞ –º—ã –ø–∞—Ä—Å–∏–º {country.upper()}"
                        logger.warning(f"!!! [GEO_ANCHOR] ‚úó PRIORITY 2: –ö–ª—é—á –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ —è–∫–æ—Ä—è: '{keyword}' | –ü—Ä–∏—á–∏–Ω–∞: {reason} (—Ä–µ–∞–ª—å–Ω—ã–π —á—É–∂–æ–π –≥–æ—Ä–æ–¥)")
                        return False, reason, f"{found_country}_cities"
                    
                    # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    # ‚îÇ PRIORITY 3: –°–ü–û–†–ù–û–ï –°–õ–û–í–û (–≤–æ–∑–º–æ–∂–Ω—ã–π –±—Ä–µ–Ω–¥)         ‚îÇ
                    # ‚îÇ –ö–æ—Ä–æ—Ç–∫–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –ª–∞—Ç–∏–Ω–∏—Ü–∞ - –º–æ–∂–µ—Ç –±—ã—Ç—å –±—Ä–µ–Ω–¥–æ–º   ‚îÇ
                    # ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    
                    if has_seed:
                        # –°–ª–æ–≤–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –±—Ä–µ–Ω–¥ –ò –µ—Å—Ç—å seed - —Ä–∞–∑—Ä–µ—à–∞–µ–º
                        logger.info(f"[GEO_ALLOW] ‚úì PRIORITY 3: –ì–æ—Ä–æ–¥ '{item}' ({found_country.upper()}) —Ä–∞–∑—Ä–µ—à–µ–Ω (—Å–ø–æ—Ä–Ω–æ–µ —Å–ª–æ–≤–æ + –µ—Å—Ç—å seed)")
                        continue
                    else:
                        # –ù–µ—Ç seed - –±–ª–æ–∫–∏—Ä—É–µ–º –¥–∞–∂–µ —Å–ø–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞
                        reason = f"–°–ª–æ–≤–æ '{item}' ‚Äî —ç—Ç–æ –≥–æ—Ä–æ–¥ –≤ {found_country.upper()}, –∞ –º—ã –ø–∞—Ä—Å–∏–º {country.upper()}"
                        logger.warning(f"!!! [GEO_ANCHOR] –ö–ª—é—á –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ —è–∫–æ—Ä—è: '{keyword}' | –ü—Ä–∏—á–∏–Ω–∞: {reason}")
                        return False, reason, f"{found_country}_cities"
            
            # –ï—Å–ª–∏ –≥–æ—Ä–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏–≥–¥–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ–±—ã—á–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
            if self._is_common_noun(item_normalized, language):
                continue
        
        if not self._is_grammatically_valid(keyword, language):
            return False, "–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞", "grammar"
        
        return True, "", ""

    def _is_common_noun(self, word: str, language: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –æ–±—ã—á–Ω—ã–º —Å–ª–æ–≤–æ–º —è–∑—ã–∫–∞ (–Ω–µ –≥–µ–æ-–Ω–∞–∑–≤–∞–Ω–∏–µ–º).
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ü–ï–†–í–´–ô (—Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π) –≤–∞—Ä–∏–∞–Ω—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞.
        """
        if not self._has_morph or language not in ['ru', 'uk']:
            return False
        
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        
        try:
            parsed = morph.parse(word)
            if not parsed:
                return False
            
            first = parsed[0]
            tag_str = str(first.tag)
            
            # –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç ‚Äî –≥–µ–æ-–Ω–∞–∑–≤–∞–Ω–∏–µ, —ç—Ç–æ –ù–ï –æ–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ
            if 'Geox' in tag_str:
                return False
            
            # –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç ‚Äî NOUN –∏–ª–∏ ADJF –±–µ–∑ Geox ‚Üí –æ–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ
            if ('NOUN' in tag_str or 'ADJF' in tag_str) and word.islower():
                return True
        except:
            pass
        
        return False

    def _extract_cities_from_seed(self, seed: str, country: str, language: str) -> Set[str]:
        """üî• FIX: –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ—Ä–æ–¥–∞ –∏–∑ seed –ë–ï–ó —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —Å—Ç—Ä–∞–Ω–µ"""
        if not self._has_morph:
            return set()
        
        seed_cities = set()
        words = re.findall(r'[–∞-—è—ëa-z0-9-]+', seed.lower())
        
        for word in words:
            # –ë–ï–ó –ü–†–û–í–ï–†–ö–ò country!
            if word in self.all_cities_global:
                logger.debug(f"[BPF] seed_city WORD '{word}' -> {self.all_cities_global[word]}")
                seed_cities.add(word)
            
            lemma = self._get_lemma(word, language)
            if lemma in self.all_cities_global:
                logger.debug(f"[BPF] seed_city LEMMA '{lemma}' <- '{word}' "
                             f"-> {self.all_cities_global[lemma]}")
                seed_cities.add(lemma)
        
        bigrams = self._extract_ngrams(words, 2)
        for bigram in bigrams:
            if bigram in self.all_cities_global:
                logger.debug(f"[BPF] seed_city BIGRAM '{bigram}' -> {self.all_cities_global[bigram]}")
                seed_cities.add(bigram)
        
        return seed_cities

    def _batch_lemmatize(self, words: Set[str], language: str) -> Dict[str, str]:
        if not self._has_morph:
            return {w: w for w in words}
        
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        lemmas = {}
        
        for word in words:
            lemma = self._get_lemma(word, language, morph)
            lemmas[word] = lemma
        
        return lemmas

    def _get_lemma(self, word: str, language: str, morph=None) -> str:
        if not self._has_morph:
            return word
        
        if morph is None:
            morph = self.morph_ru if language == 'ru' else self.morph_uk
        
        try:
            parsed = morph.parse(word)
            if parsed:
                return parsed[0].normal_form
        except:
            pass
        
        return word

    def _extract_ngrams(self, words: List[str], n: int = 2) -> List[str]:
        if len(words) < n:
            return []
        return [" ".join(words[i:i+n]) for i in range(len(words) - n + 1)]

    def _is_grammatically_valid(self, keyword: str, language: str) -> bool:
        return True  # –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ–ª–Ω–∞—è –∞–º–Ω–∏—Å—Ç–∏—è, —á—Ç–æ–±—ã —Å–ø–∞—Å—Ç–∏ –∫–ª—é—á–∏


# ============================================
# DISTRICTS
# ============================================

DISTRICTS_MINSK = {
    "—É—Ä—É—á—å–µ": "by",
    "—à–∞–±–∞–Ω—ã": "by",
    "–∫–∞–º–µ–Ω–Ω–∞—è –≥–æ—Ä–∫–∞": "by",
    "—Å–µ—Ä–µ–±—Ä—è–Ω–∫–∞": "by"
}

DISTRICTS_TASHKENT = {
    "—á–∏–ª–∞–Ω–∑–∞—Ä": "uz",
    "—é–Ω—É—Å–∞–±–∞–¥": "uz",
    "—Å–µ—Ä–≥–µ–ª–∏": "uz",
    "—è–∫–∫–∞—Å–∞—Ä–∞–π": "uz"
}

DISTRICTS_EXTENDED = {**DISTRICTS_MINSK, **DISTRICTS_TASHKENT}
