"""
TailFunctionClassifier v2 ‚Äî –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä.

–ò–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ v1:
1. –ü—É—Å—Ç–æ–π —Ö–≤–æ—Å—Ç = VALID (–∑–∞–ø—Ä–æ—Å = seed), –Ω–µ TRASH
2. –ê—Ä–±–∏—Ç—Ä–∞–∂ —Å –í–ï–°–ê–ú–ò —Å–∏–≥–Ω–∞–ª–æ–≤ (–≥–µ–æ/–±—Ä–µ–Ω–¥ > —ç–≤—Ä–∏—Å—Ç–∏–∫)
3. –î–æ–±–∞–≤–ª–µ–Ω detect_noise_suffix (12-–π –¥–µ—Ç–µ–∫—Ç–æ—Ä)
4. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π —Å–∏–≥–Ω–∞–ª –∏–∑ –ë–î –ø–µ—Ä–µ–≤–µ—à–∏–≤–∞–µ—Ç —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–µ–≥–∞—Ç–∏–≤
"""

from typing import Dict, List, Tuple, Set
import pymorphy3
from .function_detectors import (
    detect_geo, detect_brand, detect_commerce, detect_reputation,
    detect_location, detect_action, detect_time,
    detect_fragment, detect_meta,
    detect_dangling, detect_duplicate_words, detect_brand_collision,
    detect_noise_suffix, detect_type_specifier,
    detect_seed_echo, detect_broken_grammar,
    detect_number_hijack, detect_short_garbage,
    # –ù–æ–≤—ã–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã
    detect_contacts, detect_marketplace, detect_trash_marketplace,
    detect_technical_garbage, detect_mixed_alphabet, detect_standalone_number,
    detect_verb_modifier, detect_conjunctive_extension,
)

morph = pymorphy3.MorphAnalyzer()


# –í–µ—Å–∞ —Å–∏–≥–Ω–∞–ª–æ–≤: —á–µ–º –≤—ã—à–µ, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ
SIGNAL_WEIGHTS = {
    # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ ‚Äî –æ–ø–∏—Ä–∞—é—Ç—Å—è –Ω–∞ –ë–ê–ó–´ –î–ê–ù–ù–´–• (–≤—ã—Å–æ–∫–∞—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å)
    'geo':        1.0,    # –≥–æ—Ä–æ–¥ –∏–∑ 65k –±–∞–∑—ã ‚Äî –ø–æ—á—Ç–∏ –≥–∞—Ä–∞–Ω—Ç–∏—è
    'brand':      1.0,    # –±—Ä–µ–Ω–¥ –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–π –±–∞–∑—ã
    
    # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ ‚Äî –æ–ø–∏—Ä–∞—é—Ç—Å—è –Ω–∞ –ü–ê–¢–¢–ï–†–ù–´ (—Å—Ä–µ–¥–Ω—è—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å)
    'commerce':   0.8,
    'reputation': 0.8,
    'location':   0.9,    # "—Ä—è–¥–æ–º" ‚Äî —Ç–∏–ø–∏—á–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
    'action':     0.7,
    'time':       0.8,    # "–∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ", "—Å—Ä–æ—á–Ω–æ" ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª
    'type_spec':  0.85,   # —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Å seed ‚Äî –Ω–∞–¥—ë–∂–Ω—ã–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∏–≥–Ω–∞–ª
    'contacts':   0.85,   # "—Ç–µ–ª–µ—Ñ–æ–Ω", "–∞–¥—Ä–µ—Å" ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏–Ω—Ç–µ–Ω—Ç
    'marketplace_valid': 0.9,  # –ø–ª–æ—â–∞–¥–∫–∞ UA ‚Äî —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª
    'verb_modifier': 0.85,  # –Ω–∞—Ä–µ—á–∏–µ –ø—Ä–∏ –≥–ª–∞–≥–æ–ª–µ seed ‚Äî –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–¥—ë–∂–Ω—ã–π
    'conjunctive': 0.8,    # "–∏ –ø–æ–¥–∞—Ä–∫–æ–≤" ‚Äî —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
    
    # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ ‚Äî –≠–í–†–ò–°–¢–ò–ö–ò (–º–æ–≥—É—Ç –æ—à–∏–±–∞—Ç—å—Å—è)
    'fragment':        0.8,
    'meta':            0.9,    # –º–µ—Ç–∞-–≤–æ–ø—Ä–æ—Å—ã –¥–æ–≤–æ–ª—å–Ω–æ –Ω–∞–¥—ë–∂–Ω–æ –ª–æ–≤—è—Ç—Å—è
    'dangling':        0.6,    # –º–æ–∂–µ—Ç –æ—à–∏–±–∞—Ç—å—Å—è (pymorphy –Ω–µ –∏–¥–µ–∞–ª–µ–Ω)
    'duplicate':       0.9,    # –¥—É–±–ª–∏–∫–∞—Ç –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –º—É—Å–æ—Ä
    'brand_collision': 0.5,    # —Å–ø–æ—Ä–Ω—ã–π —Å–∏–≥–Ω–∞–ª, –Ω–∏–∑–∫–∏–π –≤–µ—Å
    'noise_suffix':    0.7,
    'seed_echo':       0.9,    # –ø–æ–≤—Ç–æ—Ä —Å–ª–æ–≤–∞ –∏–∑ seed ‚Äî –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –º—É—Å–æ—Ä
    'broken_grammar':  0.8,    # —Å–ª–æ–º–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–≥–∞
    'number_hijack':   0.85,   # –≥–µ–Ω–∏—Ç–∏–≤-–ø–∞—Ä–∞–∑–∏—Ç –Ω–∞ —á–∏—Å–ª–µ –∏–∑ seed
    'short_garbage':   0.9,    # –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ 1-2 —Å–∏–º–≤–æ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    'marketplace_trash': 0.95, # –ø–ª–æ—â–∞–¥–∫–∞ –†–§/–†–ë ‚Äî –æ—á–µ–Ω—å –Ω–∞–¥—ë–∂–Ω—ã–π —Å–∏–≥–Ω–∞–ª
    'tech_garbage':    0.95,   # email/URL/—Ç–µ–ª–µ—Ñ–æ–Ω ‚Äî –ø–æ—á—Ç–∏ 100% –º—É—Å–æ—Ä
    'mixed_alpha':     0.9,    # —Å–º–µ—à–∞–Ω–Ω—ã–µ –∞–ª—Ñ–∞–≤–∏—Ç—ã
    'standalone_num':  0.7,    # –≥–æ–ª–æ–µ —á–∏—Å–ª–æ ‚Äî –º–æ–∂–µ—Ç –æ—à–∏–±–∏—Ç—å—Å—è (–º–æ–¥–µ–ª–∏)
    'incoherent_tail': 0.85,   # –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–π —Ö–≤–æ—Å—Ç —Å "—á—É–∂–∏–º–∏" —Å–ª–æ–≤–∞–º–∏
}


class TailFunctionClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ö–≤–æ—Å—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π."""
    
    def __init__(self, geo_db: Set[str], brand_db: Set[str], seed: str = "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", target_country: str = "ua"):
        self.geo_db = geo_db
        self.brand_db = brand_db
        self.seed = seed
        self.target_country = target_country
    
    def classify(self, tail: str) -> Dict:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ö–≤–æ—Å—Ç –∑–∞–ø—Ä–æ—Å–∞.
        
        Returns:
            {
                'label': 'VALID' | 'TRASH' | 'GREY',
                'positive_signals': [...],
                'negative_signals': [...],
                'reasons': [...],
                'confidence': float,
                'positive_score': float,
                'negative_score': float,
            }
        """
        # ===== –ü–£–°–¢–û–ô –•–í–û–°–¢ = –∑–∞–ø—Ä–æ—Å —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å seed ‚Üí VALID =====
        if not tail or not tail.strip():
            return {
                'label': 'VALID',
                'positive_signals': ['exact_seed'],
                'negative_signals': [],
                'reasons': ['–ó–∞–ø—Ä–æ—Å —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å seed ‚Äî –≤–∞–ª–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å'],
                'confidence': 0.95,
                'positive_score': 1.0,
                'negative_score': 0.0,
            }
        
        positive_signals = []
        negative_signals = []
        reasons = []
        
        # ===== –ü–û–ó–ò–¢–ò–í–ù–´–ï –î–ï–¢–ï–ö–¢–û–†–´ =====
        detectors_positive = [
            ('geo',        lambda: detect_geo(tail, self.geo_db)),
            ('brand',      lambda: detect_brand(tail, self.brand_db)),
            ('commerce',   lambda: detect_commerce(tail)),
            ('reputation', lambda: detect_reputation(tail)),
            ('location',   lambda: detect_location(tail)),
            ('action',     lambda: detect_action(tail)),
            ('time',       lambda: detect_time(tail)),
            ('type_spec',  lambda: detect_type_specifier(tail, self.seed)),
            ('contacts',   lambda: detect_contacts(tail)),
            ('marketplace_valid', lambda: detect_marketplace(tail, self.target_country)),
            ('verb_modifier', lambda: detect_verb_modifier(tail, self.seed)),
            ('conjunctive', lambda: detect_conjunctive_extension(tail, self.seed)),
        ]
        
        for signal_name, detector in detectors_positive:
            detected, reason = detector()
            if detected:
                positive_signals.append(signal_name)
                reasons.append(f"‚úÖ {reason}")
        
        # ===== –ù–ï–ì–ê–¢–ò–í–ù–´–ï –î–ï–¢–ï–ö–¢–û–†–´ =====
        detectors_negative = [
            ('fragment',        lambda: detect_fragment(tail)),
            ('meta',            lambda: detect_meta(tail)),
            ('dangling',        lambda: detect_dangling(tail, self.seed, self.geo_db)),
            ('duplicate',       lambda: detect_duplicate_words(tail)),
            ('brand_collision', lambda: detect_brand_collision(tail, self.brand_db)),
            ('noise_suffix',    lambda: detect_noise_suffix(tail)),
            ('seed_echo',       lambda: detect_seed_echo(tail, self.seed)),
            ('broken_grammar',  lambda: detect_broken_grammar(tail)),
            ('number_hijack',   lambda: detect_number_hijack(tail, self.seed)),
            ('short_garbage',   lambda: detect_short_garbage(tail)),
            ('marketplace_trash', lambda: detect_trash_marketplace(tail, self.target_country)),
            ('tech_garbage',    lambda: detect_technical_garbage(tail)),
            ('mixed_alpha',     lambda: detect_mixed_alphabet(tail)),
            ('standalone_num',  lambda: detect_standalone_number(tail, self.seed)),
        ]
        
        for signal_name, detector in detectors_negative:
            detected, reason = detector()
            if detected:
                negative_signals.append(signal_name)
                reasons.append(f"‚ùå {reason}")
        
        # ===== –ü–†–û–í–ï–†–ö–ê –ö–û–ì–ï–†–ï–ù–¢–ù–û–°–¢–ò –•–í–û–°–¢–ê =====
        # –ï—Å–ª–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä –ø–æ–π–º–∞–ª –æ–¥–Ω–æ —Å–ª–æ–≤–æ –≤ –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ–º —Ö–≤–æ—Å—Ç–µ,
        # –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ ‚Äî "—á—É–∂–∏–µ", –ø–æ–Ω–∏–∂–∞–µ–º –¥–æ GREY
        if positive_signals:
            is_coherent, orphans = self._check_coherence(tail)
            if not is_coherent:
                negative_signals.append('incoherent_tail')
                reasons.append(f"‚ö†Ô∏è –ù–µ–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω—ã–π —Ö–≤–æ—Å—Ç: —Å–ª–æ–≤–∞ {orphans} –Ω–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –ø–æ–∏—Å–∫–æ–≤—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º")
        
        # ===== –ê–†–ë–ò–¢–†–ê–ñ –° –í–ï–°–ê–ú–ò =====
        label, confidence, pos_score, neg_score = self._arbitrate(
            positive_signals, negative_signals
        )
        
        return {
            'label': label,
            'positive_signals': positive_signals,
            'negative_signals': negative_signals,
            'reasons': reasons,
            'confidence': confidence,
            'positive_score': pos_score,
            'negative_score': neg_score,
        }
    
    def _check_coherence(self, tail: str):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ–≥–æ —Ö–≤–æ—Å—Ç–∞.
        
        –ü—Ä–∏–Ω—Ü–∏–ø: –µ—Å–ª–∏ —Ö–≤–æ—Å—Ç 2+ —Å–ª–æ–≤ –∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä –ø–æ–π–º–∞–ª –æ–¥–Ω–æ,
        –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π ‚Üí incoherent.
        
        "—Ç–∏–≥—Ä–æ–≤ —Ñ–æ—Ç–æ" ‚Üí —Ñ–æ—Ç–æ=action ‚úÖ, —Ç–∏–≥—Ä–æ–≤=??? ‚Üí incoherent
        "–∑–∞–º–µ–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞" ‚Üí –∑–∞–º–µ–Ω–∞=action ‚úÖ, —Ñ–∏–ª—å—Ç—Ä=action ‚úÖ ‚Üí coherent
        
        Returns: (is_coherent: bool, orphan_words: list)
        """
        words = tail.lower().split()
        if len(words) < 2:
            return True, []
        
        # –°–ª–æ–≤–∞—Ä–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ª–µ–º–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        commerce_lemmas = {
            '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø—Ä–∞–π—Å', '—Ç–∞—Ä–∏—Ñ', '—Ä–∞—Å—Ü–µ–Ω–∫–∞',
            '–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–∑–∞–∫–∞–∑', '–ø–æ–∫—É–ø–∫–∞', '–æ–ø–ª–∞—Ç–∞',
            '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥—ë—à–µ–≤–æ', '–¥–µ—à–µ–≤–æ', '–±—é–¥–∂–µ—Ç–Ω—ã–π', '–∞–∫—Ü–∏—è',
            '—Å–∫–∏–¥–∫–∞', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '—Å—Ç–æ–∏—Ç—å',
            '—É—Å–ª—É–≥–∞', '—Å–µ—Ä–≤–∏—Å', '–ø—Ä–µ–π—Å–∫—É—Ä–∞–Ω—Ç', '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä',
        }
        reputation_lemmas = {
            '–æ—Ç–∑—ã–≤', '—Ä–µ–π—Ç–∏–Ω–≥', '–æ—Ü–µ–Ω–∫–∞', '–æ–±–∑–æ—Ä', '–º–Ω–µ–Ω–∏–µ',
            '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è', '–∂–∞–ª–æ–±–∞', '—Ñ–æ—Ä—É–º', '–±–ª–æ–≥',
            '–ª—É—á—à–∏–π', '—Ç–æ–ø', '—Ö—É–¥—à–∏–π', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å',
        }
        action_lemmas = {
            '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–º–∞–Ω—É–∞–ª',
            '–≤–∏–¥–µ–æ', '–≤–∏–¥–µ–æ–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '—Ñ–æ—Ç–æ', '—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è',
            '—Å—Ö–µ–º–∞', '—á–µ—Ä—Ç—ë–∂', '—á–µ—Ä—Ç–µ–∂', '–¥–∏–∞–≥—Ä–∞–º–º–∞',
            '—Ä–∞–∑–±–æ—Ä–∫–∞', '—Å–±–æ—Ä–∫–∞', '—á–∏—Å—Ç–∫–∞', '–∑–∞–º–µ–Ω–∞',
            '–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞', '–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞', '–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',
            '–∑–∞–ø—á–∞—Å—Ç—å', '–¥–µ—Ç–∞–ª—å', '–∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ', '—Ñ–∏–ª—å—Ç—Ä',
            '—â—ë—Ç–∫–∞', '—â–µ—Ç–∫–∞', '—à–ª–∞–Ω–≥', '–º–µ—à–æ–∫', '–ø—ã–ª–µ—Å–±–æ—Ä–Ω–∏–∫',
            '–º–æ—Ç–æ—Ä', '–¥–≤–∏–≥–∞—Ç–µ–ª—å', '—Ç—É—Ä–±–∏–Ω–∞', '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä',
            '—Å–º–æ—Ç—Ä–µ—Ç—å', '—Å–∫–∞—á–∞—Ç—å', '–Ω–∞–π—Ç–∏', '—Å–¥–µ–ª–∞—Ç—å', '–ø–æ—á–∏–Ω–∏—Ç—å',
            '–ø–æ—á–∏—Å—Ç–∏—Ç—å', '—Ä–∞–∑–æ–±—Ä–∞—Ç—å', '—Å–æ–±—Ä–∞—Ç—å', '–ø–æ–¥–∫–ª—é—á–∏—Ç—å',
            '—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å', '–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å', '–ø—Ä–æ–≤–µ—Ä–∏—Ç—å', '–∑–∞–º–µ–Ω–∏—Ç—å',
            '–ø–æ–∫–∞–∑–∞—Ç—å', '–æ–±—ä—è—Å–Ω–∏—Ç—å',
        }
        contacts_lemmas = {
            '–∞–¥—Ä–µ—Å', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–∫–æ–Ω—Ç–∞–∫—Ç', '–∫–∞—Ä—Ç–∞', '–º–∞—Ä—à—Ä—É—Ç',
            '–≥—Ä–∞—Ñ–∏–∫', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '—Ä–µ–∂–∏–º', '—á–∞—Å—ã', '—Ä–∞–±–æ—Ç–∞',
        }
        location_lemmas = {
            '—Ä—è–¥–æ–º', '–ø–æ–±–ª–∏–∑–æ—Å—Ç–∏', '–±–ª–∏–∂–∞–π—à–∏–π', '–Ω–µ–¥–∞–ª–µ–∫–æ',
            '—Ä–∞–π–æ–Ω', '—É–ª–∏—Ü–∞', '–¥–æ–º', '–∫–≤–∞—Ä—Ç–∏—Ä–∞',
        }
        time_lemmas = {
            '–∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ', '—Å—Ä–æ—á–Ω–æ', '–±—ã—Å—Ç—Ä–æ', '—Å–µ–≥–æ–¥–Ω—è', '—Å–µ–π—á–∞—Å',
        }
        marketplace_lemmas = {
            '–æ–ª—Ö', 'olx', '—Ä–æ–∑–µ—Ç–∫–∞', 'rozetka', '–ø—Ä–æ–º', 'hotline',
            '–∞–ª–∏—ç–∫—Å–ø—Ä–µ—Å—Å', 'aliexpress', '–∞–º–∞–∑–æ–Ω', 'amazon',
            '—ç–ø–∏—Ü–µ–Ω—Ç—Ä',
        }
        
        all_known = (commerce_lemmas | reputation_lemmas | action_lemmas |
                     contacts_lemmas | location_lemmas | time_lemmas | marketplace_lemmas)
        
        # POS –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (—Å–ª—É–∂–µ–±–Ω—ã–µ/–º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã ‚Äî –Ω–µ –Ω–µ—Å—É—Ç —Ç–µ–º–∞—Ç–∏–∫—É)
        skip_pos = {'PREP', 'CONJ', 'PRCL', 'INTJ', 'ADVB', 'PRED',
                     'ADJF', 'ADJS', 'COMP', 'PRTS', 'PRTF'}
        
        orphans = []
        for w in words:
            parsed = morph.parse(w)[0]
            pos = parsed.tag.POS
            lemma = parsed.normal_form
            
            # –°–ª—É–∂–µ–±–Ω—ã–µ –∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if pos in skip_pos:
                continue
            # –ò–∑–≤–µ—Å—Ç–Ω–∞—è –ª–µ–º–º–∞
            if lemma in all_known or w in all_known:
                continue
            # –ì–µ–æ –∏–ª–∏ –±—Ä–µ–Ω–¥
            if w in self.geo_db or lemma in self.geo_db:
                continue
            if w in self.brand_db or lemma in self.brand_db:
                continue
            
            orphans.append(w)
        
        return len(orphans) == 0, orphans
    
    def _arbitrate(
        self, positive: List[str], negative: List[str]
    ) -> Tuple[str, float, float, float]:
        """
        –ê—Ä–±–∏—Ç—Ä–∞–∂ —Å –≤–µ—Å–∞–º–∏.
        
        –ö–ª—é—á–µ–≤–∞—è –ª–æ–≥–∏–∫–∞:
        - –°–∏–≥–Ω–∞–ª—ã –∏–∑ –ë–î (geo, brand) –ø–µ—Ä–µ–≤–µ—à–∏–≤–∞—é—Ç —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ (dangling)
        - –ü—Ä–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–µ: –µ—Å–ª–∏ –µ—Å—Ç—å geo/brand ‚Üí —Å–∫–æ—Ä–µ–µ VALID
        - –ë–µ–∑ —Å–∏–≥–Ω–∞–ª–æ–≤ –≤–æ–æ–±—â–µ ‚Üí GREY
        
        Returns:
            (label, confidence, positive_score, negative_score)
        """
        pos_score = sum(SIGNAL_WEIGHTS.get(s, 0.5) for s in positive)
        neg_score = sum(SIGNAL_WEIGHTS.get(s, 0.5) for s in negative)
        
        has_positive = len(positive) > 0
        has_negative = len(negative) > 0
        
        # --- –°–ª—É—á–∞–π 1: –¢–æ–ª—å–∫–æ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ ---
        if has_positive and not has_negative:
            confidence = min(0.85 + pos_score * 0.05, 0.99)
            return 'VALID', confidence, pos_score, neg_score
        
        # --- –°–ª—É—á–∞–π 2: –¢–æ–ª—å–∫–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ ---
        if has_negative and not has_positive:
            confidence = min(0.85 + neg_score * 0.05, 0.99)
            return 'TRASH', confidence, pos_score, neg_score
        
        # --- –°–ª—É—á–∞–π 3: –ö–æ–Ω—Ñ–ª–∏–∫—Ç ---
        if has_positive and has_negative:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ë–î-—Å–∏–≥–Ω–∞–ª–æ–≤: –µ—Å–ª–∏ geo –∏–ª–∏ brand –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω,
            # –∞ –Ω–µ–≥–∞—Ç–∏–≤ ‚Äî —Ç–æ–ª—å–∫–æ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞, –¥–æ–≤–µ—Ä—è–µ–º –ë–î
            db_signals = {'geo', 'brand', 'marketplace_valid', 'verb_modifier', 'conjunctive'}
            has_db_positive = bool(set(positive) & db_signals)
            
            # –ñ—ë—Å—Ç–∫–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ (–ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –ø—Ä–∞–≤—ã)
            hard_negatives = {'duplicate', 'meta', 'marketplace_trash', 'tech_garbage', 'mixed_alpha'}
            has_hard_negative = bool(set(negative) & hard_negatives)
            
            if has_db_positive and not has_hard_negative:
                # –ë–î –≥–æ–≤–æ—Ä–∏—Ç VALID, —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –≥–æ–≤–æ—Ä–∏—Ç TRASH ‚Üí –¥–æ–≤–µ—Ä—è–µ–º –ë–î
                confidence = 0.75
                return 'VALID', confidence, pos_score, neg_score
            
            if has_hard_negative:
                # –ú–µ—Ç–∞-–≤–æ–ø—Ä–æ—Å –∏–ª–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Üí –¥–∞–∂–µ –±—Ä–µ–Ω–¥ –Ω–µ —Å–ø–∞—Å–∞–µ—Ç
                if pos_score > neg_score * 1.5:
                    return 'GREY', 0.3, pos_score, neg_score
                return 'TRASH', 0.65, pos_score, neg_score
            
            # –û–±—ã—á–Ω—ã–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç ‚Äî –ø–æ –≤–µ—Å–∞–º
            if pos_score > neg_score * 1.2:
                return 'VALID', 0.6, pos_score, neg_score
            elif neg_score > pos_score * 1.2:
                return 'TRASH', 0.6, pos_score, neg_score
            else:
                return 'GREY', 0.3, pos_score, neg_score
        
        # --- –°–ª—É—á–∞–π 4: –ù–∏—á–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ ---
        return 'GREY', 0.5, pos_score, neg_score


# ==================== –¢–ï–°–¢–´ ====================

def run_tests():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞."""
    
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï TailFunctionClassifier v2\n")
    
    from databases import load_geonames_db, load_brands_db
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
    geo_db = load_geonames_db()
    brand_db = load_brands_db()
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(geo_db)} –≥–æ—Ä–æ–¥–æ–≤, {len(brand_db)} –±—Ä–µ–Ω–¥–æ–≤\n")
    
    classifier = TailFunctionClassifier(geo_db, brand_db, seed="—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∫–µ–π—Å—ã: (tail, expected_label, description)
    test_cases = [
        # VALID ‚Äî –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
        ("",              "VALID", "–ü—É—Å—Ç–æ–π —Ö–≤–æ—Å—Ç (= seed)"),
        ("–∫–∏–µ–≤",          "VALID", "–ì–æ—Ä–æ–¥"),
        ("samsung",       "VALID", "–ë—Ä–µ–Ω–¥"),
        ("—Ü–µ–Ω–∞",          "VALID", "–ö–æ–º–º–µ—Ä—Ü–∏—è"),
        ("–æ—Ç–∑—ã–≤—ã",        "VALID", "–†–µ–ø—É—Ç–∞—Ü–∏—è"),
        ("—Ä—è–¥–æ–º",         "VALID", "–õ–æ–∫–∞—Ü–∏—è"),
        ("—Å–≤–æ–∏–º–∏ —Ä—É–∫–∞–º–∏",  "VALID", "–î–µ–π—Å—Ç–≤–∏–µ"),
        ("—Ñ–æ—Ä—É–º",         "VALID", "–†–µ–ø—É—Ç–∞—Ü–∏—è (—Ñ–æ—Ä—É–º)"),
        ("—É—Å–ª—É–≥–∏",        "GREY",  "–ù–µ—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ ‚Üí GREY"),
        ("—Ä–∞–±–æ—Ç–∞",        "GREY",  "–ù–µ—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ ‚Üí GREY"),
        ("–Ω–∞ –¥–æ–º—É",       "VALID", "–õ–æ–∫–∞—Ü–∏—è (–Ω–∞ –¥–æ–º—É)"),
        ("–Ω–µ–¥–æ—Ä–æ–≥–æ",      "VALID", "–ö–æ–º–º–µ—Ä—Ü–∏—è (–Ω–µ–¥–æ—Ä–æ–≥–æ)"),
        
        # TRASH ‚Äî –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
        ("–µ—Å—Ç—å",          "TRASH", "–ö–æ–ø—É–ª–∞ –±–µ–∑ –æ–±—ä–µ–∫—Ç–∞"),
        ("–∑–∞—á–µ–º",         "TRASH", "–ú–µ—Ç–∞-–≤–æ–ø—Ä–æ—Å"),
        ("–ª—É—á—à–∏–µ",        "TRASH", "–í–∏—Å—è—á–∏–π –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä"),
        ("–∏",             "TRASH", "–°–æ—é–∑ –Ω–∞ –∫–æ–Ω—Ü–µ (–æ–±—Ä—ã–≤–æ–∫)"),
        ("–¥–ª—è",           "TRASH", "–ü—Ä–µ–¥–ª–æ–≥ –Ω–∞ –∫–æ–Ω—Ü–µ (–æ–±—Ä—ã–≤–æ–∫)"),
        ("—Ä–∞–∑–ª–∏—á–∏—è",      "TRASH", "–ú—É—Å–æ—Ä–Ω—ã–π —Å—É—Ñ—Ñ–∏–∫—Å"),
        ("—ç—Ç–æ —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç", "TRASH", "–ú–µ—Ç–∞-–≤–æ–ø—Ä–æ—Å"),
        ("–º–æ–∂–Ω–æ",         "TRASH", "–ú–æ–¥–∞–ª—å–Ω–æ–µ –±–µ–∑ –¥–µ–π—Å—Ç–≤–∏—è"),
        
        # GREY ‚Äî –∫–æ–Ω—Ñ–ª–∏–∫—Ç –∏–ª–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å
        ("xiaomi dreame", "GREY",  "Brand collision + brand ‚Üí –∫–æ–Ω—Ñ–ª–∏–∫—Ç"),
        ("–∫—É–ø–∏—Ç—å",        "GREY",  "–ù–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π, –Ω–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π"),
    ]
    
    print("=" * 70)
    passed = 0
    
    for tail, expected, description in test_cases:
        result = classifier.classify(tail)
        label = result['label']
        
        status = "‚úÖ" if label == expected else "‚ùå"
        if label == expected:
            passed += 1
        
        print(f"{status} {description}")
        print(f"   –•–≤–æ—Å—Ç: '{tail}'")
        print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: {expected}, –ü–æ–ª—É—á–µ–Ω–æ: {label} "
              f"(conf: {result['confidence']:.2f}, "
              f"+{result['positive_score']:.1f} / -{result['negative_score']:.1f})")
        
        if result['positive_signals']:
            print(f"   ‚úÖ {', '.join(result['positive_signals'])}")
        if result['negative_signals']:
            print(f"   ‚ùå {', '.join(result['negative_signals'])}")
        print()
    
    print("=" * 70)
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢: {passed}/{len(test_cases)} "
          f"({passed/len(test_cases)*100:.1f}%)")
    
    return passed, len(test_cases)


if __name__ == "__main__":
    run_tests()
