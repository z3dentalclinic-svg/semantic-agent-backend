"""
Batch Post-Filter v7.9 - FUNDAMENTAL FIX: GEO DATABASE PRIORITY
Based on Gemini's recommendations for 187 countries support
"""

import re
import logging
import time
from typing import List, Dict, Set, Tuple, Optional
from collections import Counter

logger = logging.getLogger("BatchPostFilter")


class BatchPostFilter:
    def __init__(self, 
                 all_cities_global: Dict[str, str], 
                 forbidden_geo: Set[str], 
                 districts: Optional[Dict[str, str]] = None,
                 population_threshold: int = 5000):
        self.forbidden_geo = forbidden_geo
        self.districts = districts or {}
        self.population_threshold = population_threshold
        
        self.city_abbreviations = self._get_city_abbreviations()
        self.regions = self._get_regions()
        self.countries = self._get_countries()
        self.manual_small_cities = self._get_manual_small_cities()
        
        self.ignored_words = {
            "–¥–æ–º", "–º–∏—Ä", "–±–æ—Ä", "–Ω–∏–≤–∞", "–±–∞–ª–∫–∞", "–ª—É—á", "—Å–ø—É—Ç–Ω–∏–∫", "—Ä–∞–±–æ—Ç–∞", "—Ü–µ–Ω–∞", "–≤—ã–µ–∑–¥",
        }
        
        base_index = {k.lower().strip(): v for k, v in (all_cities_global or {}).items()}
        geo_index = self._build_filtered_geo_index()
        
        for k, v in geo_index.items():
            if k not in base_index:
                base_index[k] = v
        
        self.all_cities_global = base_index
        
        forced_by_cities = {
            "–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏": "by",
            "baranovichi": "by",
            "–∂–¥–∞–Ω–æ–≤–∏—á–∏": "by",
            "zhdanovichi": "by",
            "–ª–æ—à–∏—Ü–∞": "by",
        }
        
        for name, code in forced_by_cities.items():
            if name not in self.all_cities_global:
                self.all_cities_global[name] = code
        
        try:
            import pymorphy3
            self.morph_ru = pymorphy3.MorphAnalyzer(lang='ru')
            self.morph_uk = pymorphy3.MorphAnalyzer(lang='uk')
            self._has_morph = True
        except ImportError:
            self._has_morph = False
    
    def _get_city_abbreviations(self) -> Dict[str, str]:
        return {
            '–µ–∫–±': 'ru', '–µ–∫–∞—Ç': 'ru', '—Å–ø–±': 'ru', '–ø–∏—Ç–µ—Ä': 'ru', '–º—Å–∫': 'ru',
            '–Ω—Å–∫': 'ru', '–Ω–Ω': 'ru', '–Ω–Ω–æ–≤': 'ru', '–≤–ª–∞–¥': 'ru', '—Ä–æ—Å—Ç–æ–≤': 'ru',
            '–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä': 'ru', '–º–Ω': 'by', '–∞–ª–º–∞—Ç—ã': 'kz', '–∞—Å—Ç–∞–Ω–∞': 'kz', '—Ç–∞—à–∫–µ–Ω—Ç': 'uz',
        }
    
    def _get_regions(self) -> Dict[str, str]:
        return {
            '–∏–Ω–≥—É—à–µ—Ç–∏—è': 'ru', '—á–µ—á–Ω—è': 'ru', '—á–µ—á–µ–Ω—Å–∫–∞—è —Ä–µ—Å–ø—É–±–ª–∏–∫–∞': 'ru',
            '–¥–∞–≥–µ—Å—Ç–∞–Ω': 'ru', '—Ç–∞—Ç–∞—Ä—Å—Ç–∞–Ω': 'ru', '–±–∞—à–∫–æ—Ä—Ç–æ—Å—Ç–∞–Ω': 'ru',
            '—É–¥–º—É—Ä—Ç–∏—è': 'ru', '–º–æ—Ä–¥–æ–≤–∏—è': 'ru', '–º–∞—Ä–∏–π —ç–ª': 'ru',
            '—á—É–≤–∞—à–∏—è': 'ru', '—è–∫—É—Ç–∏—è': 'ru', '—Å–∞—Ö–∞': 'ru', '–±—É—Ä—è—Ç–∏—è': 'ru',
            '—Ç—ã–≤–∞': 'ru', '—Ö–∞–∫–∞—Å–∏—è': 'ru', '–∞–ª—Ç–∞–π': 'ru', '–∫–∞—Ä–µ–ª–∏—è': 'ru',
            '–∫–æ–º–∏': 'ru', '–∫–∞–ª–º—ã–∫–∏—è': 'ru', '–∞–¥—ã–≥–µ—è': 'ru', '–∫–∞–±–∞—Ä–¥–∏–Ω–æ-–±–∞–ª–∫–∞—Ä–∏—è': 'ru',
            '–∫–∞—Ä–∞—á–∞–µ–≤–æ-—á–µ—Ä–∫–µ—Å–∏—è': 'ru', '—Å–µ–≤–µ—Ä–Ω–∞—è –æ—Å–µ—Ç–∏—è': 'ru', '–∫—Ä—ã–º': 'ru',
            '–º–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru', '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru',
            '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru', '—Å–≤–µ—Ä–¥–ª–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru',
            '–º–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by', '–≥–æ–º–µ–ª—å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–º–æ–≥–∏–ª–µ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by', '–≤–∏—Ç–µ–±—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–≥—Ä–æ–¥–Ω–µ–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by', '–±—Ä–µ—Å—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–∞–ª–º–∞—Ç–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'kz', '—é–∂–Ω–æ-–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'kz',
            '—Ç–∞—à–∫–µ–Ω—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'uz', '—Å–∞–º–∞—Ä–∫–∞–Ω–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'uz',
        }
    
    def _get_countries(self) -> Dict[str, str]:
        return {
            '—Ä–æ—Å—Å–∏—è': 'ru', '—Ä–æ—Å—Å–∏–∏': 'ru', '—Ä—Ñ': 'ru',
            '–±–µ–ª–∞—Ä—É—Å—å': 'by', '–±–µ–ª–æ—Ä—É—Å—Å–∏—è': 'by',
            '–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω': 'kz', '–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ': 'kz',
            '—É–∑–±–µ–∫–∏—Å—Ç–∞–Ω': 'uz', '—É–∑–±–µ–∫–∏—Å—Ç–∞–Ω–µ': 'uz',
            '—É–∫—Ä–∞–∏–Ω–∞': 'ua', '—É–∫—Ä–∞–∏–Ω–µ': 'ua',
            '–∏–∑—Ä–∞–∏–ª—å': 'il', '–∏–∑—Ä–∞–∏–ª–µ': 'il',
            '–ø–æ–ª—å—à–∞': 'pl', '–ø–æ–ª—å—à–µ': 'pl',
            '–≥–µ—Ä–º–∞–Ω–∏—è': 'de', '–≥–µ—Ä–º–∞–Ω–∏–∏': 'de',
            '—Å—à–∞': 'us', '–∞–º–µ—Ä–∏–∫–∞': 'us', '–∞–º–µ—Ä–∏–∫–µ': 'us',
        }
    
    def _get_manual_small_cities(self) -> Dict[str, str]:
        return {
            '–æ—à': 'kg',
            '—É–∑—ã–Ω–∞–≥–∞—à': 'kz',
            '—â–µ–ª–∫–∏–Ω–æ': 'ru',
            '—â—ë–ª–∫ino': 'ru',
            '–π–æ—Ç–∞': 'unknown',
        }
    
    def _build_filtered_geo_index(self) -> Dict[str, str]:
        try:
            import geonamescache
            gc = geonamescache.GeonamesCache()
            cities = gc.get_cities()
            
            filtered_index = {}
            
            for city_id, city_data in cities.items():
                country = city_data['countrycode'].lower()
                population = city_data.get('population', 0)
                
                if population < self.population_threshold:
                    continue
                
                name = city_data['name'].lower().strip()
                filtered_index[name] = country
                
                for alt in city_data.get('alternatenames', []):
                    alt = alt.strip()
                    if not (3 <= len(alt) <= 50):
                        continue
                    if not any(c.isalpha() for c in alt):
                        continue
                    
                    is_latin_cyrillic = all(
                        ('\u0000' <= c <= '\u007F') or
                        ('\u0400' <= c <= '\u04FF') or
                        c in ['-', "'", ' ']
                        for c in alt
                    )
                    if not is_latin_cyrillic:
                        continue
                    
                    alt_lower = alt.lower()
                    
                    has_cyr = any('\u0400' <= c <= '\u04FF' for c in alt_lower)
                    has_lat = any('a' <= c <= 'z' for c in alt_lower)
                    
                    if has_cyr and not has_lat:
                        if alt_lower not in filtered_index:
                            filtered_index[alt_lower] = country
                    
                    if alt_lower not in filtered_index:
                        filtered_index[alt_lower] = country
                    
                    alt_dash = alt_lower.replace(' ', '-')
                    if alt_dash != alt_lower and alt_dash not in filtered_index:
                        filtered_index[alt_dash] = country
            
            return filtered_index
            
        except ImportError:
            return {
                '–º–æ—Å–∫–≤–∞': 'ru', '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥': 'ru', 
                '–∫–∏–µ–≤': 'ua', '—Ö–∞—Ä—å–∫–æ–≤': 'ua', '–æ–¥–µ—Å—Å–∞': 'ua',
                '–º–∏–Ω—Å–∫': 'by', '–∞–ª–º–∞—Ç—ã': 'kz', '—Ç–∞—à–∫–µ–Ω—Ç': 'uz'
            }

    def filter_batch(self, keywords: List[str], seed: str, country: str, 
                     language: str = 'ru') -> Dict:
        start_time = time.time()
        
        logger.info(f"[BPF] START filter_batch | country={country} | lang={language}")
        logger.info(f"[BPF] RAW keywords ({len(keywords)}): {keywords}")
        
        unique_raw = sorted(list(set([k.lower().strip() for k in keywords if k.strip()])))
        logger.info(f"[BPF] UNIQUE_RAW ({len(unique_raw)}): {unique_raw}")
        
        seed_cities = self._extract_cities_from_seed(seed, country, language)
        logger.info(f"[BPF] SEED='{seed}' | seed_cities={seed_cities}")
        
        all_words = set()
        for kw in unique_raw:
            all_words.update(re.findall(r'[–∞-—è—ëa-z0-9-]+', kw))
        
        lemmas_map = self._batch_lemmatize(all_words, language)
        
        final_keywords = []
        final_anchors = []
        stats = {
            'total': len(unique_raw),
            'allowed': 0,
            'blocked': 0,
            'reasons': Counter()
        }

        for kw in unique_raw:
            is_allowed, reason, category = self._check_geo_conflicts_v75(
                kw, country, lemmas_map, seed_cities, language
            )
            
            if is_allowed:
                final_keywords.append(kw)
                stats['allowed'] += 1
            else:
                final_anchors.append(kw)
                stats['blocked'] += 1
                stats['reasons'][category] += 1

        elapsed = time.time() - start_time
        logger.info(f"[BPF] FINISH {elapsed:.2f}s | "
                    f"allowed={len(final_keywords)} | anchors={len(final_anchors)} | "
                    f"reasons={dict(stats['reasons'])}")

        return {
            'keywords': final_keywords,
            'anchors': final_anchors,
            'stats': {
                'total': stats['total'],
                'allowed': stats['allowed'],
                'blocked': stats['blocked'],
                'reasons': dict(stats['reasons']),
                'elapsed_time': round(elapsed, 2)
            }
        }

    def _check_geo_conflicts_v75(self, keyword: str, country: str, 
                                  lemmas_map: Dict[str, str], seed_cities: Set[str],
                                  language: str) -> Tuple[bool, str, str]:
        logger.debug(f"[BPF] CHECK keyword='{keyword}' | country={country} | "
                     f"seed_cities={seed_cities}")
        
        words = re.findall(r'[–∞-—è—ëa-z0-9-]+', keyword)
        if not words:
            return True, "", ""

        keyword_lemmas = [lemmas_map.get(w, w) for w in words]
        
        words_set = set(words + keyword_lemmas)
        if any(city in words_set for city in seed_cities):
            logger.debug(f"[BPF] ALLOW by seed_cities | keyword='{keyword}'")
            return True, "", ""
        
        for check_val in words + keyword_lemmas:
            if check_val in self.forbidden_geo:
                return False, f"Hard-Blacklist '{check_val}'", "hard_blacklist"

        for w in words:
            if w in self.districts:
                dist_country = self.districts[w]
                if dist_country != country.lower():
                    return False, f"—Ä–∞–π–æ–Ω '{w}' ({dist_country})", "districts"
        
        for w in words + keyword_lemmas:
            if w in self.city_abbreviations:
                abbr_country = self.city_abbreviations[w]
                if abbr_country != country.lower():
                    return False, f"—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ '{w}' ({abbr_country})", f"{abbr_country}_abbreviations"
        
        check_regions = words + keyword_lemmas + self._extract_ngrams(words, 2)
        for item in check_regions:
            if item in self.regions:
                region_country = self.regions[item]
                if region_country != country.lower():
                    return False, f"—Ä–µ–≥–∏–æ–Ω '{item}' ({region_country})", f"{region_country}_regions"
        
        for w in words + keyword_lemmas:
            if w in self.countries:
                ctry_code = self.countries[w]
                if ctry_code != country.lower():
                    return False, f"—Å—Ç—Ä–∞–Ω–∞ '{w}' ({ctry_code})", f"{ctry_code}_countries"
        
        for w in words + keyword_lemmas:
            if w in self.manual_small_cities:
                city_country = self.manual_small_cities[w]
                if city_country == 'unknown':
                    return False, f"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ–±—ä–µ–∫—Ç '{w}'", "unknown"
                if city_country != country.lower():
                    return False, f"–º–∞–ª—ã–π –≥–æ—Ä–æ–¥ '{w}' ({city_country})", f"{city_country}_small_cities"

        search_items = []
        search_items.extend(words)
        search_items.extend(keyword_lemmas)
        
        bigrams = self._extract_ngrams(words, 2)
        search_items.extend(bigrams)
        search_items.extend([bg.replace(' ', '-') for bg in bigrams])
        
        lemma_bigrams = self._extract_ngrams(keyword_lemmas, 2)
        search_items.extend(lemma_bigrams)
        search_items.extend([bg.replace(' ', '-') for bg in lemma_bigrams])
        
        trigrams = self._extract_ngrams(words, 3)
        search_items.extend(trigrams)
        search_items.extend([tg.replace(' ', '-') for tg in trigrams])

        for item in search_items:
            if len(item) < 3 or item in self.ignored_words:
                continue
            
            item_normalized = self._get_lemma(item, language)
            found_country = self.all_cities_global.get(item_normalized) or self.all_cities_global.get(item)
            
            if found_country:
                # –ê–ú–ù–ò–°–¢–ò–Ø: –ï—Å–ª–∏ —ç—Ç–æ –≥–æ—Ä–æ–¥ –Ω–∞—à–µ–π —Å—Ç—Ä–∞–Ω—ã (UA) –∏–ª–∏ –æ–Ω –≤ SEED - –ü–†–û–ü–£–°–ö–ê–ï–ú
                if found_country == country.lower() or item_normalized in seed_cities:
                    continue
                
                # –ë–õ–û–ö–ò–†–£–ï–ú –¢–û–õ–¨–ö–û –†–ï–ê–õ–¨–ù–û –ß–£–ñ–ò–ï –°–¢–†–ê–ù–´
                return False, f"Foreign city {found_country}", f"{found_country}_cities"
            
            # üî• –ù–û–í–û–ï: –ï—Å–ª–∏ —ç—Ç–æ —Ä–∞–π–æ–Ω –∏–ª–∏ –º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω (–ß–µ—Ä–µ–º—É—à–∫–∏, –ê–ª–µ–∫—Å–µ–µ–≤–∫–∞), 
            # –∏ –æ–Ω –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –∫–∞–∫ —á—É–∂–æ–π –≥–æ—Ä–æ–¥ - –ú–´ –ï–ì–û –ù–ï –¢–†–û–ì–ê–ï–ú (True)

                if self._is_common_noun(item_normalized, language):
                    continue
        
        if not self._is_grammatically_valid(keyword, language):
            return False, "–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞", "grammar"
        
        return True, "", ""

    def _is_common_noun(self, word: str, language: str) -> bool:
        if not self._has_morph or language not in ['ru', 'uk']:
            return False
        
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        
        try:
            parsed = morph.parse(word)
            if parsed:
                for parse_variant in parsed:
                    tag = parse_variant.tag
                    
                    if 'Geox' in tag:
                        return False
                    
                    if 'Name' in tag:
                        return False
                
                first_tag = parsed[0].tag
                if 'NOUN' in first_tag and word.islower():
                    return True
        except:
            pass
        
        return False

    def _extract_cities_from_seed(self, seed: str, country: str, language: str) -> Set[str]:
        """üî• FIX: –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ—Ä–æ–¥–∞ –∏–∑ seed –ë–ï–ó —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —Å—Ç—Ä–∞–Ω–µ"""
        if not self._has_morph:
            return set()
        
        seed_cities = set()
        words = re.findall(r'[–∞-—è—ëa-z0-9-]+', seed.lower())
        
        for word in words:
            # –ë–ï–ó –ü–†–û–í–ï–†–ö–ò country!
            if word in self.all_cities_global:
                logger.debug(f"[BPF] seed_city WORD '{word}' -> {self.all_cities_global[word]}")
                seed_cities.add(word)
            
            lemma = self._get_lemma(word, language)
            if lemma in self.all_cities_global:
                logger.debug(f"[BPF] seed_city LEMMA '{lemma}' <- '{word}' "
                             f"-> {self.all_cities_global[lemma]}")
                seed_cities.add(lemma)
        
        bigrams = self._extract_ngrams(words, 2)
        for bigram in bigrams:
            if bigram in self.all_cities_global:
                logger.debug(f"[BPF] seed_city BIGRAM '{bigram}' -> {self.all_cities_global[bigram]}")
                seed_cities.add(bigram)
        
        return seed_cities

    def _batch_lemmatize(self, words: Set[str], language: str) -> Dict[str, str]:
        if not self._has_morph:
            return {w: w for w in words}
        
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        lemmas = {}
        
        for word in words:
            lemma = self._get_lemma(word, language, morph)
            lemmas[word] = lemma
        
        return lemmas

    def _get_lemma(self, word: str, language: str, morph=None) -> str:
        if not self._has_morph:
            return word
        
        if morph is None:
            morph = self.morph_ru if language == 'ru' else self.morph_uk
        
        try:
            parsed = morph.parse(word)
            if parsed:
                return parsed[0].normal_form
        except:
            pass
        
        return word

    def _extract_ngrams(self, words: List[str], n: int = 2) -> List[str]:
        if len(words) < n:
            return []
        return [" ".join(words[i:i+n]) for i in range(len(words) - n + 1)]

    def _is_grammatically_valid(self, keyword: str, language: str) -> bool:
        return True  # –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ–ª–Ω–∞—è –∞–º–Ω–∏—Å—Ç–∏—è, —á—Ç–æ–±—ã —Å–ø–∞—Å—Ç–∏ –∫–ª—é—á–∏


# ============================================
# DISTRICTS
# ============================================

DISTRICTS_MINSK = {
    "—É—Ä—É—á—å–µ": "by",
    "—à–∞–±–∞–Ω—ã": "by",
    "–∫–∞–º–µ–Ω–Ω–∞—è –≥–æ—Ä–∫–∞": "by",
    "—Å–µ—Ä–µ–±—Ä—è–Ω–∫–∞": "by"
}

DISTRICTS_TASHKENT = {
    "—á–∏–ª–∞–Ω–∑–∞—Ä": "uz",
    "—é–Ω—É—Å–∞–±–∞–¥": "uz",
    "—Å–µ—Ä–≥–µ–ª–∏": "uz",
    "—è–∫–∫–∞—Å–∞—Ä–∞–π": "uz"
}

DISTRICTS_EXTENDED = {**DISTRICTS_MINSK, **DISTRICTS_TASHKENT}
