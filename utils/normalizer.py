import re
import pymorphy3
from typing import List

class GoldenNormalizer:
    def __init__(self):
        self.morph = pymorphy3.MorphAnalyzer()

    def normalize_by_golden_seed(self, keyword: str, golden_seed: str) -> str:
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° None Ð¸Ð»Ð¸ Ð¿ÑƒÑÑ‚ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ
        if not golden_seed or not keyword:
            return keyword
        
        # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²Ñ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÐºÐ»ÑŽÑ‡Ð°
        tokens_in = keyword.split()
            
        # 1. Ð‘ÐµÑ€ÐµÐ¼ Ð¾ÑÐ½Ð¾Ð²Ñ‹ ÑÐ»Ð¾Ð² Ð¸Ð· Ð¡Ð˜Ð”Ð
        seed_bases = {}
        for w in re.findall(r'\w+', golden_seed.lower()):
            try:
                parsed = self.morph.parse(w)
                if parsed:
                    base = parsed[0].normal_form
                    seed_bases[base] = w
                else:
                    seed_bases[w] = w  # fallback
            except Exception:
                seed_bases[w] = w  # fallback Ð¿Ñ€Ð¸ Ð»ÑŽÐ±Ð¾Ð¹ Ð¾ÑˆÐ¸Ð±ÐºÐµ

        # 2. Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ»ÑŽÑ‡ Ð½Ð° Ñ‚Ð¾ÐºÐµÐ½Ñ‹
        tokens = keyword.split()
        result = []

        for token in tokens:
            if not token:  # Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹
                continue
            
            try:
                # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
                clean_token = token.lower().strip('.,!?() ')
                if not clean_token:
                    result.append(token)
                    continue
                
                # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¾Ð¹
                parsed = self.morph.parse(clean_token)
                if not parsed:
                    # Ð•ÑÐ»Ð¸ pymorphy Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð» - Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ
                    result.append(token)
                    continue
                
                base = parsed[0].normal_form

                if base in seed_bases:
                    # Ð•ÑÐ»Ð¸ ÑÐ»Ð¾Ð²Ð¾ Ð¸Ð· ÑÐ¸Ð´Ð° â€” Ð¿Ñ€Ð¸Ð²Ð¾Ð´Ð¸Ð¼ Ðº Ñ„Ð¾Ñ€Ð¼Ðµ ÑÐ¸Ð´Ð°
                    result.append(seed_bases[base])
                else:
                    # Ð•ÑÐ»Ð¸ ÑÐ»Ð¾Ð²Ð° ÐÐ•Ð¢ Ð² ÑÐ¸Ð´Ðµ - Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ ÐšÐÐš Ð•Ð¡Ð¢Ð¬
                    result.append(token)
            except Exception:
                # ÐŸÑ€Ð¸ Ð»ÑŽÐ±Ð¾Ð¹ Ð¾ÑˆÐ¸Ð±ÐºÐµ - Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚Ð¾ÐºÐµÐ½
                result.append(token)

        final_result = " ".join(result)
        
        # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐµÑÐ»Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð¿Ð¾Ñ‚ÐµÑ€ÑÐ»Ð¸ÑÑŒ
        tokens_out = final_result.split() if final_result else []
        if len(tokens_in) != len(tokens_out):
            print(f"âš ï¸ ÐŸÐžÐ¢Ð•Ð Ð¯ Ð¢ÐžÐšÐ•ÐÐžÐ’: IN({len(tokens_in)}): '{keyword}' â†’ OUT({len(tokens_out)}): '{final_result}'")
        
        return final_result

    def process_batch(self, keywords: List[str], golden_seed: str) -> List[str]:
        if not keywords or not golden_seed: return keywords
        
        print(f"ðŸ” Normalization IN: {len(keywords)} keywords, seed: '{golden_seed}'")
        
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡
        normalized = [self.normalize_by_golden_seed(kw, golden_seed) for kw in keywords]
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        empty_count = sum(1 for n in normalized if not n or not n.strip())
        if empty_count > 0:
            print(f"âš ï¸ ÐŸÐ£Ð¡Ð¢Ð«Ð• Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹: {empty_count} Ð¸Ð· {len(normalized)}")
        
        print(f"ðŸ” Normalization OUT: {len(normalized)} keywords")
        
        # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº (Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹)
        return normalized


# Global instance
_normalizer = None


def get_normalizer():
    global _normalizer
    if _normalizer is None:
        _normalizer = GoldenNormalizer()
    return _normalizer


def normalize_keywords(keywords: List[str], language: str = 'ru', seed: str = '') -> List[str]:
    if not seed:
        return keywords
    normalizer = get_normalizer()
    return normalizer.process_batch(keywords, seed)
