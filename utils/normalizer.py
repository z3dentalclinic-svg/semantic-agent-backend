import re
import pymorphy3
from typing import List

class GoldenNormalizer:
    def __init__(self):
        self.morph = pymorphy3.MorphAnalyzer()

    def normalize_by_golden_seed(self, keyword: str, golden_seed: str) -> str:
        if not golden_seed or not keyword:
            return keyword
        
        # 1. Ð“Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð¾ÑÐ½Ð¾Ð² Ð¡Ð˜Ð”Ð (Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾, Ð±ÐµÐ· Ð¿Ñ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¾ÑÐ½Ð¾Ð² Ð¾Ñ‚ ÐšÐ»Ð¾Ð´Ð°!)
        seed_bases = {}
        for w in re.findall(r'\w+', golden_seed.lower()):
            base = self.morph.parse(w)[0].normal_form
            seed_bases[base] = w  # Ð¡Ð¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ð¾ÑÐ½Ð¾Ð²Ñƒ Ñ Ñ„Ð¾Ñ€Ð¼Ð¾Ð¹, ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ñ…Ð¾Ñ‡ÐµÑ‚ ÑŽÐ·ÐµÑ€

        # 2. ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐºÐ»ÑŽÑ‡Ð°
        tokens = keyword.split()
        new_tokens = []

        for token in tokens:
            # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð¾Ñ‚ Ð·Ð½Ð°ÐºÐ¾Ð² Ð¿Ñ€ÐµÐ¿Ð¸Ð½Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¾ÑÐ½Ð¾Ð²Ñ‹
            clean_word = re.sub(r'[^\w]', '', token.lower())
            if not clean_word:
                new_tokens.append(token)
                continue
                
            parsed = self.morph.parse(clean_word)[0]
            base = parsed.normal_form

            # Ð•Ð¡Ð›Ð˜ ÐžÐ¡ÐÐžÐ’Ð Ð•Ð¡Ð¢Ð¬ Ð’ Ð¡Ð˜Ð”Ð• - ÐœÐ•ÐÐ¯Ð•Ðœ ÐŸÐÐ”Ð•Ð– ÐÐ Ð¢ÐžÐ¢, Ð§Ð¢Ðž Ð’ Ð¡Ð˜Ð”Ð•
            if base in seed_bases:
                new_tokens.append(seed_bases[base])
            # Ð•Ð¡Ð›Ð˜ ÐÐ•Ð¢ (ÑÑ‚Ð¾ Ð³Ð¾Ñ€Ð¾Ð´, Ð¾Ñ‚Ð·Ñ‹Ð² Ð¸ Ñ‚.Ð´.) - ÐžÐ¡Ð¢ÐÐ’Ð›Ð¯Ð•Ðœ ÐšÐÐš Ð‘Ð«Ð›Ðž
            else:
                new_tokens.append(token)

        # Ð“ÐÐ ÐÐÐ¢Ð˜Ð¯: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ»Ð¾Ð² Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ðµ Ð’Ð¡Ð•Ð“Ð”Ð Ñ€Ð°Ð²Ð½Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ñƒ Ð½Ð° Ð²Ñ…Ð¾Ð´Ðµ
        return " ".join(new_tokens)

    def process_batch(self, keywords: List[str], golden_seed: str) -> List[str]:
        if not keywords or not golden_seed: return keywords
        
        print(f"ðŸ” Normalization IN: {len(keywords)} keywords, seed: '{golden_seed}'")
        
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡
        normalized = [self.normalize_by_golden_seed(kw, golden_seed) for kw in keywords]
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        empty_count = sum(1 for n in normalized if not n or not n.strip())
        if empty_count > 0:
            print(f"âš ï¸ ÐŸÐ£Ð¡Ð¢Ð«Ð• Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹: {empty_count} Ð¸Ð· {len(normalized)}")
        
        print(f"ðŸ” Normalization OUT: {len(normalized)} keywords")
        
        # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº (Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹)
        return normalized


# Global instance
_normalizer = None


def get_normalizer():
    global _normalizer
    if _normalizer is None:
        _normalizer = GoldenNormalizer()
    return _normalizer


def normalize_keywords(keywords: List[str], language: str = 'ru', seed: str = '') -> List[str]:
    if not seed:
        return keywords
    normalizer = get_normalizer()
    return normalizer.process_batch(keywords, seed)
