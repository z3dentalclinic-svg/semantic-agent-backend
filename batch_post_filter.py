"""
Batch Post-Filter v6.0 FINAL - FIXED VERSION
Authors: Gemini (original), Claude (fixes)
Date: 2026-01-11

–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï:
‚úÖ –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –≥–æ—Ä–æ–¥–∞ –¢–ï–ü–ï–†–¨ –ü–†–û–ü–£–°–ö–ê–Æ–¢–°–Ø –¥–ª—è country="UA"
‚úÖ –î–æ–±–∞–≤–ª–µ–Ω ignored_words –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π ("–¥–æ–º", "–º–∏—Ä")
‚úÖ –õ–æ–≥–∏–∫–∞: found_country == country.lower() ‚Üí –†–ê–ó–†–ï–®–ê–ï–ú

FEATURES:
- Batch processing (700 keywords ‚Üí 1 pass)
- N-gram city detection ("–Ω–∞–±–µ—Ä–µ–∂–Ω—ã–µ —á–µ–ª–Ω—ã")
- Extensible districts dictionary (–ß–∏–ª–∞–Ω–∑–∞—Ä, –£—Ä—É—á—å–µ)
- Hard-Blacklist priority (–ö—Ä—ã–º/–û–†–î–õ–û)
- Seed city allowance (if seed has Kiev ‚Üí allow Kiev in results)
- Detailed logging & Stats
"""

import re
import logging
import time
from typing import List, Dict, Set, Tuple, Optional
from collections import Counter

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger("BatchPostFilter")


class BatchPostFilter:
    def __init__(self, all_cities_global: Dict[str, str], forbidden_geo: Set[str], 
                 districts: Optional[Dict[str, str]] = None):
        """
        Args:
            all_cities_global: Dict {city_name: country_code} (lowercase)
            forbidden_geo: Set of forbidden locations (–ö—Ä—ã–º/–û–†–î–õ–û - lemmatized)
            districts: Optional Dict {district_name: country_code}
        """
        self.all_cities_global = all_cities_global
        self.forbidden_geo = forbidden_geo
        self.districts = districts or {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pymorphy3 (—Ç–æ—á–Ω–µ–µ —á–µ–º Natasha Morph)
        try:
            import pymorphy3
            self.morph_ru = pymorphy3.MorphAnalyzer(lang='ru')
            self.morph_uk = pymorphy3.MorphAnalyzer(lang='uk')
            self._has_morph = True
            logger.info("‚úÖ Pymorphy3 initialized for batch lemmatization")
        except ImportError:
            logger.error("‚ùå Pymorphy3 not found! Batch lemmatization will be skipped.")
            self._has_morph = False
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: Natasha NER –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ–≥–∏–æ–Ω–æ–≤
        try:
            from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsNERTagger, Doc
            self._segmenter = Segmenter()
            self._morph_vocab = MorphVocab()
            self._emb = NewsEmbedding()
            self._ner_tagger = NewsNERTagger(self._emb)
            self._has_natasha = True
            logger.info("‚úÖ Natasha NER initialized for region detection")
        except ImportError:
            logger.warning("‚ö†Ô∏è Natasha NER not found - will use only word-level checks")
            self._has_natasha = False

    def filter_batch(self, keywords: List[str], seed: str, country: str, 
                     language: str = 'ru') -> Dict:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–∞–∫–µ—Ç–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        
        Args:
            keywords: List of raw keywords from Google
            seed: Original seed phrase
            country: Target country code (ua, ru, by, kz)
            language: Language code (ru, uk, en)
        
        Returns:
            {
                'keywords': [...],  # Clean keywords
                'anchors': [...],   # Blocked keywords
                'stats': {...}      # Statistics
            }
        """
        start_time = time.time()
        
        # 1. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        unique_raw = sorted(list(set([k.lower().strip() for k in keywords if k.strip()])))
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ—Ä–æ–¥–∞ –∏–∑ seed (–¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è)
        seed_cities = self._extract_cities_from_seed(seed, country, language)
        logger.info(f"[BATCH-FILTER] Seed cities allowed: {seed_cities}")
        
        # 3. –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è Batch-–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        all_words = set()
        for kw in unique_raw:
            all_words.update(re.findall(r'[–∞-—è—ëa-z0-9-]+', kw))
        
        # 4. –û–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤
        lemmas_map = self._batch_lemmatize(all_words, language)
        
        final_keywords = []
        final_anchors = []
        stats = {
            'total': len(unique_raw),
            'allowed': 0,
            'blocked': 0,
            'reasons': Counter()
        }

        # 5. –§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞–∂–¥—ã–π keyword
        for kw in unique_raw:
            is_allowed, reason, category = self._check_geo_conflicts(
                kw, country, lemmas_map, seed_cities, language
            )
            
            if is_allowed:
                final_keywords.append(kw)
                stats['allowed'] += 1
                logger.debug(f"[POST-FILTER] ‚úÖ –†–ê–ó–†–ï–®–ï–ù–û: '{kw}'")
            else:
                final_anchors.append(kw)
                stats['blocked'] += 1
                stats['reasons'][category] += 1
                logger.warning(f"[POST-FILTER] ‚öì –Ø–ö–û–†–¨: '{kw}' (–ø—Ä–∏—á–∏–Ω–∞: {reason})")

        elapsed = time.time() - start_time
        logger.info(f"[BATCH-FILTER] Finished in {elapsed:.2f}s. {stats['allowed']} OK / {stats['blocked']} Anchors")

        return {
            'keywords': final_keywords,
            'anchors': final_anchors,
            'stats': {
                'total': stats['total'],
                'allowed': stats['allowed'],
                'blocked': stats['blocked'],
                'reasons': dict(stats['reasons']),
                'elapsed_time': round(elapsed, 2)
            }
        }

    def _extract_cities_from_seed(self, seed: str, country: str, language: str) -> Set[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ—Ä–æ–¥–∞ –∏–∑ seed –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
        
        –ü—Ä–∏–º–µ—Ä:
        seed = "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –∫–∏–µ–≤"
        country = "ua"
        
        Returns: {"–∫–∏–µ–≤", "kiev"}  # –í—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏—è
        """
        if not self._has_morph:
            return set()
        
        seed_cities = set()
        words = re.findall(r'[–∞-—è—ëa-z0-9-]+', seed.lower())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞
        for word in words:
            if word in self.all_cities_global:
                city_country = self.all_cities_global[word]
                if city_country == country.lower():
                    seed_cities.add(word)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–µ–º–º—É
            lemma = self._get_lemma(word, language)
            if lemma in self.all_cities_global:
                city_country = self.all_cities_global[lemma]
                if city_country == country.lower():
                    seed_cities.add(lemma)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∏–≥—Ä–∞–º–º—ã
        bigrams = self._extract_ngrams(words, 2)
        for bigram in bigrams:
            if bigram in self.all_cities_global:
                city_country = self.all_cities_global[bigram]
                if city_country == country.lower():
                    seed_cities.add(bigram)
        
        return seed_cities

    def _batch_lemmatize(self, words: Set[str], language: str) -> Dict[str, str]:
        """
        –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –û–î–ò–ù –†–ê–ó –¥–ª—è –≤—Å–µ–≥–æ –Ω–∞–±–æ—Ä–∞ —Å–ª–æ–≤ —á–µ—Ä–µ–∑ Pymorphy3
        
        Args:
            words: Set of unique words
            language: 'ru', 'uk', 'en'
        
        Returns:
            Dict {word: lemma}
        """
        if not self._has_morph:
            return {w: w for w in words}
        
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        lemmas = {}
        
        for word in words:
            lemma = self._get_lemma(word, language, morph)
            lemmas[word] = lemma
        
        logger.debug(f"[BATCH-FILTER] Lemmatized {len(words)} unique words")
        return lemmas

    def _get_lemma(self, word: str, language: str, morph=None) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –ª–µ–º–º—É —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ Pymorphy3"""
        if not self._has_morph:
            return word
        
        if morph is None:
            morph = self.morph_ru if language == 'ru' else self.morph_uk
        
        try:
            parsed = morph.parse(word)
            if parsed:
                return parsed[0].normal_form
        except:
            pass
        
        return word

    def _extract_ngrams(self, words: List[str], n: int = 2) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ n-–≥—Ä–∞–º–º (–±–∏–≥—Ä–∞–º–º) –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥–æ—Ä–æ–¥–æ–≤ —Ç–∏–ø–∞ '–Ω–∞–±–µ—Ä–µ–∂–Ω—ã–µ —á–µ–ª–Ω—ã'
        
        Args:
            words: List of words
            n: N-gram size (default 2 for bigrams)
        
        Returns:
            List of n-grams
        """
        if len(words) < n:
            return []
        
        return [" ".join(words[i:i+n]) for i in range(len(words) - n + 1)]

    def _check_geo_conflicts(self, keyword: str, country: str, 
                            lemmas_map: Dict[str, str], seed_cities: Set[str],
                            language: str) -> Tuple[bool, str, str]:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–µ–æ-–∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –ª–µ–º–º –∏ –±–∏–≥—Ä–∞–º–º
        
        –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï:
        - –î–æ–±–∞–≤–ª–µ–Ω ignored_words –¥–ª—è "–¥–æ–º", "–º–∏—Ä" –∏ —Ç.–¥.
        - –ì–æ—Ä–æ–¥–∞ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω—ã –ü–†–û–ü–£–°–ö–ê–Æ–¢–°–Ø: found_country == country.lower()
        
        Returns:
            (is_allowed, reason, category)
        """
        words = re.findall(r'[–∞-—è—ëa-z0-9-]+', keyword)
        if not words:
            return True, "", ""

        keyword_lemmas = [lemmas_map.get(w, w) for w in words]
        
        # --- 1. –ü–†–û–í–ï–†–ö–ê HARD-BLACKLIST (–ö—Ä—ã–º/–û–†–î–õ–û) - –ü–†–ò–û–†–ò–¢–ï–¢ #1 ---
        for check_val in words + keyword_lemmas:
            if check_val in self.forbidden_geo:
                return False, f"Hard-Blacklist '{check_val}'", "hard_blacklist"

        # --- 2. –ü–†–û–í–ï–†–ö–ê –†–ê–ô–û–ù–û–í (Extensible Districts) ---
        for w in words:
            if w in self.districts:
                dist_country = self.districts[w]
                if dist_country != country.lower():
                    return False, f"—Ä–∞–π–æ–Ω '{w}' ({dist_country})", "districts"

        # --- 3. –ü–†–û–í–ï–†–ö–ê –ì–û–†–û–î–û–í (N-Grams & Lookup) ---
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        search_items = []
        search_items.extend(keyword_lemmas)  # –õ–µ–º–º—ã (–º–æ—Å–∫–≤–∞, –∫–∏–µ–≤)
        search_items.extend(self._extract_ngrams(words, 2))  # –ë–∏–≥—Ä–∞–º–º—ã (–Ω–∞–±–µ—Ä–µ–∂–Ω—ã–µ —á–µ–ª–Ω—ã)
        search_items.extend(self._extract_ngrams(keyword_lemmas, 2))  # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–≥—Ä–∞–º–º—ã

        for item in search_items:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
            if len(item) < 3:
                continue
            
            found_country = self.all_cities_global.get(item)
            if found_country:
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–•: –†–ê–ó–†–ï–®–ê–ï–ú –µ—Å–ª–∏:
                # - –≠—Ç–æ –≥–æ—Ä–æ–¥ –¢–ï–ö–£–©–ï–ô —Å—Ç—Ä–∞–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–∫–∏–µ–≤' –¥–ª—è UA)
                # - –ò–õ–ò —ç—Ç–æ—Ç –≥–æ—Ä–æ–¥ –±—ã–ª –≤ –ø–æ–∏—Å–∫–æ–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ (seed)
                if found_country == country.lower() or item in seed_cities:
                    logger.debug(f"[POST-FILTER] City '{item}' ({found_country}) - ALLOWED (target country or seed)")
                    continue
                else:
                    # –ì–æ—Ä–æ–¥ –∏–∑ –ß–£–ñ–û–ô —Å—Ç—Ä–∞–Ω—ã - –±–ª–æ–∫–∏—Ä—É–µ–º
                    return False, f"{found_country.upper()} –≥–æ—Ä–æ–¥ '{item}'", f"{found_country}_cities"
        
        # --- 4. –ü–†–û–í–ï–†–ö–ê –ì–†–ê–ú–ú–ê–¢–ò–ß–ï–°–ö–û–ô –ü–†–ê–í–ò–õ–¨–ù–û–°–¢–ò ---
        if not self._is_grammatically_valid(keyword, language):
            return False, "–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞", "grammar"
        
        return True, "", ""

    def _is_grammatically_valid(self, keyword: str, language: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å keyword
        
        –ë–ª–æ–∫–∏—Ä—É–µ—Ç:
        - "—Ä–µ–º–æ–Ω—Ç–∞—Ö" (–ø—Ä–µ–¥–ª–æ–∂–Ω—ã–π –ø–∞–¥–µ–∂ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞)
        - "–æ —Ä–µ–º–æ–Ω—Ç–∞—Ö" (–Ω–µ–ø—Ä—è–º—ã–µ –ø–∞–¥–µ–∂–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ)
        """
        if not self._has_morph or language not in ['ru', 'uk']:
            return True
        
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        words = re.findall(r'[–∞-—è—ëa-z]+', keyword.lower())
        
        for word in words:
            try:
                parsed = morph.parse(word)
                if parsed:
                    tag = parsed[0].tag
                    
                    # –ë–ª–æ–∫–∏—Ä—É–µ–º –º–Ω–æ–∂–∏–Ω—É –≤ –Ω–µ–ø—Ä—è–º–∏—Ö –≤—ñ–¥–º—ñ–Ω–∫–∞—Ö
                    invalid_tags = {'datv', 'ablt', 'loct'}
                    if 'plur' in tag and any(bad in tag for bad in invalid_tags):
                        logger.debug(f"[POST-FILTER] Invalid grammar: '{word}' has {tag}")
                        return False
            except:
                pass
        
        return True


# ============================================
# EXTENSIBLE DISTRICTS - –ü–†–ò–ú–ï–†–´
# ============================================

DISTRICTS_MINSK = {
    "—É—Ä—É—á—å–µ": "by",
    "—à–∞–±–∞–Ω—ã": "by",
    "–∫–∞–º–µ–Ω–Ω–∞—è –≥–æ—Ä–∫–∞": "by",
    "—Å–µ—Ä–µ–±—Ä—è–Ω–∫–∞": "by"
}

DISTRICTS_TASHKENT = {
    "—á–∏–ª–∞–Ω–∑–∞—Ä": "uz",
    "—é–Ω—É—Å–∞–±–∞–¥": "uz",
    "—Å–µ—Ä–≥–µ–ª–∏": "uz",
    "—è–∫–∫–∞—Å–∞—Ä–∞–π": "uz"
}

DISTRICTS_EXTENDED = {**DISTRICTS_MINSK, **DISTRICTS_TASHKENT}


# ============================================
# EXAMPLE USAGE
# ============================================

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    
    # –ú–∏–Ω–∏-–±–∞–∑–∞ –≥–æ—Ä–æ–¥–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
    test_cities = {
        "–º–æ—Å–∫–≤–∞": "ru",
        "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥": "ru",
        "–Ω–∞–±–µ—Ä–µ–∂–Ω—ã–µ —á–µ–ª–Ω—ã": "ru",
        "–∫–∏–µ–≤": "ua",
        "–¥–Ω–µ–ø—Ä": "ua",
        "—Ö–∞—Ä—å–∫–æ–≤": "ua",
        "–∑–∞–ø–æ—Ä–æ–∂—å–µ": "ua",
        "–æ–¥–µ—Å—Å–∞": "ua",
        "–ª—å–≤–æ–≤": "ua",
        "–º–∏–Ω—Å–∫": "by",
        "—Ç–∞—à–∫–µ–Ω—Ç": "uz",
        "–¥–æ–º": "gh",  # –ì–∞–Ω–∞ - –¥–æ–ª–∂–µ–Ω –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è
    }
    
    # Hard-Blacklist
    test_forbidden = {
        "–∫—Ä—ã–º", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å", "—è–ª—Ç–∞",
        "–¥–æ–Ω–µ—Ü–∫", "–ª—É–≥–∞–Ω—Å–∫", "–≥–æ—Ä–ª–æ–≤–∫–∞"
    }
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä
    post_filter = BatchPostFilter(
        all_cities_global=test_cities,
        forbidden_geo=test_forbidden,
        districts=DISTRICTS_EXTENDED
    )
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_keywords = [
        # –î–æ–ª–∂–Ω—ã –ü–†–û–ü–£–°–¢–ò–¢–¨–°–Ø (UA –≥–æ—Ä–æ–¥–∞):
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –∫–∏–µ–≤",
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –¥–Ω–µ–ø—Ä",
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ —Ö–∞—Ä—å–∫–æ–≤",
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –∑–∞–ø–æ—Ä–æ–∂—å–µ",
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –æ–¥–µ—Å—Å–∞",
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –ª—å–≤–æ–≤",
        "–≤—ã–µ–∑–¥ –Ω–∞ –¥–æ–º",  # "–¥–æ–º" –¥–æ–ª–∂–µ–Ω –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è
        
        # –î–æ–ª–∂–Ω—ã –ë–õ–û–ö–ò–†–û–í–ê–¢–¨–°–Ø:
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –º–æ—Å–∫–≤–∞",  # RU –≥–æ—Ä–æ–¥
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –Ω–∞–±–µ—Ä–µ–∂–Ω—ã–µ —á–µ–ª–Ω—ã",  # RU –≥–æ—Ä–æ–¥ (–±–∏–≥—Ä–∞–º–º)
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ —Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å",  # Hard-Blacklist
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ —á–∏–ª–∞–Ω–∑–∞—Ä",  # –†–∞–π–æ–Ω UZ
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ —É—Ä—É—á—å–µ"  # –†–∞–π–æ–Ω BY
    ]
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º
    result = post_filter.filter_batch(
        keywords=test_keywords,
        seed="—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤",  # –ë–ï–ó –≥–æ—Ä–æ–¥–∞ –≤ seed
        country="ua",
        language="ru"
    )
    
    print("\n" + "="*60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –§–ò–õ–¨–¢–†–ê–¶–ò–ò (FIXED VERSION):")
    print("="*60)
    print(f"\n‚úÖ –†–ê–ó–†–ï–®–ï–ù–û ({len(result['keywords'])}):")
    for kw in result['keywords']:
        print(f"  - {kw}")
    
    print(f"\n‚öì –Ø–ö–û–†–Ø ({len(result['anchors'])}):")
    for kw in result['anchors']:
        print(f"  - {kw}")
    
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"  Total: {result['stats']['total']}")
    print(f"  Allowed: {result['stats']['allowed']}")
    print(f"  Blocked: {result['stats']['blocked']}")
    print(f"  Reasons: {result['stats']['reasons']}")
    print(f"  Time: {result['stats']['elapsed_time']}s")
    print("="*60 + "\n")
