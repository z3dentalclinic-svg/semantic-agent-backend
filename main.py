"""
GEMINI BIGRAM TEST - –î–≤—É—Ö–±—É–∫–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ "—Å–µ —Ä–µ–º–æ–Ω—Ç" ‚Üí "—Å–µ—Ä–≤–∏—Å–Ω—ã–π —Ä–µ–º–æ–Ω—Ç"?
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List
import os
import httpx
import asyncio
import time
import random

app = FastAPI(title="Gemini Bigram Test", version="1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AutocompleteParser:
    def __init__(self):
        self.base_url = "https://suggestqueries.google.com/complete/search"
    
    async def fetch_suggestions(self, query: str, country: str, language: str) -> List[str]:
        params = {"client": "chrome", "q": query, "gl": country, "hl": language}
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(self.base_url, params=params)
                if response.status_code == 200:
                    data = response.json()
                    if isinstance(data, list) and len(data) > 1:
                        return [s for s in data[1] if isinstance(s, str)]
                return []
        except Exception as e:
            print(f"Error: {e}")
            return []
    
    async def gemini_bigram_test(self, seed: str, country: str, language: str) -> List[str]:
        all_keywords = set()
        
        print(f"\n{'='*60}")
        print(f"üî¨ GEMINI BIGRAM TEST")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"–ì–∏–ø–æ—Ç–µ–∑–∞: '—Å–µ —Ä–µ–º–æ–Ω—Ç' ‚Üí '—Å–µ—Ä–≤–∏—Å–Ω—ã–π —Ä–µ–º–æ–Ω—Ç'\n")
        
        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ü–ï–†–í–û–ï —Å–ª–æ–≤–æ –∏–∑ seed
        first_word = seed.split()[0]
        print(f"–ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ: '{first_word}'\n")
        
        # ========================================
        # –≠–¢–ê–ü 1: –¢–µ—Å—Ç —Ç–æ–ø-20 –±–∏–≥—Ä–∞–º–º
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 1: –¢–æ–ø-20 –±–∏–≥—Ä–∞–º–º (–±—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç)")
        print(f"{'='*60}\n")
        
        # –¢–æ–ø —á–∞—Å—Ç–æ—Ç–Ω—ã–µ –±–∏–≥—Ä–∞–º–º—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        top_bigrams = {
            "—Å–µ": "—Å–µ—Ä–≤–∏—Å, —Å–µ—Ä–≤–∏—Å–Ω—ã–π",
            "—Å—Ä": "—Å—Ä–æ—á–Ω—ã–π",
            "–≥–¥": "–≥–¥–µ",
            "–º–∞": "–º–∞—Å—Ç–µ—Ä, –º–∞—Å—Ç–µ—Ä—Å–∫–∞—è",
            "–Ω–µ": "–Ω–µ–¥–æ—Ä–æ–≥–æ–π",
            "–¥–µ": "–¥–µ—à–µ–≤—ã–π",
            "–ø—Ä": "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π",
            "–∫–∞": "–∫–∞–∫, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π",
            "—Å–ª": "—Å–ª–æ–∂–Ω—ã–π",
            "–æ—Ç": "–æ—Ç–ª–∏—á–Ω—ã–π",
            "—Å–∫": "—Å–∫–æ–ª—å–∫–æ",
            "—Å–æ": "—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π",
            "—Ü–µ": "—Ü–µ–Ω—Ç—Ä, —Ü–µ–Ω–∞",
            "—á–∞": "—á–∞—Å—Ç–Ω—ã–π",
            "–∫–æ": "–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π",
            "–º–µ": "–º–µ–ª–∫–∏–π",
            "–±–µ": "–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π",
            "—Ä–µ": "—Ä–µ–º–æ–Ω—Ç",
            "–Ω–∞": "–Ω–∞–¥–µ–∂–Ω—ã–π",
            "–∫—É": "–∫—É–ø–∏—Ç—å"
        }
        
        discovered_words = set()
        total_queries = 0
        
        for bigram, expected in top_bigrams.items():
            # –ë–∏–≥—Ä–∞–º–º–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            query = f"{bigram} {first_word}"
            results = await self.fetch_suggestions(query, country, language)
            total_queries += 1
            
            print(f"'{query}' (–æ–∂–∏–¥–∞–µ–º: {expected})")
            print(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            
            if len(results) == 0:
                print(f"  ‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n")
                continue
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            found_expansions = []
            
            for result in results:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å –±–∏–≥—Ä–∞–º–º—ã
                if result.lower().startswith(bigram.lower()):
                    # –£–±–∏—Ä–∞–µ–º –±–∏–≥—Ä–∞–º–º—É
                    after_bigram = result[len(bigram):].strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –Ω–∞—à–µ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
                    if first_word.lower() in after_bigram.lower():
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á—Ç–æ –º–µ–∂–¥—É –±–∏–≥—Ä–∞–º–º–æ–π –∏ first_word
                        word_pos = after_bigram.lower().find(first_word.lower())
                        if word_pos > 0:
                            expanded_word = after_bigram[:word_pos].strip()
                            if expanded_word:
                                found_expansions.append(expanded_word)
                                discovered_words.add(expanded_word)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if len(found_expansions) > 0:
                print(f"  ‚úÖ –ù–ê–ô–î–ï–ù–´ –†–ê–°–®–ò–†–ï–ù–ò–Ø:")
                for word in set(found_expansions):
                    print(f"     üéØ '{word}'")
                    for r in results:
                        if word in r:
                            print(f"        –ü—Ä–∏–º–µ—Ä: {r}")
                            break
            else:
                print(f"  ‚ùå –†–∞—Å—à–∏—Ä–µ–Ω–∏—è –ù–ï –Ω–∞–π–¥–µ–Ω—ã")
                print(f"  –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
                for r in results[:3]:
                    print(f"     ‚Ä¢ {r}")
            
            print()
            await asyncio.sleep(random.uniform(0.3, 0.8))
        
        print(f"{'='*60}")
        print(f"‚úÖ –≠–¢–ê–ü 1 –∑–∞–≤–µ—Ä—à—ë–Ω")
        print(f"{'='*60}")
        print(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {total_queries}")
        print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π: {len(discovered_words)}")
        
        if len(discovered_words) > 0:
            print(f"\nüéâ –ë–ò–ì–†–ê–ú–ú–´ –†–ê–ë–û–¢–ê–Æ–¢!")
            print(f"–ù–∞–π–¥–µ–Ω—ã —Å–ª–æ–≤–∞:\n")
            for word in sorted(discovered_words):
                print(f"  ‚Ä¢ {word}")
            
            # ========================================
            # –≠–¢–ê–ü 2: –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø–æ–ª–Ω—ã–º seed
            # ========================================
            print(f"\n{'='*60}")
            print(f"–≠–¢–ê–ü 2: –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è PREFIX")
            print(f"{'='*60}\n")
            
            verified_keywords = set()
            
            for word in sorted(discovered_words):
                full_query = f"{word} {seed}"
                results = await self.fetch_suggestions(full_query, country, language)
                total_queries += 1
                
                if len(results) > 0:
                    verified_keywords.update(results)
                    all_keywords.update(results)
                    print(f"‚úÖ '{full_query}' ‚Üí {len(results)} –∫–ª—é—á–µ–π")
                    for r in results[:3]:
                        print(f"    ‚Ä¢ {r}")
                else:
                    print(f"‚ùå '{full_query}' ‚Üí –Ω–µ—Ç")
                
                await asyncio.sleep(random.uniform(0.3, 0.8))
            
            print(f"\n{'='*60}")
            print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
            print(f"{'='*60}")
            print(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {total_queries}")
            print(f"–ù–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤: {len(discovered_words)}")
            print(f"–í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö PREFIX: {len(verified_keywords)}")
            
            if "—Å–µ—Ä–≤–∏—Å" in discovered_words or "—Å—Ä–æ—á–Ω—ã–π" in discovered_words:
                print(f"\nüéØ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê! –ù–∞—à–ª–∏ '—Å–µ—Ä–≤–∏—Å' –∏–ª–∏ '—Å—Ä–æ—á–Ω—ã–π'!")
            
        else:
            print(f"\n‚ùå –ë–ò–ì–†–ê–ú–ú–´ –ù–ï –†–ê–ë–û–¢–ê–Æ–¢!")
            print(f"Google –ù–ï —Ä–∞—Å—à–∏—Ä—è–µ—Ç –¥–≤—É—Ö–±—É–∫–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã")
            print(f"–ú–µ—Ç–æ–¥ –æ—Ç Gemini –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º")
        
        print(f"\n{'='*60}")
        print(f"–ò–¢–û–ì–û –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"{'='*60}\n")
        
        return list(all_keywords)


@app.get("/api/test-parser/gemini-bigram")
async def test_gemini_bigram(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru")
):
    parser = AutocompleteParser()
    start = time.time()
    keywords = await parser.gemini_bigram_test(seed, country, language)
    return {
        "seed": seed,
        "method": "Gemini Bigram",
        "keywords": keywords,
        "count": len(keywords),
        "time": round(time.time() - start, 2)
    }


@app.get("/")
async def root():
    return {
        "api": "Gemini Bigram Test",
        "url": "/api/test-parser/gemini-bigram?seed=—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
