"""
Batch Post-Filter v8.0 - TWO-LEVEL GEO DATABASE SUPPORT
Based on Gemini's recommendations for 187 countries support

üéØ –ù–û–í–û–ï –í v8.0:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
–î–í–£–•–£–†–û–í–ù–ï–í–ê–Ø –ë–ê–ó–ê –ì–û–†–û–î–û–í:
  Level 1: Cities >15k (global) - ~158k –Ω–∞–∑–≤–∞–Ω–∏–π
  Level 2: Cities >1k (CIS: BY, KZ, RU, PL, LT, LV, EE) - +27k
  
–†–ï–ó–£–õ–¨–¢–ê–¢:
  - –ñ–¥–∞–Ω–æ–≤–∏—á–∏ (BY, 7k) ‚úÖ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è
  - –ë–∞—Ä–∞–Ω–æ–≤–∏—á–∏ (BY, 170k) ‚úÖ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è
  - +85% –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–ª—è –ë–µ–ª–∞—Ä—É—Å–∏
  - –í—Å–µ–≥–æ: ~183k –≥–æ—Ä–æ–¥–æ–≤
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üî• –§–£–ù–î–ê–ú–ï–ù–¢–ê–õ–¨–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï v7.9:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
–ü–†–û–ë–õ–ï–ú–ê v7.7-v7.8:
  –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (_is_common_noun) –ø—Ä–æ–≤–µ—Ä—è–ª–∞—Å—å –î–û –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
  ‚Üí "–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏" = NOUN ‚Üí –ø—Ä–æ–ø—É—Å–∫–∞–ª—Å—è
  ‚Üí "–ª–æ—à–∏—Ü–∞" = NOUN ‚Üí –ø—Ä–æ–ø—É—Å–∫–∞–ª—Å—è
  
–†–ï–®–ï–ù–ò–ï v7.9:
  –ë–∞–∑–∞ –≥–æ—Ä–æ–¥–æ–≤ = –ü–ï–†–í–ò–ß–ù–ê
  –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è = –í–¢–û–†–ò–ß–ù–ê (—Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ª–æ–≤ –í–ù–ï –±–∞–∑—ã)
  
  –ù–û–í–´–ô –ê–õ–ì–û–†–ò–¢–ú:
  1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ª–µ–º–º–∞)
  2. –ü–æ–∏—Å–∫ –≤ all_cities_global
  3. –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –ò country != target ‚Üí –ë–õ–û–ö (–ë–ï–ó –ø—Ä–æ–≤–µ—Ä–∫–∏ NOUN!)
  4. –ï—Å–ª–∏ –ù–ï –Ω–∞–π–¥–µ–Ω–æ ‚Üí –ø—Ä–æ–≤–µ—Ä—è–µ–º _is_common_noun
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢ v7.9:
  - "–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏" (BY) ‚Üí –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ ‚Üí –ë–õ–û–ö ‚öì
  - "—Ç–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω" (KZ) ‚Üí –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ ‚Üí –ë–õ–û–ö ‚öì
  - "–¥–æ–º" (Ghana) ‚Üí –ù–ï –Ω–∞–π–¥–µ–Ω–æ ‚Üí _is_common_noun ‚Üí —Ä–∞–∑—Ä–µ—à–µ–Ω–æ ‚úÖ

–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø v7.7:
üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ü–ï–†–ï–î –ø–æ–∏—Å–∫–æ–º –≤ –±–∞–∑–µ –≥–æ—Ä–æ–¥–æ–≤
üî• –ò–°–ü–†–ê–í–õ–ï–ù–û: –í—Å–µ —Å–∫–ª–æ–Ω—ë–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è ("–≤ –∞–∫—Ç–æ–±–µ" ‚Üí "–∞–∫—Ç–æ–±–µ")
‚úÖ –ê–∫—Ç–æ–±–µ, –§–∞–Ω–∏–ø–æ–ª—å, –û—à–º—è–Ω—ã —Ç–µ–ø–µ—Ä—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –±–ª–æ–∫–∏—Ä—É—é—Ç—Å—è

–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –£–õ–£–ß–®–ï–ù–ò–Ø v7.6:
‚úÖ Population filter (> 5000) - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –º–∞–ª—ã–µ —Å—ë–ª–∞-—Ç—ë–∑–∫–∏
‚úÖ Smart disambiguation —á–µ—Ä–µ–∑ Pymorphy3 (NOUN vs Geox)
‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è N-gram detection
‚úÖ –ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –¥–ª—è –ª—é–±–æ–π –∏–∑ 187 —Å—Ç—Ä–∞–Ω
‚úÖ O(1) lookup —á–µ—Ä–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å
‚úÖ –†—É—á–Ω–æ–π —Å–ª–æ–≤–∞—Ä—å –º–∞–ª—ã—Ö –≥–æ—Ä–æ–¥–æ–≤ –°–ù–ì (–æ—à, —É–∑—ã–Ω–∞–≥–∞—à, —â–µ–ª–∫–∏–Ω–æ)

FIXES v7.9 ‚Üí v8.0:
- –†–∞—Å—à–∏—Ä–µ–Ω–∞ –±–∞–∑–∞ –≥–æ—Ä–æ–¥–æ–≤: +27k –º–∞–ª—ã—Ö –≥–æ—Ä–æ–¥–æ–≤ –°–ù–ì
- –ü–æ–∫—Ä—ã—Ç–∏–µ BY —É–≤–µ–ª–∏—á–µ–Ω–æ –Ω–∞ 85% (971 ‚Üí 1,796 –Ω–∞–∑–≤–∞–Ω–∏–π)
- –ñ–¥–∞–Ω–æ–≤–∏—á–∏, –°–µ—Ä–µ–±—Ä—è–Ω–∫–∞ –∏ –¥—Ä—É–≥–∏–µ –º–∞–ª—ã–µ –≥–æ—Ä–æ–¥–∞ —Ç–µ–ø–µ—Ä—å –Ω–∞—Ö–æ–¥—è—Ç—Å—è
- –õ–æ–≥–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ –∏–∑–º–µ–Ω–µ–Ω–∞ (—Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å v7.9)
"""

import re
import logging
import time
from typing import List, Dict, Set, Tuple, Optional
from collections import Counter

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger("BatchPostFilter")


class BatchPostFilter:
    def __init__(self, 
                 all_cities_global: Dict[str, str], 
                 forbidden_geo: Set[str], 
                 districts: Optional[Dict[str, str]] = None,
                 population_threshold: int = 5000):
        """
        v7.5 Constructor with population filtering
        
        Args:
            all_cities_global: Dict {city_name: country_code} (lowercase)
            forbidden_geo: Set of forbidden locations (–ö—Ä—ã–º/–û–†–î–õ–û - lemmatized)
            districts: Optional Dict {district_name: country_code}
            population_threshold: Minimum city population to consider (default: 5000)
        """
        self.forbidden_geo = forbidden_geo
        self.districts = districts or {}
        self.population_threshold = population_threshold
        
        # v7.5: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
        self.city_abbreviations = self._get_city_abbreviations()
        self.regions = self._get_regions()
        self.countries = self._get_countries()
        self.manual_small_cities = self._get_manual_small_cities()  # v7.6: –ú–∞–ª—ã–µ –≥–æ—Ä–æ–¥–∞ –°–ù–ì
        
        # v7.6: Ignored words - –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞ –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —è–≤–ª—è—é—Ç—Å—è –≥–æ—Ä–æ–¥–∞–º–∏
        # –î–∞–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å –≤ –±–∞–∑–µ geonamescache
        self.ignored_words = {
            "–¥–æ–º",      # Ghana (GH) - "–≤—ã–µ–∑–¥ –Ω–∞ –¥–æ–º"
            "–º–∏—Ä",      # Russia villages - "–º–∏—Ä —Ü–µ–Ω"
            "–±–æ—Ä",      # Serbia - "—Å–æ—Å–Ω–æ–≤—ã–π –±–æ—Ä"  
            "–Ω–∏–≤–∞",     # Villages - "–∞–≤—Ç–æ–º–æ–±–∏–ª—å –Ω–∏–≤–∞"
            "–±–∞–ª–∫–∞",    # Villages - "–æ–≤—Ä–∞–∂–Ω–∞—è –±–∞–ª–∫–∞"
            "–ª—É—á",      # Villages - "—Å–æ–ª–Ω–µ—á–Ω—ã–π –ª—É—á"
            "—Å–ø—É—Ç–Ω–∏–∫",  # Villages - "—Å–ø—É—Ç–Ω–∏–∫–æ–≤–æ–µ —Ç–≤"
            "—Ä–∞–±–æ—Ç–∞",   # –ú–æ–∂–µ—Ç –±—ã—Ç—å –≥–æ—Ä–æ–¥–æ–º - "–∏—â—É —Ä–∞–±–æ—Ç—É"
            "—Ü–µ–Ω–∞",     # –ú–æ–∂–µ—Ç –±—ã—Ç—å –≥–æ—Ä–æ–¥–æ–º - "–ª—É—á—à–∞—è —Ü–µ–Ω–∞"
            "–≤—ã–µ–∑–¥",    # –ú–æ–∂–µ—Ç –±—ã—Ç—å –≥–æ—Ä–æ–¥–æ–º - "–≤—ã–µ–∑–¥ –º–∞—Å—Ç–µ—Ä–∞"
        }
        
        # v7.5: –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å —Å —É—á—ë—Ç–æ–º –Ω–∞—Å–µ–ª–µ–Ω–∏—è
        self.all_cities_global = self._build_filtered_geo_index()
        
        # v7.6: –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –õ–û–ì - –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –û—à–º—è–Ω—ã –∏ –§–∞–Ω–∏–ø–æ–ª—å –≤ –∏–Ω–¥–µ–∫—Å–µ
        # –ò—â–µ–º –ª—é–±—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π —ç—Ç–∏—Ö –≥–æ—Ä–æ–¥–æ–≤
        test_patterns = ['oshmyan', 'fanipal', 'fanipol']  # –ª–∞—Ç–∏–Ω–∏—Ü–∞ - –Ω–∞–¥—ë–∂–Ω–µ–µ
        found_test = {}
        for key, val in self.all_cities_global.items():
            if any(pattern in key for pattern in test_patterns):
                found_test[key] = val
                if len(found_test) >= 10:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –≤—ã–≤–æ–¥
                    break
        
        logger.warning(f"üîç v7.6 DEBUG: Cities matching 'oshmyan/fanipal': {found_test}")
        logger.warning(f"üîç v7.6 DEBUG: Total index size: {len(self.all_cities_global)} entries")
        logger.warning(f"üîç v7.6 DEBUG: Sample keys (first 10): {list(self.all_cities_global.keys())[:10]}")
        
        # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô DEBUG v7.7 - –ü–†–û–í–ï–†–ö–ê –ü–†–û–ë–õ–ï–ú–ù–´–• –ì–û–†–û–î–û–í
        logger.error("="*60)
        logger.error("üî• v7.7 CRITICAL DEBUG - CHECKING PROBLEM CITIES")
        logger.error("="*60)
        logger.error(f"üî• Dict size: {len(self.all_cities_global)} cities")
        
        test_problem_cities = {
            '–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏': 'by',
            'baranaviƒçy': 'by', 
            'baranovichi': 'by',
            '–∞–∫—Ç–æ–±–µ': 'kz',
            'aktobe': 'kz',
            'aqtobe': 'kz',
            '–≥—Ä–æ–∑–Ω—ã–π': 'ru',
            'grozny': 'ru',
            'groznyy': 'ru',
            '—Ç–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω': 'kz',
            'taldykorgan': 'kz',
            '—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫': 'kz',
            'oskemen': 'kz'
        }
        
        for city, expected in test_problem_cities.items():
            in_dict = city in self.all_cities_global
            actual = self.all_cities_global.get(city, 'NOT_FOUND')
            status = "‚úÖ" if in_dict else "‚ùå"
            logger.error(f"{status} '{city}': in_dict={in_dict}, value={actual}, expected={expected}")
        
        logger.error("="*60)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pymorphy3
        try:
            import pymorphy3
            self.morph_ru = pymorphy3.MorphAnalyzer(lang='ru')
            self.morph_uk = pymorphy3.MorphAnalyzer(lang='uk')
            self._has_morph = True
            logger.info("‚úÖ Pymorphy3 initialized for v7.7")
        except ImportError:
            logger.error("‚ùå Pymorphy3 not found!")
            self._has_morph = False
    
    def _get_city_abbreviations(self) -> Dict[str, str]:
        """–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤"""
        return {
            # –†–§
            '–µ–∫–±': 'ru', '–µ–∫–∞—Ç': 'ru',  # –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥
            '—Å–ø–±': 'ru', '–ø–∏—Ç–µ—Ä': 'ru',  # –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥
            '–º—Å–∫': 'ru',  # –ú–æ—Å–∫–≤–∞
            '–Ω—Å–∫': 'ru',  # –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫
            '–Ω–Ω': 'ru', '–Ω–Ω–æ–≤': 'ru',  # –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥
            '–≤–ª–∞–¥': 'ru',  # –í–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫
            '—Ä–æ—Å—Ç–æ–≤': 'ru',  # –†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É
            '–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä': 'ru',
            
            # BY
            '–º–Ω': 'by',  # –ú–∏–Ω—Å–∫
            
            # KZ
            '–∞–ª–º–∞—Ç—ã': 'kz',
            '–∞—Å—Ç–∞–Ω–∞': 'kz',
            
            # UZ
            '—Ç–∞—à–∫–µ–Ω—Ç': 'uz',
        }
    
    def _get_regions(self) -> Dict[str, str]:
        """–†–µ–≥–∏–æ–Ω—ã –†–§, BY, KZ, UZ"""
        return {
            # –†–§ —Ä–µ–≥–∏–æ–Ω—ã/—Ä–µ—Å–ø—É–±–ª–∏–∫–∏
            '–∏–Ω–≥—É—à–µ—Ç–∏—è': 'ru',
            '—á–µ—á–Ω—è': 'ru', '—á–µ—á–µ–Ω—Å–∫–∞—è —Ä–µ—Å–ø—É–±–ª–∏–∫–∞': 'ru',
            '–¥–∞–≥–µ—Å—Ç–∞–Ω': 'ru',
            '—Ç–∞—Ç–∞—Ä—Å—Ç–∞–Ω': 'ru',
            '–±–∞—à–∫–æ—Ä—Ç–æ—Å—Ç–∞–Ω': 'ru',
            '—É–¥–º—É—Ä—Ç–∏—è': 'ru',
            '–º–æ—Ä–¥–æ–≤–∏—è': 'ru',
            '–º–∞—Ä–∏–π —ç–ª': 'ru',
            '—á—É–≤–∞—à–∏—è': 'ru',
            '—è–∫—É—Ç–∏—è': 'ru', '—Å–∞—Ö–∞': 'ru',
            '–±—É—Ä—è—Ç–∏—è': 'ru',
            '—Ç—ã–≤–∞': 'ru',
            '—Ö–∞–∫–∞—Å–∏—è': 'ru',
            '–∞–ª—Ç–∞–π': 'ru',
            '–∫–∞—Ä–µ–ª–∏—è': 'ru',
            '–∫–æ–º–∏': 'ru',
            '–∫–∞–ª–º—ã–∫–∏—è': 'ru',
            '–∞–¥—ã–≥–µ—è': 'ru',
            '–∫–∞–±–∞—Ä–¥–∏–Ω–æ-–±–∞–ª–∫–∞—Ä–∏—è': 'ru',
            '–∫–∞—Ä–∞—á–∞–µ–≤–æ-—á–µ—Ä–∫–µ—Å–∏—è': 'ru',
            '—Å–µ–≤–µ—Ä–Ω–∞—è –æ—Å–µ—Ç–∏—è': 'ru',
            '–∫—Ä—ã–º': 'ru',  # –ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏ —Å–ø–æ—Ä–Ω–æ, –Ω–æ –≤ –±–∞–∑–µ –∫–∞–∫ RU
            
            # –†–§ –æ–±–ª–∞—Å—Ç–∏
            '–º–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru',
            '–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru',
            '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru',
            '—Å–≤–µ—Ä–¥–ª–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'ru',
            
            # BY –æ–±–ª–∞—Å—Ç–∏
            '–º–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–≥–æ–º–µ–ª—å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–º–æ–≥–∏–ª–µ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–≤–∏—Ç–µ–±—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–≥—Ä–æ–¥–Ω–µ–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            '–±—Ä–µ—Å—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'by',
            
            # KZ –æ–±–ª–∞—Å—Ç–∏
            '–∞–ª–º–∞—Ç–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'kz',
            '—é–∂–Ω–æ-–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'kz',
            
            # UZ –æ–±–ª–∞—Å—Ç–∏
            '—Ç–∞—à–∫–µ–Ω—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'uz',
            '—Å–∞–º–∞—Ä–∫–∞–Ω–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å': 'uz',
        }
    
    def _get_countries(self) -> Dict[str, str]:
        """–ù–∞–∑–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω (–¥–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ "–≤ –∏–∑—Ä–∞–∏–ª–µ")"""
        return {
            '—Ä–æ—Å—Å–∏—è': 'ru', '—Ä–æ—Å—Å–∏–∏': 'ru', '—Ä—Ñ': 'ru',
            '–±–µ–ª–∞—Ä—É—Å—å': 'by', '–±–µ–ª–æ—Ä—É—Å—Å–∏—è': 'by',
            '–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω': 'kz', '–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ': 'kz',
            '—É–∑–±–µ–∫–∏—Å—Ç–∞–Ω': 'uz', '—É–∑–±–µ–∫–∏—Å—Ç–∞–Ω–µ': 'uz',
            '—É–∫—Ä–∞–∏–Ω–∞': 'ua', '—É–∫—Ä–∞–∏–Ω–µ': 'ua',
            '–∏–∑—Ä–∞–∏–ª—å': 'il', '–∏–∑—Ä–∞–∏–ª–µ': 'il',
            '–ø–æ–ª—å—à–∞': 'pl', '–ø–æ–ª—å—à–µ': 'pl',
            '–≥–µ—Ä–º–∞–Ω–∏—è': 'de', '–≥–µ—Ä–º–∞–Ω–∏–∏': 'de',
            '—Å—à–∞': 'us', '–∞–º–µ—Ä–∏–∫–∞': 'us', '–∞–º–µ—Ä–∏–∫–µ': 'us',
        }
    
    def _get_manual_small_cities(self) -> Dict[str, str]:
        """
        v7.6: –†—É—á–Ω–æ–π —Å–ª–æ–≤–∞—Ä—å –º–∞–ª—ã—Ö –≥–æ—Ä–æ–¥–æ–≤ –°–ù–ì
        –ì–æ—Ä–æ–¥–∞ —Å –Ω–∞—Å–µ–ª–µ–Ω–∏–µ–º < 5000 –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è (< 3 —Å–∏–º–≤–æ–ª–∞)
        –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –æ—Å–Ω–æ–≤–Ω—É—é –±–∞–∑—É geonamescache
        """
        return {
            # –ö–æ—Ä–æ—Ç–∫–∏–µ –≥–æ—Ä–æ–¥–∞ (< 3 —Å–∏–º–≤–æ–ª–∞)
            '–æ—à': 'kg',  # –û—à, –ö–∏—Ä–≥–∏–∑–∏—è
            
            # –ú–∞–ª—ã–µ –≥–æ—Ä–æ–¥–∞ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞
            '—É–∑—ã–Ω–∞–≥–∞—à': 'kz',  # –£–∑—ã–Ω–∞–≥–∞—à, –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω
            
            # –ú–∞–ª—ã–µ –≥–æ—Ä–æ–¥–∞ –ö—Ä—ã–º–∞ (–æ–∫–∫—É–ø–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è)
            '—â–µ–ª–∫–∏–Ω–æ': 'ru',  # –©—ë–ª–∫–∏–Ω–æ, –ö—Ä—ã–º
            '—â—ë–ª–∫ino': 'ru',
            
            # –î—Ä—É–≥–∏–µ –º–∞–ª—ã–µ –≥–æ—Ä–æ–¥–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ—è–≤–∏—Ç—å—Å—è
            '–π–æ—Ç–∞': 'unknown',  # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –≥–æ—Ä–æ–¥/–±—Ä–µ–Ω–¥ - –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
        }
    
    def _build_filtered_geo_index(self) -> Dict[str, str]:
        """
        v7.5: –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å –≥–æ—Ä–æ–¥–æ–≤ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –Ω–∞—Å–µ–ª–µ–Ω–∏—é
        
        –≠—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É "–¥–æ–º" (Ghana), "–º–∏—Ä" –∏ —Ç.–¥.
        –ú–∞–ª—ã–µ —Å—ë–ª–∞ —Å –Ω–∞—Å–µ–ª–µ–Ω–∏–µ–º < 5000 –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è.
        """
        try:
            import geonamescache
            gc = geonamescache.GeonamesCache()
            cities = gc.get_cities()
            
            filtered_index = {}
            total_cities = 0
            filtered_out = 0
            
            for city_id, city_data in cities.items():
                country = city_data['countrycode'].lower()
                population = city_data.get('population', 0)
                
                # v7.5: –§–ò–õ–¨–¢–† –ü–û –ù–ê–°–ï–õ–ï–ù–ò–Æ
                if population < self.population_threshold:
                    filtered_out += 1
                    continue
                
                name = city_data['name'].lower().strip()
                filtered_index[name] = country
                total_cities += 1
                
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                for alt in city_data.get('alternatenames', []):
                    # v7.6: –û—Å—Ç–∞–≤–ª—è–µ–º –º–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞
                    if not (3 <= len(alt) <= 50):
                        continue
                    if not any(c.isalpha() for c in alt):
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ª–∞—Ç–∏–Ω–∏—Ü—É/–∫–∏—Ä–∏–ª–ª–∏—Ü—É (—Å –ø—Ä–æ–±–µ–ª–∞–º–∏!)
                    is_latin_cyrillic = all(
                        ('\u0000' <= c <= '\u007F') or
                        ('\u0400' <= c <= '\u04FF') or
                        c in ['-', "'", ' ']  # v7.5: –î–æ–±–∞–≤–∏–ª–∏ –ø—Ä–æ–±–µ–ª!
                        for c in alt
                    )
                    
                    if is_latin_cyrillic:
                        alt_lower = alt.lower().strip()
                        if alt_lower not in filtered_index:
                            filtered_index[alt_lower] = country
                            # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç —Å –¥–µ—Ñ–∏—Å–æ–º
                            alt_dash = alt_lower.replace(' ', '-')
                            if alt_dash != alt_lower:
                                filtered_index[alt_dash] = country
            
            logger.info(f"‚úÖ v7.7 Geo Index built:")
            logger.info(f"   Cities with pop > {self.population_threshold}: {total_cities}")
            logger.info(f"   Total index entries (with alts): {len(filtered_index)}")
            logger.info(f"   Filtered out (pop < {self.population_threshold}): {filtered_out}")
            
            return filtered_index
            
        except ImportError:
            logger.warning("‚ö†Ô∏è geonamescache not found, using fallback minimal dict")
            # Fallback –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            return {
                '–º–æ—Å–∫–≤–∞': 'ru', '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥': 'ru', 
                '–∫–∏–µ–≤': 'ua', '—Ö–∞—Ä—å–∫–æ–≤': 'ua', '–æ–¥–µ—Å—Å–∞': 'ua',
                '–º–∏–Ω—Å–∫': 'by', '–∞–ª–º–∞—Ç—ã': 'kz', '—Ç–∞—à–∫–µ–Ω—Ç': 'uz'
            }

    def filter_batch(self, keywords: List[str], seed: str, country: str, 
                     language: str = 'ru') -> Dict:
        """
        v7.5 Batch filtering with smart disambiguation
        """
        start_time = time.time()
        
        # 1. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        unique_raw = sorted(list(set([k.lower().strip() for k in keywords if k.strip()])))
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ—Ä–æ–¥–∞ –∏–∑ seed
        seed_cities = self._extract_cities_from_seed(seed, country, language)
        logger.info(f"[v7.7] Seed cities allowed: {seed_cities}")
        
        # 3. Batch –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        all_words = set()
        for kw in unique_raw:
            all_words.update(re.findall(r'[–∞-—è—ëa-z0-9-]+', kw))
        
        lemmas_map = self._batch_lemmatize(all_words, language)
        
        final_keywords = []
        final_anchors = []
        stats = {
            'total': len(unique_raw),
            'allowed': 0,
            'blocked': 0,
            'reasons': Counter()
        }

        # 4. –§–∏–ª—å—Ç—Ä—É–µ–º —Å v7.5 –ª–æ–≥–∏–∫–æ–π
        for kw in unique_raw:
            # v7.6 DEBUG: –ª–æ–≥–∏—Ä—É–µ–º keywords —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ oshmyan –∏–ª–∏ fanipol
            kw_lower = kw.lower()
            if 'oshmyan' in kw_lower or 'fanipal' in kw_lower or 'fanipol' in kw_lower:
                logger.warning(f"üîç v7.6 DEBUG INPUT: '{kw}' ‚Üí –ø—Ä–æ–≤–µ—Ä—è–µ–º...")
            
            is_allowed, reason, category = self._check_geo_conflicts_v75(
                kw, country, lemmas_map, seed_cities, language
            )
            
            if is_allowed:
                final_keywords.append(kw)
                stats['allowed'] += 1
                logger.debug(f"[v7.7] ‚úÖ –†–ê–ó–†–ï–®–ï–ù–û: '{kw}'")
            else:
                final_anchors.append(kw)
                stats['blocked'] += 1
                stats['reasons'][category] += 1
                logger.warning(f"[v7.7] ‚öì –Ø–ö–û–†–¨: '{kw}' (–ø—Ä–∏—á–∏–Ω–∞: {reason})")

        elapsed = time.time() - start_time
        logger.info(f"[v7.7] Finished in {elapsed:.2f}s. {stats['allowed']} OK / {stats['blocked']} Anchors")

        return {
            'keywords': final_keywords,
            'anchors': final_anchors,
            'stats': {
                'total': stats['total'],
                'allowed': stats['allowed'],
                'blocked': stats['blocked'],
                'reasons': dict(stats['reasons']),
                'elapsed_time': round(elapsed, 2)
            }
        }

    def _check_geo_conflicts_v75(self, keyword: str, country: str, 
                                  lemmas_map: Dict[str, str], seed_cities: Set[str],
                                  language: str) -> Tuple[bool, str, str]:
        """
        v7.6: –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å population filter –∏ smart disambiguation
        + –∑–∞—â–∏—Ç–∞ –æ—Ç –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π (–ê–ª–µ–∫—Å–µ–µ–≤–∫–∞ –≤ –•–∞—Ä—å–∫–æ–≤–µ)
        """
        words = re.findall(r'[–∞-—è—ëa-z0-9-]+', keyword)
        if not words:
            return True, "", ""

        keyword_lemmas = [lemmas_map.get(w, w) for w in words]
        
        # --- 0. –ü–†–ò–û–†–ò–¢–ï–¢: –ü–†–û–í–ï–†–ö–ê SEED_CITY (v7.6) ---
        # –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ –µ—Å—Ç—å –≥–æ—Ä–æ–¥ –∏–∑ seed (–Ω–∞–ø—Ä–∏–º–µ—Ä "—Ö–∞—Ä—å–∫–æ–≤ –∞–ª–µ–∫—Å–µ–µ–≤–∫–∞"),
        # —Ç–æ –¥–æ–≤–µ—Ä—è–µ–º —ç—Ç–æ–º—É –∑–∞–ø—Ä–æ—Å—É –∏ –ù–ï –±–ª–æ–∫–∏—Ä—É–µ–º –ø–æ –¥—Ä—É–≥–∏–º —Å–ª–æ–≤–∞–º
        words_set = set(words + keyword_lemmas)
        if any(city in words_set for city in seed_cities):
            logger.debug(f"[v7.6] '{keyword}' contains seed city, auto-allow")
            return True, "", ""
        
        # --- 1. HARD-BLACKLIST (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç #1) ---
        for check_val in words + keyword_lemmas:
            if check_val in self.forbidden_geo:
                return False, f"Hard-Blacklist '{check_val}'", "hard_blacklist"

        # --- 2. –†–ê–ô–û–ù–´ ---
        for w in words:
            if w in self.districts:
                dist_country = self.districts[w]
                if dist_country != country.lower():
                    return False, f"—Ä–∞–π–æ–Ω '{w}' ({dist_country})", "districts"
        
        # --- 2.5. –°–û–ö–†–ê–©–ï–ù–ò–Ø –ì–û–†–û–î–û–í (v7.5) ---
        for w in words + keyword_lemmas:
            if w in self.city_abbreviations:
                abbr_country = self.city_abbreviations[w]
                if abbr_country != country.lower():
                    return False, f"—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ '{w}' ({abbr_country})", f"{abbr_country}_abbreviations"
        
        # --- 2.6. –†–ï–ì–ò–û–ù–´ (v7.5) ---
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –±–∏–≥—Ä–∞–º–º—ã
        check_regions = words + keyword_lemmas + self._extract_ngrams(words, 2)
        for item in check_regions:
            if item in self.regions:
                region_country = self.regions[item]
                if region_country != country.lower():
                    return False, f"—Ä–µ–≥–∏–æ–Ω '{item}' ({region_country})", f"{region_country}_regions"
        
        # --- 2.7. –°–¢–†–ê–ù–´ (v7.5) ---
        for w in words + keyword_lemmas:
            if w in self.countries:
                ctry_code = self.countries[w]
                if ctry_code != country.lower():
                    return False, f"—Å—Ç—Ä–∞–Ω–∞ '{w}' ({ctry_code})", f"{ctry_code}_countries"
        
        # --- 2.8. –ú–ê–õ–´–ï –ì–û–†–û–î–ê –°–ù–ì (v7.6) ---
        for w in words + keyword_lemmas:
            if w in self.manual_small_cities:
                city_country = self.manual_small_cities[w]
                if city_country == 'unknown':
                    # –ë–ª–æ–∫–∏—Ä—É–µ–º –≤—Å–µ–≥–¥–∞ (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
                    return False, f"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ–±—ä–µ–∫—Ç '{w}'", "unknown"
                if city_country != country.lower():
                    return False, f"–º–∞–ª—ã–π –≥–æ—Ä–æ–¥ '{w}' ({city_country})", f"{city_country}_small_cities"

        # --- 3. –ì–û–†–û–î–ê (v7.5 —Å population filter) ---
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)
        search_items = []
        
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ (–¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π —Ç–∏–ø–∞ "–µ–∫–±")
        search_items.extend(words)
        
        # –õ–µ–º–º—ã —Å–ª–æ–≤
        search_items.extend(keyword_lemmas)
        
        # –ë–∏–≥—Ä–∞–º–º—ã –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ (–Ω–∞–±–µ—Ä–µ–∂–Ω—ã–µ —á–µ–ª–Ω—ã, –π–æ—à–∫–∞—Ä –æ–ª–∞)
        bigrams = self._extract_ngrams(words, 2)
        search_items.extend(bigrams)
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ë–∏–≥—Ä–∞–º–º—ã —Å –¥–µ—Ñ–∏—Å–æ–º –≤–º–µ—Å—Ç–æ –ø—Ä–æ–±–µ–ª–∞ (–π–æ—à–∫–∞—Ä-–æ–ª–∞ –≤–º–µ—Å—Ç–æ –π–æ—à–∫–∞—Ä –æ–ª–∞)
        search_items.extend([bg.replace(' ', '-') for bg in bigrams])
        
        # –ë–∏–≥—Ä–∞–º–º—ã –∏–∑ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
        lemma_bigrams = self._extract_ngrams(keyword_lemmas, 2)
        search_items.extend(lemma_bigrams)
        search_items.extend([bg.replace(' ', '-') for bg in lemma_bigrams])
        
        # –¢—Ä–∏–≥—Ä–∞–º–º—ã –¥–ª—è –≥–æ—Ä–æ–¥–æ–≤ –∏–∑ 3 —Å–ª–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        trigrams = self._extract_ngrams(words, 3)
        search_items.extend(trigrams)
        search_items.extend([tg.replace(' ', '-') for tg in trigrams])

        for item in search_items:
            if len(item) < 3:
                continue
            
            # v7.6: –ü–†–ò–û–†–ò–¢–ï–¢ - –ø—Ä–æ–≤–µ—Ä—è–µ–º ignored_words –î–û –±–∞–∑—ã –≥–æ—Ä–æ–¥–æ–≤
            if item in self.ignored_words:
                logger.debug(f"[v7.6] '{item}' in ignored_words, skipping")
                continue
            
            # ‚úÖ v8.0 DEBUG: –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤
            debug_cities = ['–∂–¥–∞–Ω–æ–≤–∏—á–∏', 'zhdanovichi', '–∂–¥–∞–Ω–æ–≤–∏—á', '–ª–æ—à–∏—Ü–∞', 'losica']
            is_debug_city = any(dc in item.lower() for dc in debug_cities)
            
            if is_debug_city:
                logger.warning(f"üîç [v8.0 DEBUG] Processing '{item}'")
            
            # ‚úÖ v7.9 –§–£–ù–î–ê–ú–ï–ù–¢–ê–õ–¨–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–ê–ó–ê –ì–û–†–û–î–û–í = –ü–ï–†–í–ò–ß–ù–ê
            # 
            # –°–¢–ê–†–ê–Ø –û–®–ò–ë–ö–ê v7.7-v7.8: 
            #   1. –ü—Ä–æ–≤–µ—Ä—è–ª–∏ –±–∞–∑—É
            #   2. –ï—Å–ª–∏ –Ω–∞—à–ª–∏ ‚Üí –ø—Ä–æ–≤–µ—Ä—è–ª–∏ _is_common_noun 
            #   3. –ï—Å–ª–∏ NOUN ‚Üí –ø—Ä–æ–ø—É—Å–∫–∞–ª–∏ ("–ª–æ—à–∏—Ü–∞", "–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏")
            #
            # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê v7.9:
            #   1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑—É
            #   2. –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ò –≥–æ—Ä–æ–¥ –∏–∑ –¥—Ä—É–≥–æ–π —Å—Ç—Ä–∞–Ω—ã ‚Üí –ë–õ–û–ö–ò–†–£–ï–ú –ù–ï–ú–ï–î–õ–ï–ù–ù–û
            #   3. –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –ù–ï –í–õ–ò–Ø–ï–¢ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–æ–≤–æ (—Å–∫–ª–æ–Ω—ë–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã ‚Üí –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞)
            item_normalized = self._get_lemma(item, language)
            
            if is_debug_city:
                logger.warning(f"üîç [v8.0 DEBUG] Normalized: '{item}' ‚Üí '{item_normalized}'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –±–∞–∑–µ: —Å–Ω–∞—á–∞–ª–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞, –ø–æ—Ç–æ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
            found_country = self.all_cities_global.get(item_normalized)
            
            if is_debug_city:
                logger.warning(f"üîç [v8.0 DEBUG] Database lookup (normalized): '{item_normalized}' ‚Üí {found_country}")
                logger.warning(f"üîç [v8.0 DEBUG] Database size: {len(self.all_cities_global)} cities")
            
            if not found_country:
                # –ù–µ –Ω–∞—à–ª–∏ –ª–µ–º–º—É - –ø—Ä–æ–±—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª (–¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π —Ç–∏–ø–∞ "–µ–∫–±")
                found_country = self.all_cities_global.get(item)
                
                if is_debug_city:
                    logger.warning(f"üîç [v8.0 DEBUG] Database lookup (original): '{item}' ‚Üí {found_country}")
                
                if found_country:
                    logger.debug(f"[v8.0] Found original: '{item}' ‚Üí {found_country}")
                    item_normalized = item  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
            elif item_normalized != item:
                logger.debug(f"[v8.0] Found via lemma: '{item}' ‚Üí '{item_normalized}' ‚Üí {found_country}")
            
            # ========== –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –õ–û–ì–ò–ö–ê v7.9 ==========
            if found_country:
                # –ì–æ—Ä–æ–¥ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ!
                
                if is_debug_city:
                    logger.warning(f"üîç [v8.0 DEBUG] FOUND IN DATABASE: '{item_normalized}' = {found_country.upper()}")
                    logger.warning(f"üîç [v8.0 DEBUG] Target country: {country.lower()}")
                    logger.warning(f"üîç [v8.0 DEBUG] Match: {found_country == country.lower()}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –≠—Ç–æ –Ω–∞—à —Ü–µ–ª–µ–≤–æ–π –≥–æ—Ä–æ–¥? (–Ω–∞–ø—Ä–∏–º–µ—Ä, –•–∞—Ä—å–∫–æ–≤ –≤ UA)
                if found_country == country.lower():
                    logger.debug(f"[v8.0] City '{item_normalized}' ({found_country}) - ALLOWED (target country)")
                    if is_debug_city:
                        logger.warning(f"üîç [v8.0 DEBUG] ‚úÖ ALLOWED - same country")
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –≠—Ç–æ –≥–æ—Ä–æ–¥ –∏–∑ seed? (–∑–∞—â–∏—Ç–∞ –æ—Ç –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π)
                # –ü—Ä–∏–º–µ—Ä: seed="—Ä–µ–º–æ–Ω—Ç —Ö–∞—Ä—å–∫–æ–≤ –∞–ª–µ–∫—Å–µ–µ–≤–∫–∞" ‚Üí "–∞–ª–µ–∫—Å–µ–µ–≤–∫–∞" –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ RU, –Ω–æ —ç—Ç–æ —Ä–∞–π–æ–Ω –•–∞—Ä—å–∫–æ–≤–∞
                if item_normalized in seed_cities:
                    logger.debug(f"[v8.0] City '{item_normalized}' in seed_cities - ALLOWED")
                    if is_debug_city:
                        logger.warning(f"üîç [v8.0 DEBUG] ‚úÖ ALLOWED - in seed")
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ì–æ—Ä–æ–¥ –∏–∑ –¥—Ä—É–≥–æ–π —Å—Ç—Ä–∞–Ω—ã ‚Üí –ë–õ–û–ö–ò–†–£–ï–ú
                # ‚ö†Ô∏è –í–ê–ñ–ù–û: –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (NOUN/–Ω–µ-NOUN) –ù–ï –í–õ–ò–Ø–ï–¢ –Ω–∞ —ç—Ç–æ —Ä–µ—à–µ–Ω–∏–µ!
                if is_debug_city:
                    logger.warning(f"üîç [v8.0 DEBUG] ‚öì SHOULD BLOCK: '{item}' ‚Üí '{item_normalized}' ({found_country.upper()} != {country.upper()})")
                
                logger.warning(f"[v8.0] ‚öì BLOCKING foreign city: '{item}' ‚Üí '{item_normalized}' ({found_country.upper()})")
                return False, f"{found_country.upper()} –≥–æ—Ä–æ–¥ '{item_normalized}'", f"{found_country}_cities"
            
            # ========== –ú–û–†–§–û–õ–û–ì–ò–Ø = –í–¢–û–†–ò–ß–ù–ê (—Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ª–æ–≤ –í–ù–ï –±–∞–∑—ã) ==========
            # –ï—Å–ª–∏ –≥–æ—Ä–æ–¥ –ù–ï –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é
            # –≠—Ç–æ –∑–∞—â–∏—Ç–∞ –æ—Ç "–¥–æ–º" (Ghana), "–º–∏—Ä" (Russia) –∏ —Ç.–¥.
            else:
                # –°–ª–æ–≤–æ –Ω–µ –≤ –±–∞–∑–µ –≥–æ—Ä–æ–¥–æ–≤ - –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ –æ–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ?
                if self._is_common_noun(item_normalized, language):
                    logger.debug(f"[v7.9] '{item_normalized}' NOT in geo database + common noun - ALLOWED")
                    continue
                # –ï—Å–ª–∏ –Ω–µ NOUN –∏ –Ω–µ –≤ –±–∞–∑–µ - —Ç–æ–∂–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–ª–æ–≤–æ)
        
        # --- 4. –ì–†–ê–ú–ú–ê–¢–ò–ö–ê ---
        if not self._is_grammatically_valid(keyword, language):
            return False, "–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞", "grammar"
        
        return True, "", ""

    def _is_common_noun(self, word: str, language: str) -> bool:
        """
        v7.7 FIXED: Smart disambiguation —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º Geox
        
        –ü—Ä–∏–º–µ—Ä—ã:
        - "–¥–æ–º" ‚Üí True (–æ–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ, –ù–ï –≥–æ—Ä–æ–¥)
        - "–æ—à–º—è–Ω—ã" ‚Üí False (Geox - –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç)
        - "–∫–∏–µ–≤" ‚Üí False (—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–º—è, –≥–æ—Ä–æ–¥)
        """
        if not self._has_morph or language not in ['ru', 'uk']:
            return False
        
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        
        try:
            parsed = morph.parse(word)
            if parsed:
                # ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –í–°–ï –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
                for parse_variant in parsed:
                    tag = parse_variant.tag
                    
                    # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç = Geox ‚Üí —ç—Ç–æ –≥–æ—Ä–æ–¥!
                    if 'Geox' in tag:
                        logger.debug(f"[v7.7] '{word}' is Geox (geographic), NOT common noun")
                        return False
                    
                    # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –ï—Å–ª–∏ Name (—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–º—è) ‚Üí –Ω–µ –æ–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ
                    if 'Name' in tag:
                        return False
                
                # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ò –û–î–ò–ù –≤–∞—Ä–∏–∞–Ω—Ç –Ω–µ Geox/Name - –ø—Ä–æ–≤–µ—Ä—è–µ–º NOUN
                first_tag = parsed[0].tag
                if 'NOUN' in first_tag and word.islower():
                    logger.debug(f"[v7.7] '{word}' is common noun")
                    return True
        except:
            pass
        
        return False

    def _extract_cities_from_seed(self, seed: str, country: str, language: str) -> Set[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ—Ä–æ–¥–∞ –∏–∑ seed"""
        if not self._has_morph:
            return set()
        
        seed_cities = set()
        words = re.findall(r'[–∞-—è—ëa-z0-9-]+', seed.lower())
        
        for word in words:
            if word in self.all_cities_global:
                city_country = self.all_cities_global[word]
                if city_country == country.lower():
                    seed_cities.add(word)
            
            lemma = self._get_lemma(word, language)
            if lemma in self.all_cities_global:
                city_country = self.all_cities_global[lemma]
                if city_country == country.lower():
                    seed_cities.add(lemma)
        
        bigrams = self._extract_ngrams(words, 2)
        for bigram in bigrams:
            if bigram in self.all_cities_global:
                city_country = self.all_cities_global[bigram]
                if city_country == country.lower():
                    seed_cities.add(bigram)
        
        return seed_cities

    def _batch_lemmatize(self, words: Set[str], language: str) -> Dict[str, str]:
        """Batch –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è"""
        if not self._has_morph:
            return {w: w for w in words}
        
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        lemmas = {}
        
        for word in words:
            lemma = self._get_lemma(word, language, morph)
            lemmas[word] = lemma
        
        return lemmas

    def _get_lemma(self, word: str, language: str, morph=None) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –ª–µ–º–º—É —Å–ª–æ–≤–∞"""
        if not self._has_morph:
            return word
        
        if morph is None:
            morph = self.morph_ru if language == 'ru' else self.morph_uk
        
        try:
            parsed = morph.parse(word)
            if parsed:
                return parsed[0].normal_form
        except:
            pass
        
        return word

    def _extract_ngrams(self, words: List[str], n: int = 2) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç n-–≥—Ä–∞–º–º—ã (–±–∏–≥—Ä–∞–º–º—ã, —Ç—Ä–∏–≥—Ä–∞–º–º—ã)
        
        v7.5: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ n=2,3 –¥–ª—è –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤
        """
        if len(words) < n:
            return []
        return [" ".join(words[i:i+n]) for i in range(len(words) - n + 1)]

    def _is_grammatically_valid(self, keyword: str, language: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å"""
        if not self._has_morph or language not in ['ru', 'uk']:
            return True
        
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        words = re.findall(r'[–∞-—è—ëa-z]+', keyword.lower())
        
        for word in words:
            try:
                parsed = morph.parse(word)
                if parsed:
                    tag = parsed[0].tag
                    invalid_tags = {'datv', 'ablt', 'loct'}
                    if 'plur' in tag and any(bad in tag for bad in invalid_tags):
                        return False
            except:
                pass
        
        return True


# ============================================
# DISTRICTS
# ============================================

DISTRICTS_MINSK = {
    "—É—Ä—É—á—å–µ": "by",
    "—à–∞–±–∞–Ω—ã": "by",
    "–∫–∞–º–µ–Ω–Ω–∞—è –≥–æ—Ä–∫–∞": "by",
    "—Å–µ—Ä–µ–±—Ä—è–Ω–∫–∞": "by"
}

DISTRICTS_TASHKENT = {
    "—á–∏–ª–∞–Ω–∑–∞—Ä": "uz",
    "—é–Ω—É—Å–∞–±–∞–¥": "uz",
    "—Å–µ—Ä–≥–µ–ª–∏": "uz",
    "—è–∫–∫–∞—Å–∞—Ä–∞–π": "uz"
}

DISTRICTS_EXTENDED = {**DISTRICTS_MINSK, **DISTRICTS_TASHKENT}


# ============================================
# EXAMPLE USAGE
# ============================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("üöÄ BATCH POST-FILTER v7.5 - AUTONOMOUS GLOBAL GEO-FILTER")
    print("="*60)
    
    # Hard-Blacklist
    test_forbidden = {
        "–∫—Ä—ã–º", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å", "—è–ª—Ç–∞",
        "–¥–æ–Ω–µ—Ü–∫", "–ª—É–≥–∞–Ω—Å–∫", "–≥–æ—Ä–ª–æ–≤–∫–∞"
    }
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä v7.5 (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∑–∏—Ç –±–∞–∑—É —Å population > 5000)
    print("\nüì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–∞...")
    post_filter = BatchPostFilter(
        all_cities_global={},  # –ë—É–¥–µ—Ç –ø–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        forbidden_geo=test_forbidden,
        districts=DISTRICTS_EXTENDED,
        population_threshold=5000
    )
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ - –†–ï–ê–õ–¨–ù–´–ï –ü–†–û–ë–õ–ï–ú–ù–´–ï KEYWORDS
    test_keywords = [
        # ‚úÖ –î–æ–ª–∂–Ω—ã –ü–†–û–ü–£–°–¢–ò–¢–¨–°–Ø (UA –≥–æ—Ä–æ–¥–∞):
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –∫–∏–µ–≤",
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –¥–Ω–µ–ø—Ä",
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ —Ö–∞—Ä—å–∫–æ–≤",
        
        # ‚öì –î–æ–ª–∂–Ω—ã –ë–õ–û–ö–ò–†–û–í–ê–¢–¨–°–Ø (RU –≥–æ—Ä–æ–¥–∞):
        "—Ä–µ–º–æ–Ω—Ç —Ä–æ–±–æ—Ç–æ–≤ –ø—ã–ª–µ—Å–æ—Å–æ–≤ –π–æ—à–∫–∞—Ä –æ–ª–∞",  # RU (–±–∏–≥—Ä–∞–º–º–∞ —Å –ø—Ä–æ–±–µ–ª–æ–º)
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ —É–ª–∞–Ω —É–¥—ç",            # RU (–±–∏–≥—Ä–∞–º–º–∞)
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –Ω–∞–±–µ—Ä–µ–∂–Ω—ã–µ —á–µ–ª–Ω—ã",    # RU (–±–∏–≥—Ä–∞–º–º–∞)
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –æ—Ä–µ—Ö–æ–≤–æ –∑—É–µ–≤–æ",       # RU (–±–∏–≥—Ä–∞–º–º–∞)
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –µ–∫–±",                 # RU (—Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥)
        
        # ‚öì –î–æ–ª–∂–Ω—ã –ë–õ–û–ö–ò–†–û–í–ê–¢–¨–°–Ø (BY –≥–æ—Ä–æ–¥–∞):
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ —Ñ–∞–Ω–∏–ø–æ–ª—å",            # BY
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –æ—à–º—è–Ω—ã",              # BY
        
        # ‚öì –î–æ–ª–∂–Ω—ã –ë–õ–û–ö–ò–†–û–í–ê–¢–¨–°–Ø (KZ –≥–æ—Ä–æ–¥–∞):
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ —É–∑—ã–Ω–∞–≥–∞—à",            # KZ
        
        # ‚öì –î–æ–ª–∂–Ω—ã –ë–õ–û–ö–ò–†–û–í–ê–¢–¨–°–Ø (–¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω—ã):
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –¥–∞–π—Å–æ–Ω –≤ –∏–∑—Ä–∞–∏–ª–µ",    # IL (Israel)
        
        # ‚öì –î–æ–ª–∂–Ω—ã –ë–õ–û–ö–ò–†–û–í–ê–¢–¨–°–Ø (—Ä–µ–≥–∏–æ–Ω—ã RU):
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –∏–Ω–≥—É—à–µ—Ç–∏—è",           # –†–µ–≥–∏–æ–Ω RU
        
        # ‚öì –î–æ–ª–∂–Ω—ã –ë–õ–û–ö–ò–†–û–í–ê–¢–¨–°–Ø (Hard-Blacklist):
        "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ —Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å",         # –ö—Ä—ã–º
    ]
    
    print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {len(test_keywords)} keywords...")
    result = post_filter.filter_batch(
        keywords=test_keywords,
        seed="—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤",
        country="ua",
        language="ru"
    )
    
    print("\n" + "="*60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("="*60)
    print(f"\n‚úÖ –†–ê–ó–†–ï–®–ï–ù–û ({len(result['keywords'])}):")
    for kw in result['keywords']:
        print(f"  - {kw}")
    
    print(f"\n‚öì –Ø–ö–û–†–Ø ({len(result['anchors'])}):")
    for kw in result['anchors']:
        print(f"  - {kw}")
    
    print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"  Total: {result['stats']['total']}")
    print(f"  Allowed: {result['stats']['allowed']}")
    print(f"  Blocked: {result['stats']['blocked']}")
    print(f"  Reasons: {result['stats']['reasons']}")
    print(f"  Time: {result['stats']['elapsed_time']}s")
    print("="*60 + "\n")
