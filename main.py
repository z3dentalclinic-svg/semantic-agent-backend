"""
PREPOSITIONAL BRIDGE TEST - –º–µ—Ç–æ–¥ –æ—Ç Gemini
–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –¢–ï–°–¢: –†–∞–±–æ—Ç–∞–µ—Ç –ª–∏ —Ç—Ä–∏–≥—Ä–∞–º–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ?

–ü—Ä–æ–≤–µ—Ä—è–µ–º: "—Å —Ä–µ–º–æ–Ω—Ç" ‚Üí "—Å—Ä–æ—á–Ω—ã–π —Ä–µ–º–æ–Ω—Ç", "—Å–µ—Ä–≤–∏—Å–Ω—ã–π —Ä–µ–º–æ–Ω—Ç"?
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List
import os
import httpx
import asyncio
import time
import random

app = FastAPI(title="Gemini Trigram Test", version="1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AutocompleteParser:
    def __init__(self):
        self.base_url = "https://suggestqueries.google.com/complete/search"
    
    async def fetch_suggestions(self, query: str, country: str, language: str) -> List[str]:
        params = {"client": "chrome", "q": query, "gl": country, "hl": language}
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(self.base_url, params=params)
                if response.status_code == 200:
                    data = response.json()
                    if isinstance(data, list) and len(data) > 1:
                        return [s for s in data[1] if isinstance(s, str)]
                return []
        except Exception as e:
            print(f"Error: {e}")
            return []
    
    async def gemini_trigram_test(self, seed: str, country: str, language: str) -> List[str]:
        all_keywords = set()
        
        print(f"\n{'='*60}")
        print(f"üî¨ PREPOSITIONAL BRIDGE - –º–µ—Ç–æ–¥ –æ—Ç Gemini")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –¢–ï–°–¢: –†–∞–±–æ—Ç–∞–µ—Ç –ª–∏ —Ç—Ä–∏–≥—Ä–∞–º–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ?\n")
        
        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ü–ï–†–í–û–ï —Å–ª–æ–≤–æ –∏–∑ seed
        first_word = seed.split()[0]
        print(f"–ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ: '{first_word}'")
        print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º: '[–±—É–∫–≤–∞] {first_word}'\n")
        
        # ========================================
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –¢–ï–°–¢: –¢—Ä–∏–≥—Ä–∞–º–º—ã
        # ========================================
        print(f"{'='*60}")
        print(f"–¢–ï–°–¢: –¢—Ä–∏–≥—Ä–∞–º–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ")
        print(f"{'='*60}\n")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –±—É–∫–≤—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö PREFIX
        test_letters = {
            "—Å": "–æ–∂–∏–¥–∞–µ–º: '—Å—Ä–æ—á–Ω—ã–π', '—Å–µ—Ä–≤–∏—Å–Ω—ã–π', '—Å–µ—Ä–≤–∏—Å'",
            "–≥": "–æ–∂–∏–¥–∞–µ–º: '–≥–¥–µ', '–≥–∞—Ä–∞–Ω—Ç–∏–π–Ω—ã–π'",
            "–∞": "–æ–∂–∏–¥–∞–µ–º: '–∞–≤–∏—Ç–æ'",
            "–º": "–æ–∂–∏–¥–∞–µ–º: '–º–∞—Å—Ç–µ—Ä', '–º–∞—Å—Ç–µ—Ä—Å–∫–∞—è'",
            "–Ω": "–æ–∂–∏–¥–∞–µ–º: '–Ω–µ–¥–æ—Ä–æ–≥–æ–π'",
            "—Ü": "–æ–∂–∏–¥–∞–µ–º: '—Ü–µ–Ω—Ç—Ä'",
            "—á": "–æ–∂–∏–¥–∞–µ–º: '—á–∞—Å—Ç–Ω—ã–π'",
            "–∫": "–æ–∂–∏–¥–∞–µ–º: '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–∫–∞–∫'",
        }
        
        discovered_prefixes = set()
        total_queries = 0
        
        for letter, expectation in test_letters.items():
            # –¢—Ä–∏–≥—Ä–∞–º–º–Ω—ã–π –∑–∞–ø—Ä–æ—Å: –±—É–∫–≤–∞ + –ø–µ—Ä–≤–æ–µ_—Å–ª–æ–≤–æ
            trigram_query = f"{letter} {first_word}"
            results = await self.fetch_suggestions(trigram_query, country, language)
            total_queries += 1
            
            print(f"'{trigram_query}' ({expectation})")
            print(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            
            if len(results) == 0:
                print(f"  ‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n")
                continue
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            found_prefix_words = []
            
            for result in results:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å –Ω–∞—à–µ–π –±—É–∫–≤—ã?
                if result.lower().startswith(letter.lower()):
                    # –£–±–∏—Ä–∞–µ–º –±—É–∫–≤—É –∏ —Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å
                    after_letter = result[1:].strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å—Ç—å –ª–∏ –Ω–∞—à–µ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ?
                    if first_word.lower() in after_letter.lower():
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á—Ç–æ –ü–ï–†–ï–î –ø–µ—Ä–≤—ã–º —Å–ª–æ–≤–æ–º
                        word_position = after_letter.lower().find(first_word.lower())
                        if word_position > 0:
                            # –ï—Å—Ç—å —Å–ª–æ–≤–æ –º–µ–∂–¥—É –±—É–∫–≤–æ–π –∏ first_word!
                            prefix_word = after_letter[:word_position].strip()
                            if prefix_word:
                                found_prefix_words.append(prefix_word)
                                discovered_prefixes.add(prefix_word)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –Ω–∞—à–ª–∏
            if len(found_prefix_words) > 0:
                print(f"  ‚úÖ –ù–ê–ô–î–ï–ù–´ PREFIX —Å–ª–æ–≤–∞:")
                for word in set(found_prefix_words):
                    print(f"     üéØ '{word}'")
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä
                    for r in results:
                        if word in r:
                            print(f"        –ü—Ä–∏–º–µ—Ä: {r}")
                            break
            else:
                print(f"  ‚ùå PREFIX —Å–ª–æ–≤–∞ –ù–ï –Ω–∞–π–¥–µ–Ω—ã")
                print(f"  –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
                for r in results[:3]:
                    print(f"     ‚Ä¢ {r}")
            
            print()
            await asyncio.sleep(random.uniform(0.5, 1.5))
        
        print(f"{'='*60}")
        print(f"‚úÖ –¢–ï–°–¢ –ó–ê–í–ï–†–®–Å–ù")
        print(f"{'='*60}")
        print(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {total_queries}")
        print(f"–ù–∞–π–¥–µ–Ω–æ PREFIX —Å–ª–æ–≤: {len(discovered_prefixes)}")
        
        if len(discovered_prefixes) > 0:
            print(f"\nüéâ –ú–ï–¢–û–î GEMINI –†–ê–ë–û–¢–ê–ï–¢!")
            print(f"–¢—Ä–∏–≥—Ä–∞–º–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞—Ö–æ–¥–∏—Ç PREFIX —Å–ª–æ–≤–∞:\n")
            for word in sorted(discovered_prefixes):
                print(f"  ‚Ä¢ {word}")
            
            # ========================================
            # –≠–¢–ê–ü 2: –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ seed
            # ========================================
            print(f"\n{'='*60}")
            print(f"–≠–¢–ê–ü 2: –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø–æ–ª–Ω—ã–º seed")
            print(f"{'='*60}\n")
            
            verified_keywords = set()
            
            for prefix_word in sorted(discovered_prefixes):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω—ã–π –∑–∞–ø—Ä–æ—Å: prefix_word + –ø–æ–ª–Ω—ã–π seed
                full_query = f"{prefix_word} {seed}"
                results = await self.fetch_suggestions(full_query, country, language)
                total_queries += 1
                
                if len(results) > 0:
                    verified_keywords.update(results)
                    all_keywords.update(results)
                    print(f"‚úÖ '{full_query}' ‚Üí {len(results)} –∫–ª—é—á–µ–π")
                    for r in results[:3]:
                        print(f"    ‚Ä¢ {r}")
                else:
                    print(f"‚ùå '{full_query}' ‚Üí –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                
                await asyncio.sleep(random.uniform(0.5, 1.5))
            
            print(f"\n{'='*60}")
            print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
            print(f"{'='*60}")
            print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {total_queries}")
            print(f"PREFIX —Å–ª–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(discovered_prefixes)}")
            print(f"–í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–π: {len(verified_keywords)}")
            
            if "—Å–µ—Ä–≤–∏—Å" in discovered_prefixes or "—Å—Ä–æ—á–Ω—ã–π" in discovered_prefixes:
                print(f"\nüéØ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê!")
                print(f"–ù–∞—à–ª–∏ —Ü–µ–ª–µ–≤—ã–µ PREFIX: '—Å–µ—Ä–≤–∏—Å' / '—Å—Ä–æ—á–Ω—ã–π'")
            
        else:
            print(f"\n‚ùå –ú–ï–¢–û–î GEMINI –ù–ï –†–ê–ë–û–¢–ê–ï–¢!")
            print(f"–¢—Ä–∏–≥—Ä–∞–º–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ù–ï –Ω–∞—Ö–æ–¥–∏—Ç PREFIX —Å–ª–æ–≤–∞")
            print(f"Google –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é –∏–ª–∏ –¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã")
        
        print(f"\n{'='*60}")
        print(f"–ò–¢–û–ì–û –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"{'='*60}\n")
        
        return list(all_keywords)


@app.get("/api/test-parser/gemini")
async def test_gemini(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru")
):
    parser = AutocompleteParser()
    start = time.time()
    keywords = await parser.gemini_trigram_test(seed, country, language)
    return {
        "seed": seed,
        "method": "PREPOSITIONAL BRIDGE (Gemini)",
        "keywords": keywords,
        "count": len(keywords),
        "time": round(time.time() - start, 2)
    }


@app.get("/")
async def root():
    return {
        "api": "Gemini Trigram Test",
        "url": "/api/test-parser/gemini?seed=—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
