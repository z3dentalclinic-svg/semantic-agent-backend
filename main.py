"""
FGS Parser API - Semantic keyword research with geo-filtering
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from typing import List, Dict
import httpx
import asyncio
import time
import random
import re
import logging
from difflib import SequenceMatcher

from filters import (
    BatchPostFilter, 
    DISTRICTS_EXTENDED,
    filter_infix_results,
    filter_relevant_keywords,
    filter_geo_garbage,
    apply_pre_filter,  # ‚Üê —Å–∞–Ω–∏—Ç–∞—Ä–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–î–û –≥–µ–æ-—Ñ–∏–ª—å—Ç—Ä–∞)
    apply_l0_filter,   # ‚Üê L0 –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ö–≤–æ—Å—Ç–æ–≤ (–ü–û–°–õ–ï –≤—Å–µ—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤)
    apply_l2_filter,   # ‚Üê L2 Tri-Signal –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (PMI + Centroid + L0 signals)
)
from geo import generate_geo_blacklist_full
from config import USER_AGENTS, WHITELIST_TOKENS, MANUAL_RARE_CITIES, FORBIDDEN_GEO
from utils.normalizer import normalize_keywords
from utils.tracer import FilterTracer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

normalizer_logger = logging.getLogger("GoldenNormalizer")
normalizer_logger.setLevel(logging.DEBUG)
normalizer_logger.propagate = True

import nltk
from nltk.stem import SnowballStemmer

try:
    from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsNERTagger, Doc
    NATASHA_AVAILABLE = True
except ImportError:
    NATASHA_AVAILABLE = False
    print("‚ö†Ô∏è Natasha –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. EntityLogicManager –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å –∂—ë—Å—Ç–∫–∏–º –∫–µ—à–µ–º.")

try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except Exception as e:
    print(f"Warning: Could not download NLTK data: {e}")

import pymorphy3

app = FastAPI(
    title="FGS Parser API",
    version="8.0.0",
    description="6 –º–µ—Ç–æ–¥–æ–≤ | 3 sources | Batch Post-Filter | L0 + L2 Classifiers | v8.0 DUAL COSINE"
)

app.add_middleware(
    CORSMiddleware,
    allow_origin_regex='.*',  # –†–∞–∑—Ä–µ—à–∞–µ—Ç –ª—é–±—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –≤–∫–ª—é—á–∞—è –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.options("/{rest_of_path:path}")
async def preflight_handler():
    return {}

# === –ó–ê–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–ù–û: –¥—É–±–ª—å geo/blacklist.py, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–º–ø–æ—Ä—Ç (—Å—Ç—Ä–æ–∫–∞ 25) ===
# def generate_geo_blacklist_full():
#     """
#     """
#     try:
#         from geonamescache import GeonamesCache
# 
#         gc = GeonamesCache()
#         cities = gc.get_cities()
# 
#         all_cities_global = {}  # {–≥–æ—Ä–æ–¥: –∫–æ–¥_—Å—Ç—Ä–∞–Ω—ã}
# 
#         for city_id, city_data in cities.items():
#             country = city_data['countrycode'].lower()  # 'RU', 'UA', 'BY' ‚Üí 'ru', 'ua', 'by'
# 
#             name = city_data['name'].lower().strip()
#             all_cities_global[name] = country
# 
#             for alt in city_data.get('alternatenames', []):
#                 if ' ' in alt:
#                     continue
# 
#                 if not (3 <= len(alt) <= 30):
#                     continue
# 
#                 if not any(c.isalpha() for c in alt):
#                     continue
# 
#                 alt_clean = alt.replace('-', '').replace("'", "")
#                 if alt_clean.isalpha():
#                     is_latin_cyrillic = all(
#                         ('\u0000' <= c <= '\u007F') or  # ASCII (–ª–∞—Ç–∏–Ω–∏—Ü–∞)
#                         ('\u0400' <= c <= '\u04FF') or  # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞
#                         c in ['-', "'"]
#                         for c in alt
#                     )
# 
#                     if is_latin_cyrillic:
#                         alt_lower = alt.lower().strip()
#                         if alt_lower not in all_cities_global:
#                             all_cities_global[alt_lower] = country
# 
#         print("‚úÖ v5.6.0 TURBO: O(1) WORD BOUNDARY LOOKUP - –ì–µ–æ-–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
#         print(f"   ALL_CITIES_GLOBAL: {len(all_cities_global)} –≥–æ—Ä–æ–¥–æ–≤ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ —Å—Ç—Ä–∞–Ω–∞–º")
#         
#         from collections import Counter
#         country_stats = Counter(all_cities_global.values())
#         print(f"   –¢–æ–ø-5 —Å—Ç—Ä–∞–Ω: {dict(country_stats.most_common(5))}")
# 
#         return all_cities_global
# 
#     except ImportError:
#         print("‚ö†Ô∏è geonamescache –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å")
#         
#         all_cities_global = {
#             '–º–æ—Å–∫–≤–∞': 'ru', '–º—Å–∫': 'ru', '—Å–ø–±': 'ru', '–ø–∏—Ç–µ—Ä': 'ru', 
#             '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥': 'ru', '–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': 'ru', '–∫–∞–∑–∞–Ω—å': 'ru',
#             '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫': 'ru', '—á–µ–ª—è–±–∏–Ω—Å–∫': 'ru', '–æ–º—Å–∫': 'ru',
#             '–º–∏–Ω—Å–∫': 'by', '–≥–æ–º–µ–ª—å': 'by', '–≤–∏—Ç–µ–±—Å–∫': 'by', '–º–æ–≥–∏–ª–µ–≤': 'by',
#             '–∞–ª–º–∞—Ç—ã': 'kz', '–∞—Å—Ç–∞–Ω–∞': 'kz', '–∫–∞—Ä–∞–≥–∞–Ω–¥–∞': 'kz',
#             '–∫–∏–µ–≤': 'ua', '—Ö–∞—Ä—å–∫–æ–≤': 'ua', '–æ–¥–µ—Å—Å–∞': 'ua', '–¥–Ω–µ–ø—Ä': 'ua',
#             '–ª—å–≤–æ–≤': 'ua', '–∑–∞–ø–æ—Ä–æ–∂—å–µ': 'ua', '–∫—Ä–∏–≤–æ–π —Ä–æ–≥': 'ua',
#             '–Ω–∏–∫–æ–ª–∞–µ–≤': 'ua', '–≤–∏–Ω–Ω–∏—Ü–∞': 'ua', '—Ö–µ—Ä—Å–æ–Ω': 'ua',
#             '–ø–æ–ª—Ç–∞–≤–∞': 'ua', '—á–µ—Ä–Ω–∏–≥–æ–≤': 'ua', '—á–µ—Ä–∫–∞—Å—Å—ã': 'ua',
#             '–¥–Ω–µ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å–∫': 'ua', 'kyiv': 'ua', 'kiev': 'ua',
#             'kharkiv': 'ua', 'odessa': 'ua', 'lviv': 'ua', 'dnipro': 'ua',
#         }
#         
#         return all_cities_global

ALL_CITIES_GLOBAL = generate_geo_blacklist_full()

# –ë–∞–∑—ã –¥–ª—è L0 –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ —á—Ç–æ –£–ñ–ï –∑–∞–≥—Ä—É–∂–µ–Ω–æ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
# GEO_DB: Dict[str, Set[str]] ‚Äî –≥–æ—Ä–æ–¥ ‚Üí –º–Ω–æ–∂–µ—Å—Ç–≤–æ ISO-–∫–æ–¥–æ–≤ —Å—Ç—Ä–∞–Ω
# detect_geo() –ø—Ä–æ–≤–µ—Ä—è–µ—Ç geo_db[word] —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –≤ –∫–∞–∫–æ–π —Å—Ç—Ä–∞–Ω–µ –≥–æ—Ä–æ–¥
GEO_DB = {}
for city_name, country_code in ALL_CITIES_GLOBAL.items():
    GEO_DB.setdefault(city_name, set()).add(country_code.upper())
for district_name, country_code in DISTRICTS_EXTENDED.items():
    GEO_DB.setdefault(district_name, set()).add(country_code.upper())
logger.info(f"[L0] GEO_DB: {len(GEO_DB)} –∑–∞–ø–∏—Å–µ–π (cities: {len(ALL_CITIES_GLOBAL)}, districts: {len(DISTRICTS_EXTENDED)})")

# BRAND_DB: –≥—Ä—É–∑–∏–º –æ—Ç–¥–µ–ª—å–Ω–æ (–º–∞–ª–µ–Ω—å–∫–∞—è, ~100 –∑–∞–ø–∏—Å–µ–π)
try:
    from databases import load_brands_db
    BRAND_DB = load_brands_db()
    logger.info(f"[L0] BRAND_DB: {len(BRAND_DB)} –∑–∞–ø–∏—Å–µ–π")
except ImportError:
    BRAND_DB = set()
    logger.warning("[L0] databases.py not found, BRAND_DB –ø—É—Å—Ç")


def deduplicate_final_results(data: dict) -> dict:
    """
    –°—Ç—Ä–æ–≥–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
    –£–¥–∞–ª—è–µ—Ç –ø–æ–≤—Ç–æ—Ä—ã, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ä–µ–≥–∏—Å—Ç—Ä –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã.
    """
    if not data or "keywords" not in data:
        return data

    seen = set()
    unique_keywords = []

    for item in data["keywords"]:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ item —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—ë–º
        if isinstance(item, str):
            raw_query = item
        elif isinstance(item, dict):
            raw_query = item.get("query", "")
        else:
            continue
            
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        norm_query = " ".join(raw_query.lower().split())
        
        if norm_query not in seen:
            seen.add(norm_query)
            unique_keywords.append(item)
    
    data["keywords"] = unique_keywords
    if "total_count" in data:
        data["total_count"] = len(unique_keywords)
    if "count" in data:
        data["count"] = len(unique_keywords)
    if "total_unique_keywords" in data:
        data["total_unique_keywords"] = len(unique_keywords)
    
    return data


class AdaptiveDelay:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""

    def __init__(self, initial_delay: float = 0.2, min_delay: float = 0.1, max_delay: float = 1.0):
        self.delay = initial_delay
        self.min_delay = min_delay
        self.max_delay = max_delay

    def get_delay(self) -> float:
        return self.delay

    def on_success(self):
        self.delay = max(self.min_delay, self.delay * 0.95)

    def on_rate_limit(self):
        self.delay = min(self.max_delay, self.delay * 1.5)

class GoogleAutocompleteParser:
    def __init__(self):
        self.adaptive_delay = AdaptiveDelay()


        self.morph_ru = pymorphy3.MorphAnalyzer(lang='ru')
        self.morph_uk = pymorphy3.MorphAnalyzer(lang='uk')
        
        if NATASHA_AVAILABLE:
            try:
                self.segmenter = Segmenter()
                self.morph_vocab = MorphVocab()
                self.emb = NewsEmbedding()
                self.ner_tagger = NewsNERTagger(self.emb)
                self.natasha_ready = True
                print("‚úÖ Natasha NER initialized for geo-filtering")
            except Exception as e:
                print(f"‚ö†Ô∏è Natasha initialization failed: {e}")
                self.natasha_ready = False
        else:
            self.natasha_ready = False
        
        self.forbidden_geo = FORBIDDEN_GEO

        self.stemmers = {
            'en': SnowballStemmer("english"),
            'de': SnowballStemmer("german"),
            'fr': SnowballStemmer("french"),
            'es': SnowballStemmer("spanish"),
            'it': SnowballStemmer("italian"),
        }

        self.stop_words = {
            'ru': {'–∏', '–≤', '–≤–æ', '–Ω–µ', '–Ω–∞', '—Å', '–æ—Ç', '–¥–ª—è', '–ø–æ', '–æ', '–æ–±', '–∫', '—É', '–∑–∞', 
                   '–∏–∑', '—Å–æ', '–¥–æ', '–ø—Ä–∏', '–±–µ–∑', '–Ω–∞–¥', '–ø–æ–¥', '–∞', '–Ω–æ', '–¥–∞', '–∏–ª–∏', '—á—Ç–æ–±—ã', 
                   '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '–ø–æ—á–µ–º—É'},
            'uk': {'—ñ', '–≤', '–Ω–∞', '–∑', '–≤—ñ–¥', '–¥–ª—è', '–ø–æ', '–æ', '–¥–æ', '–ø—Ä–∏', '–±–µ–∑', '–Ω–∞–¥', '–ø—ñ–¥', 
                   '–∞', '–∞–ª–µ', '—Ç–∞', '–∞–±–æ', '—â–æ', '—è–∫', '–¥–µ', '–∫–æ–ª–∏', '–∫—É–¥–∏', '–∑–≤—ñ–¥–∫–∏', '—á–æ–º—É'},
            'en': {'a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'o', 'with', 'by', 'from', 
                   'up', 'about', 'into', 'through', 'during', 'and', 'or', 'but', 'i', 'when', 
                   'where', 'how', 'why', 'what'},
            'de': {'der', 'die', 'das', 'den', 'dem', 'des', 'ein', 'eine', 'einen', 'einem', 
                   'und', 'oder', 'aber', 'in', 'au', 'von', 'zu', 'mit', 'f√ºr', 'bei', 'nach',
                   'wie', 'wo', 'wann', 'warum', 'was', 'wer'},
            'fr': {'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'et', 'ou', 'mais', 'dans',
                   'sur', 'avec', 'pour', 'par', '√†', 'en', 'au', 'aux', 'ce', 'qui', 'que',
                   'comment', 'o√π', 'quand', 'pourquoi', 'quoi'},
            'es': {'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'de', 'del', 'y', 'o',
                   'pero', 'en', 'con', 'por', 'para', 'a', 'al', 'como', 'que', 'quien',
                   'donde', 'cuando', 'porque', 'qu√©'},
            'it': {'il', 'lo', 'la', 'i', 'gli', 'le', 'un', 'uno', 'una', 'di', 'da', 'e', 'o',
                   'ma', 'in', 'su', 'con', 'per', 'a', 'come', 'che', 'chi', 'dove', 'quando',
                   'perch√©', 'cosa'},
            'pl': {'i', 'w', 'na', 'z', 'do', 'dla', 'po', 'o', 'przy', 'bez', 'nad', 'pod',
                   'a', 'ale', 'lub', 'czy', '≈ºe', 'jak', 'gdzie', 'kiedy', 'dlaczego', 'co'}
        }
        
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: —Ä–∞–Ω—å—à–µ –ø–µ—Ä–µ–¥–∞–≤–∞–ª—Å—è –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å {}
        self.post_filter = BatchPostFilter(
            all_cities_global=ALL_CITIES_GLOBAL,  # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø–µ—Ä–µ–¥–∞—ë–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –±–∞–∑—É
            forbidden_geo=self.forbidden_geo,
            districts=DISTRICTS_EXTENDED,
            population_threshold=5000
        )
        logger.info("‚úÖ Batch Post-Filter v7.9 initialized with REAL cities database")
        logger.info(f"   Database contains {len(ALL_CITIES_GLOBAL)} cities")
        logger.info("   GEO DATABASE = PRIMARY, morphology = secondary")

        # –¢—Ä–∞—Å—Å–∏—Ä–æ–≤—â–∏–∫ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        self.tracer = FilterTracer(enabled=True)
        
        # –§–ª–∞–≥ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è relevance_filter —á–µ—Ä–µ–∑ ?filters=
        self.skip_relevance_filter = False

    def is_city_allowed(self, word: str, target_country: str) -> bool:
        """
        """
        try:
            parsed = self.morph_ru.parse(word.lower())[0]
            lemma = parsed.normal_form
        except:
            lemma = word.lower()
        
        if lemma not in ALL_CITIES_GLOBAL:
            return True
        
        city_country = ALL_CITIES_GLOBAL.get(lemma)  # –ø–æ–ª—É—á–∞–µ–º –∫–æ–¥ —Å—Ç—Ä–∞–Ω—ã (–Ω–∞–ø—Ä. 'ru', 'kz', 'ua')
        
        if city_country == target_country.lower():
            return True  # –ì–æ—Ä–æ–¥ –Ω–∞—à–µ–π —Å—Ç—Ä–∞–Ω—ã ‚Äî —Ä–∞–∑—Ä–µ—à–∞–µ–º
        
        return False  # –ì–æ—Ä–æ–¥ —á—É–∂–æ–π —Å—Ç—Ä–∞–Ω—ã ‚Äî –±–ª–æ–∫–∏—Ä—É–µ–º
    
    def strip_geo_to_anchor(self, text: str, seed: str, target_country: str) -> str:
        """
        """
        import re
        
        seed_words = re.findall(r'[–∞-—è—ëa-z0-9-]+', seed.lower())
        seed_lemmas = set()
        
        for word in seed_words:
            if len(word) < 2:
                continue
            try:
                if any(c in '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è' for c in word):
                    lemma = self.morph_ru.parse(word)[0].normal_form
                    seed_lemmas.add(lemma)
                else:
                    seed_lemmas.add(word)  # –õ–∞—Ç–∏–Ω–∏—Ü–∞ –∫–∞–∫ –µ—Å—Ç—å
            except:
                seed_lemmas.add(word)
        
        text_words = re.findall(r'[–∞-—è—ëa-z0-9-]+', text.lower())
        
        # NEW: –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –≥–æ—Ä–æ–¥ –∏–∑ —Ç–æ–π –∂–µ —Å—Ç—Ä–∞–Ω—ã, —á—Ç–æ –∏ target_country,
        # –ù–ï –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –≤ anchor –≤–æ–æ–±—â–µ
        has_local_city = False
        for word in text_words:
            if len(word) < 3:
                continue
            city_country = ALL_CITIES_GLOBAL.get(word)
            if city_country and city_country == target_country.lower():
                has_local_city = True
                break

        if has_local_city:
            logger.info(
                f"ANCHOR BLOCKED (local city present): text='{text}' | seed='{seed}' | country={target_country}"
            )
            return ""
        
        remaining_words = []
        
        for word in text_words:
            if len(word) < 2:
                remaining_words.append(word)
                continue
            
            try:
                if any(c in '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è' for c in word):
                    word_lemma = self.morph_ru.parse(word)[0].normal_form
                else:
                    word_lemma = word
            except:
                word_lemma = word
            
            if word_lemma in seed_lemmas:
                logger.info(f"üóëÔ∏è SEED REMOVED: '{word}' (lemma: {word_lemma}) from '{text}'")
                continue
            
            remaining_words.append(word)
        
        clean_words = []
        
        for word in remaining_words:
            if len(word) < 2:
                clean_words.append(word)
                continue
            
            try:
                if any(c in '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è' for c in word):
                    lemma = self.morph_ru.parse(word)[0].normal_form
                else:
                    lemma = word
            except:
                lemma = word
            
            city_country_word = ALL_CITIES_GLOBAL.get(word)
            city_country_lemma = ALL_CITIES_GLOBAL.get(lemma)
            
            if city_country_word and city_country_word != target_country.lower():
                logger.info(f"üßº CITY REMOVED: '{word}' (city of {city_country_word}) from anchor")
                continue
            
            if city_country_lemma and city_country_lemma != target_country.lower():
                logger.info(f"üßº CITY REMOVED: '{word}' (lemma '{lemma}' city of {city_country_lemma}) from anchor")
                continue
            
            clean_words.append(word)
        
        anchor = " ".join(clean_words).strip()
        
        if anchor and anchor != text.lower():
            logger.warning(f"‚öì ANCHOR CREATED: '{text}' ‚Üí '{anchor}'")
        
        return anchor

    def detect_seed_language(self, seed: str) -> str:
        """–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ seed"""
        if any('\u0400' <= char <= '\u04FF' for char in seed):
            if any(char in '—ñ—ó—î“ë' for char in seed.lower()):
                return 'uk'
            return 'ru'
        return 'en'

    def get_modifiers(self, language: str, use_numbers: bool, seed: str, cyrillic_only: bool = False) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —è–∑—ã–∫–∞ —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        modifiers = []

        seed_lower = seed.lower()
        has_cyrillic = any('\u0400' <= c <= '\u04FF' for c in seed_lower)
        has_latin = any('a' <= c <= 'z' for c in seed_lower)

        if language.lower() == 'ru':
            modifiers.extend(list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ç—é—è"))
        elif language.lower() == 'uk':
            modifiers.extend(list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—é—è—ñ—ó—î“ë"))

        if not cyrillic_only:
            if has_cyrillic and not has_latin and language.lower() not in ['en', 'de', 'fr', 'es', 'pl']:
                pass
            else:
                modifiers.extend(list("abcdefghijklmnopqrstuvwxyz"))

        if use_numbers:
            modifiers.extend([str(i) for i in range(10)])

        return modifiers

    def get_morphological_forms(self, word: str, language: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ pymorphy3"""
        forms = set([word])

        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                parsed = morph.parse(word)

                if parsed:
                    for form in parsed[0].lexeme:
                        pos = form.tag.POS
                        if pos not in ['PRTS', 'PRTF', 'GRND']:
                            forms.add(form.word)
            except:
                pass
        return sorted(list(forms))

    def is_query_allowed(self, query: str, seed: str, country: str) -> bool:
        """
        v7.6: –ü–û–õ–ù–û–°–¢–¨–Æ –û–¢–ö–õ–Æ–ß–ï–ù - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ BatchPostFilter
        –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç True
        
        –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∏–∂–µ - –º–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å –µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è
        """
        return True
        
        # ============================================
        # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å —Å—Ç–∞—Ä—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
        # ============================================
        # import re
        # 
        # q_lower = query.lower().strip()
        # target_country = country.lower()
        # 
        # for forbidden in self.forbidden_geo:
        #     if forbidden in q_lower:
        #         logger.warning(f"üö´ HARD-BLACKLIST: '{query}' contains '{forbidden}'")
        #         return False
        # 
        # words = re.findall(r'[–∞-—è—ëa-z0-9-]+', q_lower)
        # lemmas = set()
        # 
        # for word in words:
        #     if len(word) < 3:
        #         lemmas.add(word)
        #         continue
        #     
        #     try:
        #         if any(c in '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è' for c in word):
        #             lemma = self.morph_ru.parse(word)[0].normal_form
        #             lemmas.add(lemma)
        #         else:
        #             lemmas.add(word)
        #     except:
        #         lemmas.add(word)
        # 
        # for forbidden in self.forbidden_geo:
        #     if forbidden in lemmas:
        #         logger.warning(f"üö´ HARD-BLACKLIST (lemma): '{query}' ‚Üí lemma '{forbidden}'")
        #         return False
        # 
        # stopwords = ['–∏–∑—Ä–∞–∏–ª—å', '—Ä–æ—Å—Å–∏—è', '–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω', '—É–∑–±–µ–∫–∏—Å—Ç–∞–Ω', '–±–µ–ª–∞—Ä—É—Å—å', '–º–æ–ª–¥–æ–≤–∞']
        # if any(stop in q_lower for stop in stopwords):
        #     if target_country == 'ua' and '—É–∫—Ä–∞–∏–Ω–∞' not in q_lower:
        #         logger.warning(f"üö´ COUNTRY BLOCK: '{query}' contains {[s for s in stopwords if s in q_lower]}")
        #         return False
        # 
        # for word in words:
        #     if len(word) < 3:
        #         continue
        #     
        #     city_country_word = ALL_CITIES_GLOBAL.get(word)
        #     
        #     if city_country_word and city_country_word != target_country:
        #         logger.warning(f"üö´ FAST BLOCK: '{word}' ({city_country_word}) in '{query}'")
        #         return False
        # 
        # for lemma in lemmas:
        #     if len(lemma) < 3:
        #         continue
        #     
        #     city_country_lemma = ALL_CITIES_GLOBAL.get(lemma)
        #     
        #     if city_country_lemma and city_country_lemma != target_country:
        #         logger.warning(f"üö´ FAST BLOCK (lemma): '{lemma}' ({city_country_lemma}) in '{query}'")
        #         return False
        # 
        # if self.natasha_ready and NATASHA_AVAILABLE:
        #     try:
        #         from natasha import Doc
        #         
        #         doc = Doc(query)
        #         doc.segment(self.segmenter)
        #         doc.tag_ner(self.ner_tagger)
        #         
        #         for span in doc.spans:
        #             if span.type == 'LOC':
        #                 span.normalize(self.morph_vocab)
        #                 loc_name = span.normal.lower()
        #                 
        #                 if loc_name in ALL_CITIES_GLOBAL:
        #                     loc_country = ALL_CITIES_GLOBAL[loc_name]
        #                     if loc_country != target_country:
        #                         logger.warning(f"üìç NATASHA BLOCKED: '{loc_name}' ({loc_country}) in '{query}'")
        #                         return False
        #                 else:
        #                     loc_words = loc_name.split()
        #                     for loc_word in loc_words:
        #                         if len(loc_word) < 3:
        #                             continue
        #                         word_country = ALL_CITIES_GLOBAL.get(loc_word)
        #                         if word_country and word_country != target_country:
        #                             logger.warning(f"üìç NATASHA BLOCKED (word): '{loc_word}' ({word_country}) in '{loc_name}'")
        #                             return False
        #                 
        #     except Exception as e:
        #         logger.debug(f"Natasha NER error: {e}")
        # 
        # logger.info(f"‚úÖ ALLOWED: {query}")
        # return True
    
    async def autocorrect_text(self, text: str, language: str) -> Dict:
        """–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ Yandex Speller (ru/uk/en) –∏–ª–∏ LanguageTool (–æ—Å—Ç–∞–ª—å–Ω—ã–µ)"""

        if language.lower() in ['ru', 'uk', 'en']:
            url = "https://speller.yandex.net/services/spellservice.json/checkText"
            lang_map = {'ru': 'ru', 'uk': 'uk', 'en': 'en'}
            yandex_lang = lang_map.get(language.lower(), 'ru')

            params = {"text": text, "lang": yandex_lang, "options": 0}

            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get(url, params=params)

                    if response.status_code == 200:
                        errors = response.json()

                        if not errors:
                            return {"original": text, "corrected": text, "corrections": [], "has_errors": False}

                        corrected = text
                        corrections = []
                        errors_sorted = sorted(errors, key=lambda x: x.get('pos', 0), reverse=True)

                        for error in errors_sorted:
                            word = error.get('word', '')
                            suggestions = error.get('s', [])

                            if suggestions:
                                suggestion = suggestions[0]
                                pos = error.get('pos', 0)
                                corrected = corrected[:pos] + suggestion + corrected[pos + len(word):]
                                corrections.append({"word": word, "suggestion": suggestion})

                        return {
                            "original": text,
                            "corrected": corrected,
                            "corrections": corrections,
                            "has_errors": True
                        }
            except:
                pass
        return await self.autocorrect_languagetool(text, language)

    async def autocorrect_languagetool(self, text: str, language: str) -> Dict:
        """–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ LanguageTool API (30+ —è–∑—ã–∫–æ–≤)"""
        url = "https://api.languagetool.org/v2/check"

        data = {
            "text": text,
            "language": language.lower(),
            "enabledOnly": "false"
        }

        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(url, data=data)

                if response.status_code == 200:
                    result = response.json()
                    matches = result.get('matches', [])

                    if not matches:
                        return {"original": text, "corrected": text, "corrections": [], "has_errors": False}

                    corrected = text
                    corrections = []

                    for match in reversed(matches):
                        offset = match.get('offset', 0)
                        length = match.get('length', 0)
                        replacements = match.get('replacements', [])

                        if replacements:
                            suggestion = replacements[0].get('value', '')
                            word = text[offset:offset+length]
                            corrected = corrected[:offset] + suggestion + corrected[offset+length:]
                            corrections.append({"word": word, "suggestion": suggestion})

                    return {
                        "original": text,
                        "corrected": corrected,
                        "corrections": corrections,
                        "has_errors": True
                    }
        except:
            pass
        return {"original": text, "corrected": text, "corrections": [], "has_errors": False}

    async def fetch_suggestions(self, query: str, country: str, language: str, client: httpx.AsyncClient) -> List[str]:
        """Google Autocomplete"""
        url = "https://www.google.com/complete/search"
        params = {"q": query, "client": "firefox", "hl": language, "gl": country}
        headers = {"User-Agent": random.choice(USER_AGENTS)}

        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)

            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []

            self.adaptive_delay.on_success()

            if response.status_code == 200:
                data = response.json()
                return data[1] if len(data) > 1 else []
        except:
            pass
        return []

    async def fetch_suggestions_yandex(self, query: str, language: str, region_id: int, client: httpx.AsyncClient) -> List[str]:
        """Yandex Suggest"""
        url = "https://suggest-maps.yandex.ru/suggest-geo"

        params = {
            "v": "9",
            "search_type": "tp",
            "part": query,
            "lang": language,
            "n": "10",
            "geo": str(region_id),
            "fullpath": "1"
        }

        headers = {"User-Agent": random.choice(USER_AGENTS)}

        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)

            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []

            self.adaptive_delay.on_success()

            if response.status_code == 200:
                data = response.json()
                results = data.get('results', [])
                return [item.get('text', '') for item in results if item.get('text')]
        except:
            pass
        return []

    async def fetch_suggestions_bing(self, query: str, language: str, country: str, client: httpx.AsyncClient) -> List[str]:
        """Bing Autosuggest"""
        url = "https://www.bing.com/AS/Suggestions"

        params = {
            "q": query,
            "mkt": f"{language}-{country}",
            "cvid": "0",
            "qry": query
        }

        headers = {"User-Agent": random.choice(USER_AGENTS)}

        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)

            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []

            self.adaptive_delay.on_success()

            if response.status_code == 200:
                data = response.json()
                suggestion_groups = data.get('AS', {}).get('Results', [])

                suggestions = []
                for group in suggestion_groups:
                    for item in group.get('Suggests', []):
                        text = item.get('Txt', '')
                        if text:
                            suggestions.append(text)

                return suggestions
        except:
            pass
        return []

    async def parse_with_semaphore(self, queries: List[str], country: str, language: str, 
                                   parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""

        semaphore = asyncio.Semaphore(parallel_limit)
        all_keywords = set()
        success_count = 0
        failed_count = 0

        async def fetch_with_limit(query: str, client: httpx.AsyncClient):
            nonlocal success_count, failed_count

            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())

                if source == "google":
                    results = await self.fetch_suggestions(query, country, language, client)
                elif source == "yandex":
                    results = await self.fetch_suggestions_yandex(query, language, region_id, client)
                elif source == "bing":
                    results = await self.fetch_suggestions_bing(query, language, country, client)
                else:
                    results = []

                if results:
                    all_keywords.update(results)
                    success_count += 1
                else:
                    failed_count += 1

                return results

        async with httpx.AsyncClient() as client:
            tasks = [fetch_with_limit(q, client) for q in queries]
            await asyncio.gather(*tasks)

        return {
            "keywords": sorted(list(all_keywords)),
            "success": success_count,
            "failed": failed_count
        }

    async def parse_suffix(self, seed: str, country: str, language: str, use_numbers: bool, 
                          parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """SUFFIX –º–µ—Ç–æ–¥: seed + –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä"""
        start_time = time.time()

        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]

        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)

        keywords = set()
        internal_anchors = set()
        
        for kw in result_raw['keywords']:
            if not self.is_query_allowed(kw, seed, country):
                anchor = self.strip_geo_to_anchor(kw, seed, country)
                logger.debug(
                    f"[SUFFIX] BLOCK by is_query_allowed | kw='{kw}' | anchor='{anchor}' "
                    f"| seed='{seed}' | country={country}"
                )
                if anchor and anchor != seed.lower() and len(anchor) > 5:
                    internal_anchors.add(anchor)
                continue  # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –º—É—Å–æ—Ä –≤ keywords
            
            keywords.add(kw)
        
        all_with_anchors = keywords | internal_anchors
        if not self.skip_relevance_filter:
            filtered = await filter_relevant_keywords(list(all_with_anchors), seed, language)
        else:
            filtered = list(all_with_anchors)
        
        filtered_set = set(filtered)
        final_keywords = sorted(list(keywords & filtered_set))
        final_anchors = sorted(list(internal_anchors & filtered_set))
        
        logger.info(
            f"[SUFFIX] BEFORE BPF | seed='{seed}' | country={country} | "
            f"final_keywords={len(final_keywords)} | final_anchors={len(final_anchors)}"
        )
        logger.debug(f"[SUFFIX] final_keywords={final_keywords}")
        logger.debug(f"[SUFFIX] final_anchors={final_anchors}")
        
        # === BPF –ü–ï–†–ï–ù–ï–°–Å–ù –í apply_filters_traced (endpoint) ===
        # batch_result = self.post_filter.filter_batch(
        #     keywords=final_keywords,
        #     seed=seed,
        #     country=country,
        #     language=language
        # )
        # combined_anchors = set(final_anchors) | set(batch_result['anchors'])

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "suffix",
            "source": source,
            "keywords": final_keywords,
            "anchors": sorted(list(final_anchors)),
            "count": len(final_keywords),
            "anchors_count": len(final_anchors),
            "queries": len(queries),
            "elapsed_time": round(elapsed, 2),
            "batch_stats": {}
        }

    async def parse_infix(self, seed: str, country: str, language: str, use_numbers: bool, 
                         parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """INFIX –º–µ—Ç–æ–¥: –≤—Å—Ç–∞–≤–∫–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏"""
        start_time = time.time()

        words = seed.strip().split()

        if len(words) < 2:
            return {"error": "INFIX —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞", "seed": seed}

        modifiers = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
        queries = []

        for i in range(1, len(words)):
            for mod in modifiers:
                query = ' '.join(words[:i]) + f' {mod} ' + ' '.join(words[i:])
                queries.append(query)

        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)

        keywords = set()
        internal_anchors = set()
        
        for kw in result_raw['keywords']:
            if not self.is_query_allowed(kw, seed, country):
                anchor = self.strip_geo_to_anchor(kw, seed, country)
                logger.debug(
                    f"[INFIX] BLOCK by is_query_allowed | kw='{kw}' | anchor='{anchor}' "
                    f"| seed='{seed}' | country={country}"
                )
                if anchor and anchor != seed.lower() and len(anchor) > 5:
                    internal_anchors.add(anchor)
                continue  # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –º—É—Å–æ—Ä –≤ keywords
            
            keywords.add(kw)
        
        all_with_anchors = keywords | internal_anchors
        filtered_1 = await filter_infix_results(list(all_with_anchors), language)

        if not self.skip_relevance_filter:
            filtered_2 = await filter_relevant_keywords(filtered_1, seed, language)
        else:
            filtered_2 = filtered_1
        
        filtered_set = set(filtered_2)
        final_keywords = sorted(list(keywords & filtered_set))
        final_anchors = sorted(list(internal_anchors & filtered_set))
        
        logger.info(
            f"[INFIX] BEFORE BPF | seed='{seed}' | country={country} | "
            f"final_keywords={len(final_keywords)} | final_anchors={len(final_anchors)}"
        )
        logger.debug(f"[INFIX] final_keywords={final_keywords}")
        logger.debug(f"[INFIX] final_anchors={final_anchors}")
        
        # === BPF –ü–ï–†–ï–ù–ï–°–Å–ù –í apply_filters_traced (endpoint) ===
        # batch_result = self.post_filter.filter_batch(
        #     keywords=final_keywords,
        #     seed=seed,
        #     country=country,
        #     language=language
        # )
        # combined_anchors = set(final_anchors) | set(batch_result['anchors'])

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "infix",
            "source": source,
            "keywords": final_keywords,
            "anchors": sorted(list(final_anchors)),
            "count": len(final_keywords),
            "anchors_count": len(final_anchors),
            "queries": len(queries),
            "elapsed_time": round(elapsed, 2),
            "batch_stats": {}
        }

    async def parse_morphology(self, seed: str, country: str, language: str, use_numbers: bool, 
                               parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """MORPHOLOGY –º–µ—Ç–æ–¥: –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–æ—Ä–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö"""
        start_time = time.time()

        words = seed.strip().split()

        nouns_to_modify = []

        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                for idx, word in enumerate(words):
                    parsed = morph.parse(word)
                    if parsed and parsed[0].tag.POS == 'NOUN':
                        nouns_to_modify.append({
                            'index': idx,
                            'word': word,
                            'forms': self.get_morphological_forms(word, language)
                        })

                if not nouns_to_modify:
                    last_word = words[-1]
                    nouns_to_modify.append({
                        'index': len(words) - 1,
                        'word': last_word,
                        'forms': self.get_morphological_forms(last_word, language)
                    })
            except:
                last_word = words[-1]
                nouns_to_modify.append({
                    'index': len(words) - 1,
                    'word': last_word,
                    'forms': self.get_morphological_forms(last_word, language)
                })
        else:
            last_word = words[-1]
            nouns_to_modify.append({
                'index': len(words) - 1,
                'word': last_word,
                'forms': self.get_morphological_forms(last_word, language)
            })

        all_seeds = []
        if len(nouns_to_modify) >= 1:
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))

        unique_seeds = list(set(all_seeds))

        all_keywords = set()
        modifiers = self.get_modifiers(language, use_numbers, seed)

        for seed_variant in unique_seeds:
            queries = [f"{seed_variant} {mod}" for mod in modifiers]
            result = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)
            all_keywords.update(result['keywords'])

        keywords = set()
        internal_anchors = set()
        
        for kw in all_keywords:
            if not self.is_query_allowed(kw, seed, country):
                anchor = self.strip_geo_to_anchor(kw, seed, country)
                logger.debug(
                    f"[MORPH] BLOCK by is_query_allowed | kw='{kw}' | anchor='{anchor}' "
                    f"| seed='{seed}' | country={country}"
                )
                if anchor and anchor != seed.lower() and len(anchor) > 5:
                    internal_anchors.add(anchor)
                continue  # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –º—É—Å–æ—Ä –≤ keywords
            
            keywords.add(kw)
        
        all_with_anchors = keywords | internal_anchors
        if not self.skip_relevance_filter:
            filtered = await filter_relevant_keywords(sorted(list(all_with_anchors)), seed, language)
        else:
            filtered = sorted(list(all_with_anchors))
        
        filtered_set = set(filtered)
        final_keywords = sorted(list(keywords & filtered_set))
        final_anchors = sorted(list(internal_anchors & filtered_set))
        
        logger.info(
            f"[MORPH] BEFORE BPF | seed='{seed}' | country={country} | "
            f"final_keywords={len(final_keywords)} | final_anchors={len(final_anchors)}"
        )
        logger.debug(f"[MORPH] final_keywords={final_keywords}")
        logger.debug(f"[MORPH] final_anchors={final_anchors}")
        
        # === BPF –ü–ï–†–ï–ù–ï–°–Å–ù –í apply_filters_traced (endpoint) ===
        # batch_result = self.post_filter.filter_batch(
        #     keywords=final_keywords,
        #     seed=seed,
        #     country=country,
        #     language=language
        # )
        # combined_anchors = set(final_anchors) | set(batch_result['anchors'])

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "morphology",
            "source": source,
            "keywords": final_keywords,
            "anchors": sorted(list(final_anchors)),
            "count": len(final_keywords),
            "anchors_count": len(final_anchors),
            "elapsed_time": round(elapsed, 2),
            "batch_stats": {}
        }

    async def parse_light_search(self, seed: str, country: str, language: str, use_numbers: bool, 
                                 parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """LIGHT SEARCH: –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ (SUFFIX + INFIX)"""
        start_time = time.time()

        suffix_result = await self.parse_suffix(seed, country, language, use_numbers, parallel_limit, source, region_id)
        infix_result = await self.parse_infix(seed, country, language, use_numbers, parallel_limit, source, region_id)

        all_keywords = set(suffix_result["keywords"]) | set(infix_result.get("keywords", []))
        all_anchors = set(suffix_result.get("anchors", [])) | set(infix_result.get("anchors", []))

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "light_search",
            "source": source,
            "keywords": sorted(list(all_keywords)),
            "anchors": sorted(list(all_anchors)),
            "count": len(all_keywords),
            "anchors_count": len(all_anchors),
            "suffix_count": len(suffix_result["keywords"]),
            "infix_count": len(infix_result.get("keywords", [])),
            "elapsed_time": round(elapsed, 2)
        }

    async def parse_adaptive_prefix(self, seed: str, country: str, language: str, use_numbers: bool, 
                                    parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """ADAPTIVE PREFIX –º–µ—Ç–æ–¥: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ –∏–∑ SUFFIX + PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞"""
        start_time = time.time()

        seed_words = set(seed.lower().split())

        prefixes = ["", "–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "–æ—Ç–∑—ã–≤—ã"]
        queries = []
        for p in prefixes:
            q = f"{p} {seed}".strip()
            if self.is_query_allowed(q, seed, country):
                queries.append(q)
        
        alphabet = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
        for char in alphabet:
            q_ext = f"{seed} {char}".strip()
            if self.is_query_allowed(q_ext, seed, country):
                queries.append(q_ext)

        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)

        from collections import Counter
        word_counter = Counter()

        for result in result_raw['keywords']:
            result_words = result.lower().split()
            for word in result_words:
                if word not in seed_words and len(word) > 2:
                    word_counter[word] += 1

        candidates = {w for w, count in word_counter.items() if count >= 2}

        keywords = set()
        internal_anchors = set()
        verified_prefixes = []

        for candidate in sorted(candidates):
            query = f"{candidate} {seed}"

            if not self.is_query_allowed(query, seed, country):
                continue

            result = await self.parse_with_semaphore([query], country, language, parallel_limit, source, region_id)
            
            if result['keywords']:
                verified_prefixes.append(candidate)
                
                for kw in result['keywords']:
                    if not self.is_query_allowed(kw, seed, country):
                        anchor = self.strip_geo_to_anchor(kw, seed, country)
                        logger.debug(
                            f"[ADAPTIVE_PREFIX] BLOCK by is_query_allowed | kw='{kw}' | anchor='{anchor}' "
                            f"| seed='{seed}' | country={country}"
                        )
                        if anchor and anchor != seed.lower() and len(anchor) > 5:
                            internal_anchors.add(anchor)
                        continue  # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –º—É—Å–æ—Ä –≤ keywords
                    
                    keywords.add(kw)
        
        all_with_anchors = keywords | internal_anchors
        if not self.skip_relevance_filter:
            filtered = await filter_relevant_keywords(sorted(list(all_with_anchors)), seed, language)
        else:
            filtered = sorted(list(all_with_anchors))
        
        filtered_set = set(filtered)
        final_keywords = sorted(list(keywords & filtered_set))
        final_anchors = sorted(list(internal_anchors & filtered_set))
        
        logger.info(
            f"[ADAPTIVE_PREFIX] BEFORE BPF | seed='{seed}' | country={country} | "
            f"final_keywords={len(final_keywords)} | final_anchors={len(final_anchors)}"
        )
        logger.debug(f"[ADAPTIVE_PREFIX] final_keywords={final_keywords}")
        logger.debug(f"[ADAPTIVE_PREFIX] final_anchors={final_anchors}")
        
        # === BPF –ü–ï–†–ï–ù–ï–°–Å–ù –í apply_filters_traced (endpoint) ===
        # batch_result = self.post_filter.filter_batch(
        #     keywords=final_keywords,
        #     seed=seed,
        #     country=country,
        #     language=language
        # )
        # combined_anchors = set(final_anchors) | set(batch_result['anchors'])

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "adaptive_prefix",
            "source": source,
            "keywords": final_keywords,
            "anchors": sorted(list(final_anchors)),
            "count": len(final_keywords),
            "anchors_count": len(final_anchors),
            "candidates_found": len(candidates),
            "verified_prefixes": verified_prefixes,
            "elapsed_time": round(elapsed, 2),
            "batch_stats": {}
        }

    async def parse_deep_search(self, seed: str, country: str, region_id: int, language: str, 
                                use_numbers: bool, parallel_limit: int, include_keywords: bool) -> Dict:
        """DEEP SEARCH: –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ (–≤—Å–µ 4 –º–µ—Ç–æ–¥–∞ –ò–ó –í–°–ï–• 3 –ò–°–¢–û–ß–ù–ò–ö–û–í)"""

        correction = await self.autocorrect_text(seed, language)
        original_seed = seed

        if correction.get("has_errors"):
            seed = correction["corrected"]

        start_time = time.time()
        
        sources = ["google", "yandex", "bing"]
        all_keywords_by_source = {}
        all_anchors_by_source = {}
        
        for source in sources:
            suffix_result = await self.parse_suffix(seed, country, language, use_numbers, parallel_limit, source, region_id)
            infix_result = await self.parse_infix(seed, country, language, use_numbers, parallel_limit, source, region_id)
            morph_result = await self.parse_morphology(seed, country, language, use_numbers, parallel_limit, source, region_id)
            prefix_result = await self.parse_adaptive_prefix(seed, country, language, use_numbers, parallel_limit, source, region_id)
            
            all_keywords_by_source[source] = {
                "suffix": set(suffix_result["keywords"]),
                "infix": set(infix_result.get("keywords", [])),
                "morphology": set(morph_result["keywords"]),
                "adaptive_prefix": set(prefix_result["keywords"])
            }
            
            all_anchors_by_source[source] = {
                "suffix": set(suffix_result.get("anchors", [])),
                "infix": set(infix_result.get("anchors", [])),
                "morphology": set(morph_result.get("anchors", [])),
                "adaptive_prefix": set(prefix_result.get("anchors", []))
            }
        
        all_unique_keywords = set()
        all_unique_anchors = set()
        
        for source in sources:
            for method_kw in all_keywords_by_source[source].values():
                all_unique_keywords |= method_kw
            for method_anchors in all_anchors_by_source[source].values():
                all_unique_anchors |= method_anchors
        
        logger.info(f"[Deep Search] Before final filter: {len(all_unique_keywords)} keywords")
        
        # === BPF –ü–ï–†–ï–ù–ï–°–Å–ù –í apply_filters_traced (endpoint) ===
        # final_filter = self.post_filter.filter_batch(
        #     keywords=list(all_unique_keywords),
        #     seed=seed,
        #     country=country,
        #     language=language
        # )
        # all_unique_keywords = set(final_filter['keywords'])
        # all_unique_anchors = set(final_filter['anchors']) | all_unique_anchors
        
        logger.info(f"[Deep Search] After merge: {len(all_unique_keywords)} keywords, {len(all_unique_anchors)} anchors")

        final_keywords = sorted(list(all_unique_keywords))
        normalized_keywords = normalize_keywords(final_keywords, language, seed)

        elapsed = time.time() - start_time

        response = {
            "seed": original_seed,
            "corrected_seed": seed if correction.get("has_errors") else None,
            "corrections": correction.get("corrections", []) if correction.get("has_errors") else [],
            "keywords": normalized_keywords,
            "anchors": sorted(list(all_unique_anchors)),
            "count": len(normalized_keywords),
            "anchors_count": len(all_unique_anchors),
            "sources": sources,
            "total_unique_keywords": len(normalized_keywords),
            "total_anchors": len(all_unique_anchors),
            "results_by_source": {
                source: {
                    "count": sum(len(kw) for kw in all_keywords_by_source[source].values())
                }
                for source in sources
            },
            "sources_stats": {
                source: {
                    "keywords": sum(len(kw) for kw in all_keywords_by_source[source].values()),
                    "anchors": sum(len(anch) for anch in all_anchors_by_source[source].values())
                }
                for source in sources
            },
            "elapsed_time": round(elapsed, 2)
        }

        if include_keywords:
            response["keywords_detailed"] = {
                **{
                    source: {
                        "all": sorted(list(set.union(*all_keywords_by_source[source].values()))),
                        "suffix": sorted(list(all_keywords_by_source[source]["suffix"])),
                        "infix": sorted(list(all_keywords_by_source[source]["infix"])),
                        "morphology": sorted(list(all_keywords_by_source[source]["morphology"])),
                        "adaptive_prefix": sorted(list(all_keywords_by_source[source]["adaptive_prefix"]))
                    }
                    for source in sources
                }
            }
            response["anchors_detailed"] = {
                **{
                    source: {
                        "all": sorted(list(set.union(*all_anchors_by_source[source].values()))),
                        "suffix": sorted(list(all_anchors_by_source[source]["suffix"])),
                        "infix": sorted(list(all_anchors_by_source[source]["infix"])),
                        "morphology": sorted(list(all_anchors_by_source[source]["morphology"])),
                        "adaptive_prefix": sorted(list(all_anchors_by_source[source]["adaptive_prefix"]))
                    }
                    for source in sources
                }
            }

        return response

parser = GoogleAutocompleteParser()

def apply_smart_fix(result: dict, seed: str, language: str):
    """
    –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    –£–õ–£–ß–®–ï–ù–ò–Ø:
    - –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è seed –ø–µ—Ä–µ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π (golden base)
    - –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ dict.fromkeys
    """
    if result.get("keywords"):
        raw_keywords = result["keywords"]
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º seed –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è golden base
        golden_seed = seed
        if language in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer(lang=language)
                lemmatized_words = []
                for word in seed.split():
                    parsed = morph.parse(word)
                    if parsed:
                        lemmatized_words.append(parsed[0].normal_form)
                    else:
                        lemmatized_words.append(word)
                golden_seed = " ".join(lemmatized_words)
            except:
                golden_seed = seed
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å golden base
        norm_keywords = normalize_keywords(raw_keywords, language, golden_seed)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (—Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫)
        result["keywords"] = list(dict.fromkeys(norm_keywords))
        
        total = len(result["keywords"])
        if "count" in result: result["count"] = total
        if "total_count" in result: result["total_count"] = total
        if "total_unique_keywords" in result: result["total_unique_keywords"] = total
            
    return result

@app.get("/")
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return FileResponse('static/index.html')


def _build_l2_config(pmi_valid=None, centroid_valid=None, centroid_trash=None):
    """–°–æ–±–∏—Ä–∞–µ—Ç L2 config –∏–∑ query –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (None = –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç)."""
    from filters.l2_filter import L2Config
    
    config = L2Config()
    
    if pmi_valid is not None:
        config.pmi_valid_threshold = pmi_valid
    if centroid_valid is not None:
        config.centroid_valid_threshold = centroid_valid
    if centroid_trash is not None:
        config.centroid_trash_threshold = centroid_trash
    
    return config


def apply_filters_traced(result: dict, seed: str, country: str, 
                          method: str, language: str = "ru", deduplicate: bool = False,
                          enabled_filters: str = "pre,geo,bpf", l2_config = None) -> dict:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ü–µ–ø–æ—á–∫—É —Ñ–∏–ª—å—Ç—Ä–æ–≤ —Å —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–æ–π.
    –ü–æ—Ä—è–¥–æ–∫: pre_filter ‚Üí geo_garbage ‚Üí BPF ‚Üí deduplicate ‚Üí L0 ‚Üí L2
    –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª—é—á–∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ result["anchors"] —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–∞.
    
    enabled_filters: —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –∫–∞–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã –≤–∫–ª—é—á–µ–Ω—ã.
        "pre"  = pre_filter
        "geo"  = geo_garbage_filter  
        "bpf"  = batch_post_filter
        "l0"   = L0 tail classifier
        "l2"   = L2 Tri-Signal classifier (PMI + Centroid + L0 signals)
        "none" = –≤—Å–µ –≤—ã–∫–ª—é—á–µ–Ω—ã (—Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ)
        "all" –∏–ª–∏ "pre,geo,bpf,l0,l2" = –≤—Å–µ –≤–∫–ª—é—á–µ–Ω—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    """
    # –ü–∞—Ä—Å–∏–º —Ñ–ª–∞–≥–∏
    ef = enabled_filters.lower().strip()
    if ef == "all":
        ef = "pre,geo,bpf,l0,l2"
    parts = [x.strip() for x in ef.split(",")]
    run_pre = "pre" in parts
    run_geo = "geo" in parts
    run_bpf = "bpf" in parts
    run_l0 = "l0" in parts
    run_l2 = "l2" in parts
    
    logger.info(f"[FILTERS] enabled_filters='{enabled_filters}' ‚Üí pre={run_pre} geo={run_geo} bpf={run_bpf} l0={run_l0} l2={run_l2}")
    
    parser.tracer.start_request(seed=seed, country=country, method=method)
    
    if "anchors" not in result:
        result["anchors"] = []
    
    before_set = set(k.lower().strip() if isinstance(k, str) else k.get("query","").lower().strip() for k in result.get("keywords", []))
    
    # PRE-–§–ò–õ–¨–¢–†
    if run_pre:
        parser.tracer.before_filter("pre_filter", result.get("keywords", []))
        result = apply_pre_filter(result, seed=seed)
        parser.tracer.after_filter("pre_filter", result.get("keywords", []))
        
        after_set = set(k.lower().strip() if isinstance(k, str) else k.get("query","").lower().strip() for k in result.get("keywords", []))
        for kw in (before_set - after_set):
            result["anchors"].append(kw)
        before_set = after_set
    
    # –ì–ï–û-–§–ò–õ–¨–¢–†
    if run_geo:
        parser.tracer.before_filter("geo_garbage_filter", result.get("keywords", []))
        result = filter_geo_garbage(result, seed=seed, target_country=country)
        parser.tracer.after_filter("geo_garbage_filter", result.get("keywords", []))
        
        after_set = set(k.lower().strip() if isinstance(k, str) else k.get("query","").lower().strip() for k in result.get("keywords", []))
        for kw in (before_set - after_set):
            result["anchors"].append(kw)
        before_set = after_set
    
    # BATCH POST-FILTER
    if run_bpf:
        parser.tracer.before_filter("batch_post_filter", result.get("keywords", []))
        bpf_result = parser.post_filter.filter_batch(
            keywords=result.get("keywords", []),
            seed=seed,
            country=country,
            language=language
        )
        result["keywords"] = bpf_result["keywords"]
        parser.tracer.after_filter("batch_post_filter", result.get("keywords", []))
        
        after_set = set(k.lower().strip() if isinstance(k, str) else k.get("query","").lower().strip() for k in result.get("keywords", []))
        for kw in (before_set - after_set):
            result["anchors"].append(kw)
    
    # –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if deduplicate:
        parser.tracer.before_filter("deduplicate", result.get("keywords", []))
        result = deduplicate_final_results(result)
        parser.tracer.after_filter("deduplicate", result.get("keywords", []))
    
    # L0 –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† (–ø–æ—Å–ª–µ–¥–Ω–∏–π –≤ —Ü–µ–ø–æ—á–∫–µ)
    if run_l0:
        parser.tracer.before_filter("l0_filter", result.get("keywords", []))
        
        result = apply_l0_filter(
            result,
            seed=seed,
            target_country=country,
            geo_db=GEO_DB,
            brand_db=BRAND_DB,
        )
        
        # –¢—Ä–µ–π—Å–µ—Ä L0 ‚Äî —Ç—Ä–∏ –∏—Å—Ö–æ–¥–∞
        l0_trace = result.get("_l0_trace", [])
        l0_trash = [r["keyword"] for r in l0_trace if r.get("label") == "TRASH"]
        
        parser.tracer.after_l0_filter(
            valid=result.get("keywords", []),
            trash=l0_trash,
            grey=result.get("keywords_grey", []),
            l0_trace=l0_trace,
        )
    
    # L2 –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† (–ø–æ—Å–ª–µ L0, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç GREY)
    if run_l2 and result.get("keywords_grey"):
        parser.tracer.before_filter("l2_filter", result.get("keywords_grey", []))
        
        result = apply_l2_filter(
            result,
            seed=seed,
            enable_l2=True,
            config=l2_config,
        )
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã L2
        l2_stats = result.get("l2_stats", {})
        l2_trace = result.get("_l2_trace", [])
        
        logger.info(
            f"[L2] VALID: {l2_stats.get('l2_valid', 0)}, "
            f"TRASH: {l2_stats.get('l2_trash', 0)}, "
            f"GREY remaining: {l2_stats.get('l2_grey', 0)} "
            f"({l2_stats.get('reduction_pct', 0)}% reduction)"
        )
        
        # –¢—Ä–µ–π—Å–µ—Ä L2 ‚Äî —Ç—Ä–∏ –∏—Å—Ö–æ–¥–∞
        parser.tracer.after_l2_filter(
            valid=result.get("keywords", []),
            trash=[a for a in result.get("anchors", []) if isinstance(a, dict) and a.get("anchor_reason") == "L2_TRASH"],
            grey=result.get("keywords_grey", []),
            l2_stats=l2_stats,
            l2_trace=l2_trace,
        )
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è anchors
    seen = set()
    unique_anchors = []
    for a in result["anchors"]:
        if isinstance(a, str):
            key = a.lower().strip()
        elif isinstance(a, dict):
            key = (a.get("keyword") or a.get("query") or "").lower().strip()
        else:
            key = ""
        if key and key not in seen:
            seen.add(key)
            unique_anchors.append(a)
    result["anchors"] = unique_anchors  # –ù–ï —Å–æ—Ä—Ç–∏—Ä—É–µ–º ‚Äî dict'—ã –Ω–µ —Å–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è
    result["anchors_count"] = len(unique_anchors)
    
    result["_trace"] = parser.tracer.finish_request()
    result["_filters_enabled"] = {"pre": run_pre, "geo": run_geo, "bpf": run_bpf, "l0": run_l0, "l2": run_l2, "rel": not parser.skip_relevance_filter}
    return result


@app.get("/api/trace/last")
async def get_last_trace():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç—á—ë—Ç —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏"""
    return parser.tracer.finish_request() if parser.tracer.stages else {"message": "No trace available"}


@app.get("/api/trace/keyword")
async def trace_keyword(keyword: str = Query(..., description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏")):
    """–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã"""
    return parser.tracer.get_keyword_trace(keyword)


@app.get("/api/trace/toggle")
async def toggle_tracer(enabled: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫—É")):
    """–í–∫–ª—é—á–µ–Ω–∏–µ/–≤—ã–∫–ª—é—á–µ–Ω–∏–µ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏"""
    parser.tracer.enabled = enabled
    return {"tracer_enabled": enabled}


@app.get("/debug/l2-diag")
async def l2_diagnostic():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç L2 diagnostic dump (centroid distances, PMI, decisions)."""
    import json as _json
    try:
        with open("l2_diagnostic.json", "r", encoding="utf-8") as f:
            return _json.load(f)
    except FileNotFoundError:
        return {"error": "l2_diagnostic.json not found ‚Äî run a search with L2 enabled first"}
    except Exception as e:
        return {"error": str(e)}


@app.get("/debug/l0-trace")
async def l0_trace_endpoint(
    label: str = Query("all", description="–§–∏–ª—å—Ç—Ä: all / valid / trash / grey / no_seed"),
    tail: str = Query(None, description="–ü–æ–∏—Å–∫ –ø–æ tail (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞)"),
    keyword: str = Query(None, description="–ü–æ–∏—Å–∫ –ø–æ keyword (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞)"),
):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç L0 diagnostic trace ‚Äî tail extraction + detector signals –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞.
    
    –ü—Ä–∏–º–µ—Ä—ã:
        /debug/l0-trace                     ‚Äî –≤—Å–µ –∫–ª—é—á–∏
        /debug/l0-trace?label=trash         ‚Äî —Ç–æ–ª—å–∫–æ TRASH
        /debug/l0-trace?tail=–±—É             ‚Äî –∫–ª—é—á–∏ —Å tail —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "–±—É"
        /debug/l0-trace?keyword=–∞–≤–∏—Ç–æ       ‚Äî –∫–ª—é—á–∏ —Å keyword —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "–∞–≤–∏—Ç–æ"
        /debug/l0-trace?label=no_seed       ‚Äî –∫–ª—é—á–∏ –≥–¥–µ seed –Ω–µ –Ω–∞–π–¥–µ–Ω
    """
    import json as _json
    try:
        with open("l0_diagnostic.json", "r", encoding="utf-8") as f:
            diag = _json.load(f)
    except FileNotFoundError:
        return {"error": "l0_diagnostic.json not found ‚Äî run a search with L0 enabled first"}
    except Exception as e:
        return {"error": str(e)}
    
    traces = diag.get("trace", [])
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ label
    if label != "all":
        if label == "no_seed":
            traces = [t for t in traces if t.get("tail") is None]
        else:
            traces = [t for t in traces if t.get("label", "").lower() == label.lower()]
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ tail –ø–æ–¥—Å—Ç—Ä–æ–∫–µ
    if tail:
        tail_lower = tail.lower()
        traces = [t for t in traces if t.get("tail") and tail_lower in t["tail"].lower()]
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ keyword –ø–æ–¥—Å—Ç—Ä–æ–∫–µ
    if keyword:
        kw_lower = keyword.lower()
        traces = [t for t in traces if kw_lower in t.get("keyword", "").lower()]
    
    return {
        "seed": diag.get("seed"),
        "target_country": diag.get("target_country"),
        "stats": diag.get("stats"),
        "filter": {"label": label, "tail": tail, "keyword": keyword},
        "filtered_count": len(traces),
        "trace": traces,
    }


@app.get("/api/light-search")
async def light_search_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing"),
    filters: str = Query("all", description="–§–∏–ª—å—Ç—Ä—ã: all / none / pre,geo,bpf,rel,l0,l2"),
    # L2 –ø–æ—Ä–æ–≥–∏ (Tri-Signal)
    l2_pmi_valid: float = Query(None, description="L2: PMI VALID threshold"),
    l2_centroid_valid: float = Query(None, description="L2: Centroid VALID threshold"),
    l2_centroid_trash: float = Query(None, description="L2: Centroid TRASH threshold"),
):
    """LIGHT SEARCH: –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ (SUFFIX + INFIX)"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ relevance_filter (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤–Ω—É—Ç—Ä–∏ parse-–º–µ—Ç–æ–¥–æ–≤)
    ef = filters.lower().strip()
    parser.skip_relevance_filter = ("rel" not in ef) and (ef != "all")

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_light_search(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    # –°–æ–±–∏—Ä–∞–µ–º L2 config –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    l2_config = _build_l2_config(l2_pmi_valid, l2_centroid_valid, l2_centroid_trash)
    
    result = apply_filters_traced(result, seed, country, method="light-search", language=language, enabled_filters=filters, l2_config=l2_config)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return apply_smart_fix(result, seed, language)

@app.get("/api/deep-search")
async def deep_search_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (ua/us/de...)"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex (143=–ö–∏–µ–≤)"),
    language: str = Query("auto", description="–Ø–∑—ã–∫ (auto/ru/uk/en)"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤", alias="parallel"),
    include_keywords: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–π"),
    filters: str = Query("all", description="–§–∏–ª—å—Ç—Ä—ã: all / none / pre,geo,bpf,rel,l0,l2"),
    # L2 –ø–æ—Ä–æ–≥–∏ (Tri-Signal)
    l2_pmi_valid: float = Query(None, description="L2: PMI VALID threshold"),
    l2_centroid_valid: float = Query(None, description="L2: Centroid VALID threshold"),
    l2_centroid_trash: float = Query(None, description="L2: Centroid TRASH threshold"),
):
    """DEEP SEARCH: –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ (–≤—Å–µ 4 –º–µ—Ç–æ–¥–∞ –ò–ó –í–°–ï–• 3 –ò–°–¢–û–ß–ù–ò–ö–û–í)"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    ef = filters.lower().strip()
    parser.skip_relevance_filter = ("rel" not in ef) and (ef != "all")

    result = await parser.parse_deep_search(seed, country, region_id, language, use_numbers, parallel_limit, include_keywords)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π seed –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ (parse_deep_search –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —á–µ—Ä–µ–∑ Yandex Speller)
    filter_seed = result.get("corrected_seed") or seed
    
    # –°–æ–±–∏—Ä–∞–µ–º L2 config –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    l2_config = _build_l2_config(l2_pmi_valid, l2_centroid_valid, l2_centroid_trash)
    
    result = apply_filters_traced(result, filter_seed, country, method="deep-search", language=language, deduplicate=True, enabled_filters=filters, l2_config=l2_config)
    
    return result

@app.get("/api/compare")
async def compare_methods(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (ua/us/de...)"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex (143=–ö–∏–µ–≤)"),
    language: str = Query("auto", description="–Ø–∑—ã–∫ (auto/ru/uk/en)"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    include_keywords: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–π"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """[DEPRECATED] –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /api/deep-search"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    return await parser.parse_deep_search(seed, country, region_id, language, use_numbers, parallel_limit, include_keywords, source)

@app.get("/api/parse/suffix")
async def parse_suffix_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing"),
    filters: str = Query("all", description="–§–∏–ª—å—Ç—Ä—ã: all / none / pre,geo,bpf,rel")
):
    """–¢–æ–ª—å–∫–æ SUFFIX –º–µ—Ç–æ–¥"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    ef = filters.lower().strip()
    parser.skip_relevance_filter = ("rel" not in ef) and (ef != "all")

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_suffix(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    result = apply_filters_traced(result, seed, country, method="suffix", language=language, enabled_filters=filters)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return apply_smart_fix(result, seed, language)

@app.get("/api/parse/infix")
async def parse_infix_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞)"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing"),
    filters: str = Query("all", description="–§–∏–ª—å—Ç—Ä—ã: all / none / pre,geo,bpf,rel")
):
    """–¢–æ–ª—å–∫–æ INFIX –º–µ—Ç–æ–¥"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    ef = filters.lower().strip()
    parser.skip_relevance_filter = ("rel" not in ef) and (ef != "all")

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_infix(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    result = apply_filters_traced(result, seed, country, method="infix", language=language, enabled_filters=filters)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return apply_smart_fix(result, seed, language)

@app.get("/api/parse/morphology")
async def parse_morphology_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing"),
    filters: str = Query("all", description="–§–∏–ª—å—Ç—Ä—ã: all / none / pre,geo,bpf,rel")
):
    """–¢–æ–ª—å–∫–æ MORPHOLOGY –º–µ—Ç–æ–¥"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    ef = filters.lower().strip()
    parser.skip_relevance_filter = ("rel" not in ef) and (ef != "all")

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_morphology(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    result = apply_filters_traced(result, seed, country, method="morphology", language=language, enabled_filters=filters)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return apply_smart_fix(result, seed, language)

@app.get("/api/parse/adaptive-prefix")
async def parse_adaptive_prefix_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing"),
    filters: str = Query("all", description="–§–∏–ª—å—Ç—Ä—ã: all / none / pre,geo,bpf,rel")
):
    """ADAPTIVE PREFIX –º–µ—Ç–æ–¥ (–Ω–∞—Ö–æ–¥–∏—Ç PREFIX –∑–∞–ø—Ä–æ—Å—ã —Ç–∏–ø–∞ '–∫–∏–µ–≤ —Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤')"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    ef = filters.lower().strip()
    parser.skip_relevance_filter = ("rel" not in ef) and (ef != "all")

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_adaptive_prefix(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    result = apply_filters_traced(result, seed, country, method="adaptive-prefix", language=language, enabled_filters=filters)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return apply_smart_fix(result, seed, language)


# === Memory Audit Endpoint ===
@app.get("/debug/memory-audit")
def memory_audit():
    import sys, psutil, os
    process = psutil.Process(os.getpid())
    mem = process.memory_info()
    
    # –í—Å–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏ –∏ –∏—Ö —Ä–∞–∑–º–µ—Ä—ã
    big_modules = {}
    for name, mod in sorted(sys.modules.items()):
        if hasattr(mod, '__file__') and mod.__file__:
            try:
                size = os.path.getsize(mod.__file__)
                top = name.split('.')[0]
                big_modules[top] = big_modules.get(top, 0) + size
            except:
                pass
    
    top_modules = sorted(big_modules.items(), key=lambda x: -x[1])[:20]
    
    return {
        "ram_mb": round(mem.rss / 1024 / 1024, 1),
        "ram_percent": round(process.memory_percent(), 1),
        "system_total_mb": round(psutil.virtual_memory().total / 1024 / 1024, 1),
        "system_available_mb": round(psutil.virtual_memory().available / 1024 / 1024, 1),
        "loaded_modules_top20": {name: f"{size/1024/1024:.1f}MB" for name, size in top_modules},
        "torch_loaded": "torch" in sys.modules,
        "transformers_loaded": "transformers" in sys.modules,
        "fastembed_loaded": "fastembed" in sys.modules,
        "natasha_loaded": "natasha" in sys.modules,
        "google_ads_loaded": "google.ads" in sys.modules,
    }
