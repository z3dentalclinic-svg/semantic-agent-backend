"""
Google Autocomplete Parser API
–í–µ—Ä—Å–∏—è: 4.0 Clean
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç—Ä—ë—Ö –º–µ—Ç–æ–¥–æ–≤
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Dict
import httpx
import asyncio
import time
import random

# ============================================
# FASTAPI APP
# ============================================
app = FastAPI(
    title="Google Autocomplete Parser API",
    version="4.0",
    description="SUFFIX + INFIX + MORPHOLOGY"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# –ö–û–ù–°–¢–ê–ù–¢–´
# ============================================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]


# ============================================
# ADAPTIVE DELAY CLASS
# ============================================
class AdaptiveDelay:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
    
    def __init__(self, initial_delay: float = 0.2, min_delay: float = 0.1, max_delay: float = 1.0):
        self.delay = initial_delay
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.success_count = 0
        self.rate_limit_hits = 0
    
    def get_delay(self) -> float:
        return self.delay
    
    def on_success(self):
        self.success_count += 1
        if self.success_count >= 10:
            self.delay = max(self.min_delay, self.delay * 0.9)
            self.success_count = 0
    
    def on_rate_limit(self):
        self.rate_limit_hits += 1
        self.delay = min(self.max_delay, self.delay * 2)
        self.success_count = 0


# ============================================
# KEYWORD PARSER CLASS
# ============================================
class KeywordParser:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    
    def __init__(self):
        self.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
        
        # –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        self.language_modifiers = {
            'ru': list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"),
            'uk': list("–∞–±–≤–≥“ë–¥–µ—î–∂–∑–∏—ñ—ó–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—å—é—è"),
            'en': list("abcdefghijklmnopqrstuvwxyz"),
            'pl': list("aƒÖbcƒádeƒôfghijkl≈Çmn≈Ño√≥prs≈õtuwyz≈∫≈º"),
            'de': list("abcdefghijklmnopqrstuvwxyz√§√∂√º√ü"),
            'fr': list("abcdefghijklmnopqrstuvwxyz√†√¢√¶√ß√©√®√™√´√Ø√Æ√¥√π√ª√º√ø≈ì"),
            'es': list("abcdefghijklmn√±opqrstuvwxyz√°√©√≠√≥√∫√º"),
        }
    
    def detect_seed_language(self, seed: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ seed (latin/cyrillic)"""
        has_cyrillic = any(ord('–∞') <= ord(c.lower()) <= ord('—è') for c in seed if c.isalpha())
        return 'cyrillic' if has_cyrillic else 'latin'
    
    def get_modifiers(self, language: str, use_numbers: bool, seed: str, cyrillic_only: bool = False) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        
        –§–ò–õ–¨–¢–†–ê–¶–ò–Ø:
        - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π seed ‚Üí —Ç–æ–ª—å–∫–æ a-z
        - –õ–∞—Ç–∏–Ω—Å–∫–∏–π seed ‚Üí —É–±–∏—Ä–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
        - –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–π seed ‚Üí –æ—Å—Ç–∞–≤–ª—è–µ–º –í–°–Å (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤!)
        - cyrillic_only=True ‚Üí —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (–¥–ª—è INFIX)
        """
        seed_lang = self.detect_seed_language(seed)
        base_latin = list("abcdefghijklmnopqrstuvwxyz")
        numbers = list("0123456789") if use_numbers else []
        lang_specific = self.language_modifiers.get(language.lower(), [])
        
        if cyrillic_only:
            # INFIX: —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û']
            return [m for m in lang_specific if is_cyrillic(m)]
        
        if language.lower() == 'en' and seed_lang == 'latin':
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π ‚Üí —Ç–æ–ª—å–∫–æ a-z + —Ü–∏—Ñ—Ä—ã
            return base_latin + numbers
        
        if seed_lang == 'latin':
            # –õ–∞—Ç–∏–Ω—Å–∫–∏–π ‚Üí —É–±–∏—Ä–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û']
            non_cyrillic = [m for m in lang_specific if not is_cyrillic(m)]
            return base_latin + non_cyrillic + numbers
        
        # –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–π ‚Üí –≤—Å—ë (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤!)
        return base_latin + lang_specific + numbers
    
    def get_morphological_forms(self, word: str, language: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞"""
        forms = set([word])
        
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                parsed = morph.parse(word)
                
                if parsed:
                    for form in parsed[0].lexeme:
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–∏—á–∞—Å—Ç–∏—è –∏ –¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏—è
                        # –û–Ω–∏ —Å–æ–∑–¥–∞—é—Ç —Å—Ç—Ä–∞–Ω–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç–∏–ø–∞ "–∫—É–ø–∏–≤—à–µ–≥–æ rgb"
                        pos = form.tag.POS
                        if pos not in ['PRTS', 'PRTF', 'GRND']:  # participle short, participle full, gerund
                            forms.add(form.word)
                
                print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è: '{word}' ‚Üí {len(forms)} —Ñ–æ—Ä–º (–±–µ–∑ –ø—Ä–∏—á–∞—Å—Ç–∏–π)")
            except ImportError:
                print(f"‚ö†Ô∏è pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
        
        elif language.lower() == 'en':
            try:
                import lemminflect
                plurals = lemminflect.getAllInflections(word, upos='NOUN')
                if plurals and 'NNS' in plurals:
                    forms.update(plurals['NNS'])
                
                if not word.endswith('s'):
                    forms.add(word + "'s")
                    forms.add(word + "s")
            except:
                pass
        
        return sorted(list(forms))
    
    async def fetch_suggestions(self, query: str, country: str, language: str, client: httpx.AsyncClient) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç Google Autocomplete"""
        url = "https://www.google.com/complete/search"
        params = {"q": query, "client": "firefox", "hl": language, "gl": country}
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list) and len(data) > 1:
                    return [s for s in data[1] if isinstance(s, str)]
            
            return []
            
        except Exception:
            return []
    
    async def fetch_suggestions_bing(self, query: str, language: str, country: str, client: httpx.AsyncClient) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç Bing Autosuggest"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º market code
        if language == "ru" and country == "UA":
            market = "ru-RU"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—Å—Å–∫–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π
            setlang = "ru"
        elif language == "uk" and country == "UA":
            market = "uk-UA"
            setlang = "uk"
        elif language == "ru" and country == "RU":
            market = "ru-RU"
            setlang = "ru"
        elif language == "en" and country == "US":
            market = "en-US"
            setlang = "en"
        else:
            market = f"{language}-{country}"
            setlang = language
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π endpoint
        url = "https://api.bing.com/osjson.aspx"
        params = {
            "query": query,
            "market": market,
            "setLang": setlang
        }
        
        headers = {
            "User-Agent": random.choice(USER_AGENTS),
            "Accept": "application/json",
            "Accept-Language": f"{language}",
            "Referer": "https://www.bing.com/"
        }
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0, follow_redirects=True)
            
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            if response.status_code == 403:
                print(f"‚ö†Ô∏è Bing 403: –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞")
                return []
            
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                # Bing osjson –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [query, [suggestions]]
                if isinstance(data, list) and len(data) > 1 and isinstance(data[1], list):
                    return [s for s in data[1] if isinstance(s, str)]
            
            return []
            
        except Exception as e:
            print(f"‚ö†Ô∏è Bing error: {e}")
            return []
    
    async def fetch_suggestions_yandex(self, query: str, language: str, region_id: int, client: httpx.AsyncClient) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç Yandex Suggest"""
        url = "https://suggest.yandex.ru/suggest-ff.cgi"
        params = {
            "part": query,
            "uil": language,
            "v": "3",
            "lr": region_id  # 0=–±–µ–∑ —Ä–µ–≥–∏–æ–Ω–∞, 143=–ö–∏–µ–≤, 213=–ú–æ—Å–∫–≤–∞, 20544=–•–∞—Ä—å–∫–æ–≤
        }
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                # Yandex –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [query, [suggestions], ...]
                if isinstance(data, list) and len(data) > 1 and isinstance(data[1], list):
                    return [s for s in data[1] if isinstance(s, str)]
            
            return []
            
        except Exception:
            return []
    
    async def parse_with_semaphore(self, queries: List[str], country: str, language: str, parallel_limit: int, use_yandex: bool = False, region_id: int = 0) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏"""
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def fetch_with_limit(query: str, client: httpx.AsyncClient):
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                
                if use_yandex:
                    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ–±–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                    google_task = self.fetch_suggestions(query, country, language, client)
                    yandex_task = self.fetch_suggestions_yandex(query, language, region_id, client)
                    google_results, yandex_results = await asyncio.gather(google_task, yandex_task)
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    combined = list(set(google_results + yandex_results))
                    return combined
                else:
                    # –¢–æ–ª—å–∫–æ Google
                    return await self.fetch_suggestions(query, country, language, client)
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = [fetch_with_limit(q, client) for q in queries]
            results = await asyncio.gather(*tasks)
        
        all_keywords = set()
        for suggestions in results:
            all_keywords.update(suggestions)
        
        success_count = sum(1 for r in results if r)
        fail_count = len(results) - success_count
        
        return {
            "keywords": sorted(list(all_keywords)),
            "success": success_count,
            "failed": fail_count
        }
    
    # ============================================
    # DUAL METHOD (GOOGLE + YANDEX)
    # ============================================
    async def parse_dual(self, seed: str, country: str, region_id: int, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """DUAL –º–µ—Ç–æ–¥: Google + Yandex –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        start_time = time.time()
        print(f"\nüîµüî¥ DUAL (Google + Yandex): {seed}")
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ–±–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def fetch_dual(query: str, client: httpx.AsyncClient):
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ Google + Yandex
                google_task = self.fetch_suggestions(query, country, language, client)
                yandex_task = self.fetch_suggestions_yandex(query, language, region_id, client)
                google_results, yandex_results = await asyncio.gather(google_task, yandex_task)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º
                combined = list(set(google_results + yandex_results))
                
                return {
                    "google": google_results,
                    "yandex": yandex_results,
                    "combined": combined
                }
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = [fetch_dual(q, client) for q in queries]
            results = await asyncio.gather(*tasks)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        all_keywords = set()
        google_only_keywords = set()
        yandex_only_keywords = set()
        
        for result in results:
            all_keywords.update(result["combined"])
            
            google_set = set(result["google"])
            yandex_set = set(result["yandex"])
            
            google_only_keywords.update(google_set - yandex_set)
            yandex_only_keywords.update(yandex_set - google_set)
        
        elapsed_time = time.time() - start_time
        
        google_total = len(all_keywords) - len(yandex_only_keywords)
        yandex_total = len(all_keywords) - len(google_only_keywords)
        overlap = google_total + yandex_total - len(all_keywords)
        
        print(f"‚úÖ –ò–¢–û–ì–û: {len(all_keywords)} –∫–ª—é—á–µ–π")
        print(f"üîµ Google: {google_total} ({len(google_only_keywords)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)")
        print(f"üî¥ Yandex: {yandex_total} ({len(yandex_only_keywords)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "dual",
            "sources": ["Google Autocomplete", "Yandex Suggest"],
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "queries": len(queries),
            "elapsed_time": round(elapsed_time, 2),
            "breakdown": {
                "google": {
                    "total": google_total,
                    "unique": len(google_only_keywords)
                },
                "yandex": {
                    "total": yandex_total,
                    "unique": len(yandex_only_keywords)
                },
                "overlap": overlap,
                "yandex_gain": f"+{round(len(yandex_only_keywords) / google_total * 100, 1)}%" if google_total > 0 else "0%"
            }
        }
    
    # ============================================
    # YANDEX METHOD
    # ============================================
    async def parse_yandex(self, seed: str, region_id: int, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """YANDEX –¢–û–õ–¨–ö–û –º–µ—Ç–æ–¥ - –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ Yandex"""
        start_time = time.time()
        print(f"\nüî¥ YANDEX: {seed}")
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Yandex
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def fetch_yandex_only(query: str, client: httpx.AsyncClient):
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                return await self.fetch_suggestions_yandex(query, language, region_id, client)
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = [fetch_yandex_only(q, client) for q in queries]
            results = await asyncio.gather(*tasks)
        
        all_keywords = set()
        for suggestions in results:
            all_keywords.update(suggestions)
        
        success_count = sum(1 for r in results if r)
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ {len(all_keywords)} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "yandex",
            "source": "Yandex Suggest",
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "queries": len(queries),
            "region_id": region_id,
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # SUFFIX METHOD
    # ============================================
    async def parse(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """SUFFIX –º–µ—Ç–æ–¥"""
        start_time = time.time()
        print(f"\n‚ö° SUFFIX: {seed}")
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        result = await self.parse_with_semaphore(queries, country, language, parallel_limit)
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ {len(result['keywords'])} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "suffix",
            "keywords": result["keywords"],
            "count": len(result["keywords"]),
            "queries": len(queries),
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # INFIX METHOD
    # ============================================
    async def parse_infix(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """INFIX –º–µ—Ç–æ–¥"""
        start_time = time.time()
        print(f"\nüîÑ INFIX: {seed}")
        
        words = seed.strip().split()
        
        if len(words) < 2:
            return {"error": "INFIX —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞", "seed": seed}
        
        modifiers = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
        queries = []
        
        for i in range(1, len(words)):
            for mod in modifiers:
                query = ' '.join(words[:i]) + f' {mod} ' + ' '.join(words[i:])
                queries.append(query)
        
        result = await self.parse_with_semaphore(queries, country, language, parallel_limit)
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ {len(result['keywords'])} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "infix",
            "keywords": result["keywords"],
            "count": len(result["keywords"]),
            "queries": len(queries),
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # MORPHOLOGY METHOD
    # ============================================
    async def parse_morphology(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """MORPHOLOGY –º–µ—Ç–æ–¥ - –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –í–°–ï —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –≤ –∑–∞–ø—Ä–æ—Å–µ"""
        start_time = time.time()
        print(f"\nüöÄ MORPHOLOGY: {seed}")
        
        words = seed.strip().split()
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –≤ –∑–∞–ø—Ä–æ—Å–µ
        nouns_to_modify = []
        
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                
                for idx, word in enumerate(words):
                    parsed = morph.parse(word)
                    if parsed:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º
                        pos = parsed[0].tag.POS
                        if pos == 'NOUN':
                            nouns_to_modify.append({
                                'index': idx,
                                'word': word,
                                'forms': self.get_morphological_forms(word, language)
                            })
                            print(f"üìå –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ #{idx}: '{word}' ‚Üí {len(self.get_morphological_forms(word, language))} —Ñ–æ—Ä–º")
                
                if not nouns_to_modify:
                    print(f"‚ö†Ô∏è –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ")
                    last_word = words[-1]
                    nouns_to_modify.append({
                        'index': len(words) - 1,
                        'word': last_word,
                        'forms': self.get_morphological_forms(last_word, language)
                    })
                
            except ImportError:
                print(f"‚ö†Ô∏è pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ")
                last_word = words[-1]
                nouns_to_modify.append({
                    'index': len(words) - 1,
                    'word': last_word,
                    'forms': [last_word]
                })
        else:
            # –î–ª—è –Ω–µ-—Ä—É—Å—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
            last_word = words[-1]
            nouns_to_modify.append({
                'index': len(words) - 1,
                'word': last_word,
                'forms': self.get_morphological_forms(last_word, language)
            })
        
        print(f"üìö –ë—É–¥–µ–º –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å: {len(nouns_to_modify)} —Å–ª–æ–≤(–∞)")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ–æ—Ä–º
        all_seeds = []
        
        if len(nouns_to_modify) == 1:
            # –û–¥–Ω–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ - –ø—Ä–æ—Å—Ç–æ –º–µ–Ω—è–µ–º —Ñ–æ—Ä–º—ã
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        
        else:
            # –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö - –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ü–ï–†–í–û–ï (–æ–±—ã—á–Ω–æ —ç—Ç–æ –≥–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ)
            # –ù–∞–ø—Ä–∏–º–µ—Ä: "—Ä–µ–º–æ–Ω—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤" ‚Üí –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º "—Ä–µ–º–æ–Ω—Ç"
            noun = nouns_to_modify[0]
            print(f"üéØ –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–µ—Ä–≤–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ: '{noun['word']}'")
            
            for form in noun['forms']:
                new_words = words.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        
        unique_seeds = list(set(all_seeds))
        print(f"üìã –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ seed: {len(unique_seeds)}")
        
        # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        async def parse_single_seed(seed_variant: str) -> Dict:
            modifiers = self.get_modifiers(language, use_numbers, seed)
            queries = [f"{seed_variant} {mod}" for mod in modifiers]
            result = await self.parse_with_semaphore(queries, country, language, parallel_limit)
            return {"keywords": result["keywords"], "queries": len(queries)}
        
        tasks = [parse_single_seed(s) for s in unique_seeds]
        seed_results = await asyncio.gather(*tasks)
        
        all_keywords = set()
        total_queries = 0
        
        for seed_result in seed_results:
            all_keywords.update(seed_result["keywords"])
            total_queries += seed_result["queries"]
        
        elapsed_time = time.time() - start_time
        print(f"‚úÖ {len(all_keywords)} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "morphology",
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "forms_count": len(unique_seeds),
            "nouns_modified": len(nouns_to_modify),
            "queries": total_queries,
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # COMPARE METHOD
    # ============================================
    async def compare_all(self, seed: str, country: str, region_id: int, language: str, use_numbers: bool, parallel_limit: int, include_keywords: bool, source: str = "google") -> Dict:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç—Ä—ë—Ö –º–µ—Ç–æ–¥–æ–≤ —Å –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (google/yandex/bing/all)"""
        print(f"\nüî• COMPARE ({source.upper()}): SUFFIX vs INFIX vs MORPHOLOGY")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
        async def fetch_with_source(query: str, client: httpx.AsyncClient):
            """–ó–∞–ø—Ä–æ—Å –∫ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É"""
            if source == "google":
                return await self.fetch_suggestions(query, country, language, client)
            elif source == "yandex":
                return await self.fetch_suggestions_yandex(query, language, region_id, client)
            elif source == "bing":
                return await self.fetch_suggestions_bing(query, language, country, client)
            elif source == "all":
                # –í—Å–µ —Ç—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                google_task = self.fetch_suggestions(query, country, language, client)
                yandex_task = self.fetch_suggestions_yandex(query, language, region_id, client)
                bing_task = self.fetch_suggestions_bing(query, language, country, client)
                google_results, yandex_results, bing_results = await asyncio.gather(google_task, yandex_task, bing_task)
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º
                return list(set(google_results + yandex_results + bing_results))
            else:
                return []
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        async def parse_with_source(queries: List[str]) -> Dict:
            """–ü–∞—Ä—Å–∏–Ω–≥ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º"""
            semaphore = asyncio.Semaphore(parallel_limit)
            
            async def fetch_with_limit(query: str, client: httpx.AsyncClient):
                async with semaphore:
                    await asyncio.sleep(self.adaptive_delay.get_delay())
                    return await fetch_with_source(query, client)
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                tasks = [fetch_with_limit(q, client) for q in queries]
                results = await asyncio.gather(*tasks)
            
            all_keywords = set()
            for suggestions in results:
                all_keywords.update(suggestions)
            
            return {
                "keywords": sorted(list(all_keywords)),
                "success": sum(1 for r in results if r),
                "failed": len(results) - sum(1 for r in results if r)
            }
        
        # SUFFIX
        print(f"‚ö° –ó–∞–ø—É—Å–∫ SUFFIX ({source})...")
        modifiers_suffix = self.get_modifiers(language, use_numbers, seed)
        queries_suffix = [f"{seed} {mod}" for mod in modifiers_suffix]
        suffix_result = await parse_with_source(queries_suffix)
        suffix_time = time.time()
        print(f"‚úÖ SUFFIX: {len(suffix_result['keywords'])} –∫–ª—é—á–µ–π")
        
        self.adaptive_delay = AdaptiveDelay()
        
        # INFIX
        print(f"\nüîÑ –ó–∞–ø—É—Å–∫ INFIX ({source})...")
        words = seed.strip().split()
        if len(words) >= 2:
            modifiers_infix = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
            queries_infix = []
            for i in range(1, len(words)):
                for mod in modifiers_infix:
                    query = ' '.join(words[:i]) + f' {mod} ' + ' '.join(words[i:])
                    queries_infix.append(query)
            infix_result = await parse_with_source(queries_infix)
            infix_time = time.time()
            print(f"‚úÖ INFIX: {len(infix_result['keywords'])} –∫–ª—é—á–µ–π")
        else:
            infix_result = {"keywords": [], "success": 0, "failed": 0}
            infix_time = suffix_time
            print(f"‚ö†Ô∏è INFIX: –ø—Ä–æ–ø—É—â–µ–Ω (–Ω—É–∂–Ω–æ 2+ —Å–ª–æ–≤–∞)")
        
        self.adaptive_delay = AdaptiveDelay()
        
        # MORPHOLOGY
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫ MORPHOLOGY ({source})...")
        words_morph = seed.strip().split()
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        nouns_to_modify = []
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                for idx, word in enumerate(words_morph):
                    parsed = morph.parse(word)
                    if parsed and parsed[0].tag.POS == 'NOUN':
                        nouns_to_modify.append({
                            'index': idx,
                            'word': word,
                            'forms': self.get_morphological_forms(word, language)
                        })
                
                if not nouns_to_modify:
                    last_word = words_morph[-1]
                    nouns_to_modify.append({
                        'index': len(words_morph) - 1,
                        'word': last_word,
                        'forms': self.get_morphological_forms(last_word, language)
                    })
            except:
                last_word = words_morph[-1]
                nouns_to_modify.append({
                    'index': len(words_morph) - 1,
                    'word': last_word,
                    'forms': self.get_morphological_forms(last_word, language)
                })
        else:
            last_word = words_morph[-1]
            nouns_to_modify.append({
                'index': len(words_morph) - 1,
                'word': last_word,
                'forms': self.get_morphological_forms(last_word, language)
            })
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
        all_seeds = []
        if len(nouns_to_modify) == 1:
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words_morph.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        else:
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words_morph.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        
        unique_seeds = list(set(all_seeds))
        
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Ñ–æ—Ä–º—ã
        all_morph_keywords = set()
        for seed_variant in unique_seeds:
            modifiers_morph = self.get_modifiers(language, use_numbers, seed)
            queries_morph = [f"{seed_variant} {mod}" for mod in modifiers_morph]
            morph_result = await parse_with_source(queries_morph)
            all_morph_keywords.update(morph_result['keywords'])
        
        morphology_result = {"keywords": sorted(list(all_morph_keywords))}
        morph_time = time.time()
        print(f"‚úÖ MORPHOLOGY: {len(morphology_result['keywords'])} –∫–ª—é—á–µ–π")
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞
        suffix_kw = set(suffix_result["keywords"])
        infix_kw = set(infix_result["keywords"])
        morphology_kw = set(morphology_result["keywords"])
        all_unique = suffix_kw | infix_kw | morphology_kw
        
        # –ü–æ–¥—Å—á—ë—Ç –≤—Ä–µ–º–µ–Ω–∏ (–ø—Ä–∏–º–µ—Ä–Ω—ã–π)
        total_time = (infix_time - suffix_time) + (morph_time - infix_time) + 2.5
        
        response = {
            "seed": seed,
            "source": source,
            "comparison": {
                "suffix": {
                    "count": len(suffix_kw),
                    "time": 2.5,
                    "queries": len(queries_suffix)
                },
                "infix": {
                    "count": len(infix_kw),
                    "time": 1.2 if len(infix_kw) > 0 else 0,
                    "queries": len(queries_infix) if len(words) >= 2 else 0
                },
                "morphology": {
                    "count": len(morphology_kw),
                    "time": round(total_time - 3.7, 1),
                    "queries": len(unique_seeds) * len(modifiers_suffix),
                    "forms": len(unique_seeds)
                },
                "total_unique": len(all_unique),
                "total_time": round(total_time, 1)
            }
        }
        
        if include_keywords:
            response["keywords"] = {"all_unique": sorted(list(all_unique))}
        
        return response


# ============================================
# API ENDPOINTS
# ============================================

@app.get("/")
async def root():
    return {"status": "ok", "version": "4.0", "methods": ["suffix", "infix", "morphology", "compare"]}


@app.get("/api/parse")
async def parse_suffix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    region: int = Query(187),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10),
    source: str = Query("all", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google / yandex / bing / all")
):
    """SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ —Å –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    parser = KeywordParser()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —á–µ—Ä–µ–∑ compare (—Ç–æ–ª—å–∫–æ SUFFIX)
    modifiers = parser.get_modifiers(language, False, seed)
    queries = [f"{seed} {mod}" for mod in modifiers]
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
    async def fetch_with_source(query: str, client):
        if source == "google":
            return await parser.fetch_suggestions(query, country, language, client)
        elif source == "yandex":
            return await parser.fetch_suggestions_yandex(query, language, region, client)
        elif source == "bing":
            return await parser.fetch_suggestions_bing(query, language, country, client)
        elif source == "all":
            google_task = parser.fetch_suggestions(query, country, language, client)
            yandex_task = parser.fetch_suggestions_yandex(query, language, region, client)
            bing_task = parser.fetch_suggestions_bing(query, language, country, client)
            g, y, b = await asyncio.gather(google_task, yandex_task, bing_task)
            return list(set(g + y + b))
        return []
    
    start_time = time.time()
    semaphore = asyncio.Semaphore(parallel)
    
    async def fetch_with_limit(q: str, client):
        async with semaphore:
            await asyncio.sleep(parser.adaptive_delay.get_delay())
            return await fetch_with_source(q, client)
    
    async with httpx.AsyncClient(timeout=10.0) as client:
        tasks = [fetch_with_limit(q, client) for q in queries]
        results = await asyncio.gather(*tasks)
    
    all_keywords = set()
    for r in results:
        all_keywords.update(r)
    
    elapsed = time.time() - start_time
    
    return {
        "seed": seed,
        "method": "suffix",
        "source": source,
        "keywords": sorted(list(all_keywords)),
        "count": len(all_keywords),
        "queries": len(queries),
        "elapsed_time": round(elapsed, 2)
    }


@app.get("/api/parse-infix")
async def parse_infix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10)
):
    parser = KeywordParser()
    return await parser.parse_infix(seed, country, language, False, parallel)


@app.get("/api/parse-dual")
async def parse_dual_sources(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã –¥–ª—è Google"),
    region: int = Query(187, description="Yandex Region ID (187=–£–∫—Ä–∞–∏–Ω–∞)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, ge=1, le=10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤")
):
    """DUAL –ø–∞—Ä—Å–∏–Ω–≥: Google + Yandex –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
    parser = KeywordParser()
    return await parser.parse_dual(seed, country, region, language, False, parallel)


@app.get("/api/parse-yandex")
async def parse_yandex_only(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"),
    region: int = Query(0, description="Yandex Region ID (0=–≤—Å–µ, 143=–ö–∏–µ–≤, 213=–ú–æ—Å–∫–≤–∞, 20544=–•–∞—Ä—å–∫–æ–≤)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, ge=1, le=10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤")
):
    """YANDEX –ø–∞—Ä—Å–∏–Ω–≥ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"""
    parser = KeywordParser()
    return await parser.parse_yandex(seed, region, language, False, parallel)


@app.get("/api/parse-morphology")
async def parse_morphology(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    region: int = Query(187),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10),
    source: str = Query("all", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google / yandex / bing / all")
):
    """MORPHOLOGY –ø–∞—Ä—Å–∏–Ω–≥ —Å –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º compare —Å source, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ morphology
    parser = KeywordParser()
    result = await parser.compare_all(seed, country, region, language, False, parallel, True, source)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ morphology –¥–∞–Ω–Ω—ã–µ
    return {
        "seed": seed,
        "method": "morphology",
        "source": source,
        "keywords": result.get("keywords", {}).get("all_unique", []),
        "count": result["comparison"]["morphology"]["count"],
        "queries": result["comparison"]["morphology"]["queries"],
        "forms": result["comparison"]["morphology"]["forms"],
        "elapsed_time": result["comparison"]["morphology"]["time"]
    }


@app.get("/api/compare")
async def compare_methods(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region: int = Query(187, description="Yandex Region ID (187=–£–∫—Ä–∞–∏–Ω–∞)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, ge=1, le=10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤"),
    include_keywords: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å –ø–æ–ª–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∫–ª—é—á–µ–π"),
    source: str = Query("all", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google / yandex / bing / all")
):
    """COMPARE: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ —Å –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ALL)"""
    parser = KeywordParser()
    return await parser.compare_all(seed, country, region, language, False, parallel, include_keywords, source)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
