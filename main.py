"""
MORPHOLOGICAL ADAPTIVE TEST - –ö–æ–Ω—Å–µ–Ω—Å—É—Å AI
–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ ADAPTIVE –º–µ—Ç–æ–¥–∞
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List
from collections import Counter
import os
import httpx
import asyncio
import time
import random

app = FastAPI(title="Morphological ADAPTIVE Test", version="1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AutocompleteParser:
    def __init__(self):
        self.base_url = "https://suggestqueries.google.com/complete/search"
    
    async def fetch_suggestions(self, query: str, country: str, language: str) -> List[str]:
        params = {"client": "chrome", "q": query, "gl": country, "hl": language}
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(self.base_url, params=params)
                if response.status_code == 200:
                    data = response.json()
                    if isinstance(data, list) and len(data) > 1:
                        return [s for s in data[1] if isinstance(s, str)]
                return []
        except Exception as e:
            print(f"Error: {e}")
            return []
    
    async def morphological_adaptive_test(self, seed: str, country: str, language: str) -> List[str]:
        all_keywords = set()
        seed_words = set(seed.lower().split())
        
        print(f"\n{'='*60}")
        print(f"üî¨ MORPHOLOGICAL ADAPTIVE - –ö–æ–Ω—Å–µ–Ω—Å—É—Å AI")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"–ú–µ—Ç–æ–¥: ADAPTIVE + –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã\n")
        
        # ========================================
        # –≠–¢–ê–ü 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º")
        print(f"{'='*60}\n")
        
        # –î–ª—è "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤" —Å–æ–∑–¥–∞—ë–º —Ñ–æ—Ä–º—ã –≤—Ä—É—á–Ω—É—é
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pymorphy3
        forms = [
            seed,                           # "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤" (–∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π)
            "—Ä–µ–º–æ–Ω—Ç–∞ –ø—ã–ª–µ—Å–æ—Å–æ–≤",           # —Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂
            "–ø–æ —Ä–µ–º–æ–Ω—Ç—É –ø—ã–ª–µ—Å–æ—Å–æ–≤"         # –ø—Ä–µ–¥–ª–æ–≥ + –¥–∞—Ç–µ–ª—å–Ω—ã–π
        ]
        
        print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ñ–æ—Ä–º: {len(forms)}")
        for i, form in enumerate(forms, 1):
            print(f"  {i}. '{form}'")
        print()
        
        # ========================================
        # –≠–¢–ê–ü 2: SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—ã
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 2: SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—ã")
        print(f"{'='*60}\n")
        
        alphabet = "–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"
        all_suffix_results = []
        suffix_count = 0
        
        for form_idx, form in enumerate(forms, 1):
            print(f"--- –§–æ—Ä–º–∞ {form_idx}: '{form}' ---")
            form_results = []
            
            for letter in alphabet:
                query = f"{form} {letter}"
                results = await self.fetch_suggestions(query, country, language)
                form_results.extend(results)
                all_suffix_results.extend(results)
                suffix_count += 1
                await asyncio.sleep(random.uniform(0.3, 0.6))
            
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Ñ–æ—Ä–º—ã {form_idx}: {len(form_results)}")
            if form_results:
                print(f"–ü—Ä–∏–º–µ—Ä—ã:")
                for r in form_results[:3]:
                    print(f"  ‚Ä¢ {r}")
            print()
        
        print(f"–í—Å–µ–≥–æ SUFFIX –∑–∞–ø—Ä–æ—Å–æ–≤: {suffix_count}")
        print(f"–í—Å–µ–≥–æ SUFFIX —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_suffix_results)}\n")
        
        # ========================================
        # –≠–¢–ê–ü 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        print(f"{'='*60}\n")
        
        word_counter = Counter()
        
        for result in all_suffix_results:
            words = result.lower().split()
            for word in words:
                # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –∏–∑ seed
                if word not in seed_words and len(word) > 2:
                    word_counter[word] += 1
        
        # –ß–∞—Å—Ç–æ—Ç–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (—Å–ª–æ–≤–∞ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è 2+ —Ä–∞–∑–∞)
        all_candidates = {w for w, count in word_counter.items() if count >= 2}
        
        print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(word_counter)}")
        print(f"–°–ª–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (‚â•2 —Ä–∞–∑): {len(all_candidates)}")
        print(f"\n–¢–æ–ø-20 —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤:")
        for word, count in word_counter.most_common(20):
            print(f"  ‚Ä¢ '{word}' ({count} —Ä–∞–∑)")
        print()
        
        # ========================================
        # –≠–¢–ê–ü 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤—ã–º ADAPTIVE
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 4: –ê–Ω–∞–ª–∏–∑ –ù–û–í–´–• —Å–ª–æ–≤ –æ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏")
        print(f"{'='*60}\n")
        
        # –û—Ç–¥–µ–ª—å–Ω–æ —Å—á–∏—Ç–∞–µ–º —Å–ª–æ–≤–∞ –æ—Ç –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º—ã
        base_form_words = set()
        for result in all_suffix_results[:len(all_suffix_results)//3]:  # –ü–µ—Ä–≤–∞—è —Ç—Ä–µ—Ç—å = –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞
            words = result.lower().split()
            for word in words:
                if word not in seed_words and len(word) > 2:
                    base_form_words.add(word)
        
        # –ù–æ–≤—ã–µ —Å–ª–æ–≤–∞ = –≤—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –º–∏–Ω—É—Å –±–∞–∑–æ–≤—ã–µ
        new_from_morphology = all_candidates - base_form_words
        
        print(f"–°–ª–æ–≤ –æ—Ç –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º—ã: {len(base_form_words)}")
        print(f"–°–ª–æ–≤ –æ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º: {len(all_candidates)}")
        print(f"–ù–û–í–´–• —Å–ª–æ–≤ –±–ª–∞–≥–æ–¥–∞—Ä—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏: {len(new_from_morphology)}")
        
        if new_from_morphology:
            print(f"\n–ù–æ–≤—ã–µ —Å–ª–æ–≤–∞:")
            for word in sorted(list(new_from_morphology)[:20]):
                print(f"  ‚Ä¢ {word}")
        print()
        
        # ========================================
        # –≠–¢–ê–ü 5: PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 5: PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞")
        print(f"{'='*60}\n")
        
        prefix_count = 0
        verified_count = 0
        
        for candidate in sorted(all_candidates):
            query = f"{candidate} {seed}"
            results = await self.fetch_suggestions(query, country, language)
            prefix_count += 1
            
            if results:
                all_keywords.update(results)
                verified_count += 1
                if verified_count <= 10:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                    print(f"‚úÖ '{query}' ‚Üí {len(results)} –∫–ª—é—á–µ–π")
            
            await asyncio.sleep(random.uniform(0.3, 0.6))
        
        print(f"\n–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {prefix_count}")
        print(f"–í–∞–ª–∏–¥–Ω—ã—Ö PREFIX: {verified_count}")
        print()
        
        # ========================================
        # –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        # ========================================
        total_queries = suffix_count + prefix_count
        
        print(f"{'='*60}")
        print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"SUFFIX –ø–∞—Ä—Å–∏–Ω–≥: {suffix_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"  - –ë–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞: 29 –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"  - –ú–æ—Ä—Ñ–æ —Ñ–æ—Ä–º—ã: {suffix_count - 29} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞: {prefix_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        print(f"–í–°–ï–ì–û –∑–∞–ø—Ä–æ—Å–æ–≤: {total_queries}")
        print(f"")
        print(f"–ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_candidates)}")
        print(f"–í–∞–ª–∏–¥–Ω—ã—Ö PREFIX: {verified_count}")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"")
        print(f"–≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨:")
        print(f"  –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {len(all_candidates)/suffix_count:.2f}")
        print(f"  –í–∞–ª–∏–¥–∞—Ü–∏—è: {verified_count}/{len(all_candidates)} = {verified_count/len(all_candidates)*100:.1f}%")
        print(f"  –ö–ª—é—á–µ–π –Ω–∞ –∑–∞–ø—Ä–æ—Å: {len(all_keywords)/total_queries:.2f}")
        print(f"")
        
        if len(new_from_morphology) > 0:
            print(f"‚úÖ –ú–û–†–§–û–õ–û–ì–ò–Ø –î–ê–õ–ê –†–ï–ó–£–õ–¨–¢–ê–¢!")
            print(f"–ù–æ–≤—ã—Ö —Å–ª–æ–≤: {len(new_from_morphology)} (+{len(new_from_morphology)/len(base_form_words)*100:.1f}%)")
        else:
            print(f"‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –Ω–µ –¥–∞–ª–∞ –Ω–æ–≤—ã—Ö —Å–ª–æ–≤")
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω –±–∞–∑–æ–≤–æ–º—É ADAPTIVE")
        
        print(f"{'='*60}\n")
        
        return list(all_keywords)


@app.get("/api/test-parser/morphology")
async def test_morphology(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru")
):
    parser = AutocompleteParser()
    start = time.time()
    keywords = await parser.morphological_adaptive_test(seed, country, language)
    return {
        "seed": seed,
        "method": "Morphological ADAPTIVE",
        "keywords": keywords,
        "count": len(keywords),
        "time": round(time.time() - start, 2)
    }


@app.get("/")
async def root():
    return {
        "api": "Morphological ADAPTIVE Test",
        "url": "/api/test-parser/morphology?seed=—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
