"""
Google Autocomplete Parser API
–í–µ—Ä—Å–∏—è: 4.0 Clean
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç—Ä—ë—Ö –º–µ—Ç–æ–¥–æ–≤
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Dict
import httpx
import asyncio
import time
import random

# ============================================
# FASTAPI APP
# ============================================
app = FastAPI(
    title="Google Autocomplete Parser API",
    version="4.0",
    description="SUFFIX + INFIX + MORPHOLOGY"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# –ö–û–ù–°–¢–ê–ù–¢–´
# ============================================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]


# ============================================
# ADAPTIVE DELAY CLASS
# ============================================
class AdaptiveDelay:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
    
    def __init__(self, initial_delay: float = 0.2, min_delay: float = 0.1, max_delay: float = 1.0):
        self.delay = initial_delay
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.success_count = 0
        self.rate_limit_hits = 0
    
    def get_delay(self) -> float:
        return self.delay
    
    def on_success(self):
        self.success_count += 1
        if self.success_count >= 10:
            self.delay = max(self.min_delay, self.delay * 0.9)
            self.success_count = 0
    
    def on_rate_limit(self):
        self.rate_limit_hits += 1
        self.delay = min(self.max_delay, self.delay * 2)
        self.success_count = 0


# ============================================
# KEYWORD PARSER CLASS
# ============================================
class KeywordParser:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    
    def __init__(self):
        self.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
        
        # –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        self.language_modifiers = {
            'ru': list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"),
            'uk': list("–∞–±–≤–≥“ë–¥–µ—î–∂–∑–∏—ñ—ó–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—å—é—è"),
            'en': list("abcdefghijklmnopqrstuvwxyz"),
            'pl': list("aƒÖbcƒádeƒôfghijkl≈Çmn≈Ño√≥prs≈õtuwyz≈∫≈º"),
            'de': list("abcdefghijklmnopqrstuvwxyz√§√∂√º√ü"),
            'fr': list("abcdefghijklmnopqrstuvwxyz√†√¢√¶√ß√©√®√™√´√Ø√Æ√¥√π√ª√º√ø≈ì"),
            'es': list("abcdefghijklmn√±opqrstuvwxyz√°√©√≠√≥√∫√º"),
        }
    
    def detect_seed_language(self, seed: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ seed (latin/cyrillic)"""
        has_cyrillic = any(ord('–∞') <= ord(c.lower()) <= ord('—è') for c in seed if c.isalpha())
        return 'cyrillic' if has_cyrillic else 'latin'
    
    def get_modifiers(self, language: str, use_numbers: bool, seed: str, cyrillic_only: bool = False) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        
        –§–ò–õ–¨–¢–†–ê–¶–ò–Ø:
        - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π seed ‚Üí —Ç–æ–ª—å–∫–æ a-z
        - –õ–∞—Ç–∏–Ω—Å–∫–∏–π seed ‚Üí —É–±–∏—Ä–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
        - –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–π seed ‚Üí –æ—Å—Ç–∞–≤–ª—è–µ–º –í–°–Å (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤!)
        - cyrillic_only=True ‚Üí —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (–¥–ª—è INFIX)
        """
        seed_lang = self.detect_seed_language(seed)
        base_latin = list("abcdefghijklmnopqrstuvwxyz")
        numbers = list("0123456789") if use_numbers else []
        lang_specific = self.language_modifiers.get(language.lower(), [])
        
        if cyrillic_only:
            # INFIX: —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û']
            return [m for m in lang_specific if is_cyrillic(m)]
        
        if language.lower() == 'en' and seed_lang == 'latin':
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π ‚Üí —Ç–æ–ª—å–∫–æ a-z + —Ü–∏—Ñ—Ä—ã
            return base_latin + numbers
        
        if seed_lang == 'latin':
            # –õ–∞—Ç–∏–Ω—Å–∫–∏–π ‚Üí —É–±–∏—Ä–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û']
            non_cyrillic = [m for m in lang_specific if not is_cyrillic(m)]
            return base_latin + non_cyrillic + numbers
        
        # –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–π ‚Üí –≤—Å—ë (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤!)
        return base_latin + lang_specific + numbers
    
    def get_morphological_forms(self, word: str, language: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞"""
        forms = set([word])
        
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                parsed = morph.parse(word)
                
                if parsed:
                    for form in parsed[0].lexeme:
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–∏—á–∞—Å—Ç–∏—è –∏ –¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏—è
                        # –û–Ω–∏ —Å–æ–∑–¥–∞—é—Ç —Å—Ç—Ä–∞–Ω–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç–∏–ø–∞ "–∫—É–ø–∏–≤—à–µ–≥–æ rgb"
                        pos = form.tag.POS
                        if pos not in ['PRTS', 'PRTF', 'GRND']:  # participle short, participle full, gerund
                            forms.add(form.word)
                
                print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è: '{word}' ‚Üí {len(forms)} —Ñ–æ—Ä–º (–±–µ–∑ –ø—Ä–∏—á–∞—Å—Ç–∏–π)")
            except ImportError:
                print(f"‚ö†Ô∏è pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
        
        elif language.lower() == 'en':
            try:
                import lemminflect
                plurals = lemminflect.getAllInflections(word, upos='NOUN')
                if plurals and 'NNS' in plurals:
                    forms.update(plurals['NNS'])
                
                if not word.endswith('s'):
                    forms.add(word + "'s")
                    forms.add(word + "s")
            except:
                pass
        
        return sorted(list(forms))
    
    async def fetch_suggestions(self, query: str, country: str, language: str, client: httpx.AsyncClient) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç Google Autocomplete"""
        url = "https://www.google.com/complete/search"
        params = {"q": query, "client": "firefox", "hl": language, "gl": country}
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list) and len(data) > 1:
                    return [s for s in data[1] if isinstance(s, str)]
            
            return []
            
        except Exception:
            return []
    
    async def fetch_suggestions_yandex(self, query: str, language: str, region_id: int, client: httpx.AsyncClient) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç Yandex Suggest"""
        url = "https://suggest.yandex.ru/suggest-ff.cgi"
        params = {
            "part": query,
            "uil": language,
            "v": "3",
            "lr": region_id  # 0=–±–µ–∑ —Ä–µ–≥–∏–æ–Ω–∞, 143=–ö–∏–µ–≤, 213=–ú–æ—Å–∫–≤–∞, 20544=–•–∞—Ä—å–∫–æ–≤
        }
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                # Yandex –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [query, [suggestions], ...]
                if isinstance(data, list) and len(data) > 1 and isinstance(data[1], list):
                    return [s for s in data[1] if isinstance(s, str)]
            
            return []
            
        except Exception:
            return []
    
    async def parse_with_semaphore(self, queries: List[str], country: str, language: str, parallel_limit: int, use_yandex: bool = False, region_id: int = 0) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏"""
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def fetch_with_limit(query: str, client: httpx.AsyncClient):
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                
                if use_yandex:
                    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ–±–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                    google_task = self.fetch_suggestions(query, country, language, client)
                    yandex_task = self.fetch_suggestions_yandex(query, language, region_id, client)
                    google_results, yandex_results = await asyncio.gather(google_task, yandex_task)
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    combined = list(set(google_results + yandex_results))
                    return combined
                else:
                    # –¢–æ–ª—å–∫–æ Google
                    return await self.fetch_suggestions(query, country, language, client)
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = [fetch_with_limit(q, client) for q in queries]
            results = await asyncio.gather(*tasks)
        
        all_keywords = set()
        for suggestions in results:
            all_keywords.update(suggestions)
        
        success_count = sum(1 for r in results if r)
        fail_count = len(results) - success_count
        
        return {
            "keywords": sorted(list(all_keywords)),
            "success": success_count,
            "failed": fail_count
        }
    
    # ============================================
    # DUAL METHOD (GOOGLE + YANDEX)
    # ============================================
    async def parse_dual(self, seed: str, country: str, region_id: int, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """DUAL –º–µ—Ç–æ–¥: Google + Yandex –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        start_time = time.time()
        print(f"\nüîµüî¥ DUAL (Google + Yandex): {seed}")
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ–±–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def fetch_dual(query: str, client: httpx.AsyncClient):
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ Google + Yandex
                google_task = self.fetch_suggestions(query, country, language, client)
                yandex_task = self.fetch_suggestions_yandex(query, language, region_id, client)
                google_results, yandex_results = await asyncio.gather(google_task, yandex_task)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º
                combined = list(set(google_results + yandex_results))
                
                return {
                    "google": google_results,
                    "yandex": yandex_results,
                    "combined": combined
                }
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = [fetch_dual(q, client) for q in queries]
            results = await asyncio.gather(*tasks)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        all_keywords = set()
        google_only_keywords = set()
        yandex_only_keywords = set()
        
        for result in results:
            all_keywords.update(result["combined"])
            
            google_set = set(result["google"])
            yandex_set = set(result["yandex"])
            
            google_only_keywords.update(google_set - yandex_set)
            yandex_only_keywords.update(yandex_set - google_set)
        
        elapsed_time = time.time() - start_time
        
        google_total = len(all_keywords) - len(yandex_only_keywords)
        yandex_total = len(all_keywords) - len(google_only_keywords)
        overlap = google_total + yandex_total - len(all_keywords)
        
        print(f"‚úÖ –ò–¢–û–ì–û: {len(all_keywords)} –∫–ª—é—á–µ–π")
        print(f"üîµ Google: {google_total} ({len(google_only_keywords)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)")
        print(f"üî¥ Yandex: {yandex_total} ({len(yandex_only_keywords)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "dual",
            "sources": ["Google Autocomplete", "Yandex Suggest"],
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "queries": len(queries),
            "elapsed_time": round(elapsed_time, 2),
            "breakdown": {
                "google": {
                    "total": google_total,
                    "unique": len(google_only_keywords)
                },
                "yandex": {
                    "total": yandex_total,
                    "unique": len(yandex_only_keywords)
                },
                "overlap": overlap,
                "yandex_gain": f"+{round(len(yandex_only_keywords) / google_total * 100, 1)}%" if google_total > 0 else "0%"
            }
        }
    
    # ============================================
    # YANDEX METHOD
    # ============================================
    async def parse_yandex(self, seed: str, region_id: int, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """YANDEX –¢–û–õ–¨–ö–û –º–µ—Ç–æ–¥ - –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ Yandex"""
        start_time = time.time()
        print(f"\nüî¥ YANDEX: {seed}")
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Yandex
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def fetch_yandex_only(query: str, client: httpx.AsyncClient):
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                return await self.fetch_suggestions_yandex(query, language, region_id, client)
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = [fetch_yandex_only(q, client) for q in queries]
            results = await asyncio.gather(*tasks)
        
        all_keywords = set()
        for suggestions in results:
            all_keywords.update(suggestions)
        
        success_count = sum(1 for r in results if r)
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ {len(all_keywords)} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "yandex",
            "source": "Yandex Suggest",
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "queries": len(queries),
            "region_id": region_id,
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # SUFFIX METHOD
    # ============================================
    async def parse(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """SUFFIX –º–µ—Ç–æ–¥"""
        start_time = time.time()
        print(f"\n‚ö° SUFFIX: {seed}")
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        result = await self.parse_with_semaphore(queries, country, language, parallel_limit)
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ {len(result['keywords'])} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "suffix",
            "keywords": result["keywords"],
            "count": len(result["keywords"]),
            "queries": len(queries),
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # INFIX METHOD
    # ============================================
    async def parse_infix(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """INFIX –º–µ—Ç–æ–¥"""
        start_time = time.time()
        print(f"\nüîÑ INFIX: {seed}")
        
        words = seed.strip().split()
        
        if len(words) < 2:
            return {"error": "INFIX —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞", "seed": seed}
        
        modifiers = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
        queries = []
        
        for i in range(1, len(words)):
            for mod in modifiers:
                query = ' '.join(words[:i]) + f' {mod} ' + ' '.join(words[i:])
                queries.append(query)
        
        result = await self.parse_with_semaphore(queries, country, language, parallel_limit)
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ {len(result['keywords'])} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "infix",
            "keywords": result["keywords"],
            "count": len(result["keywords"]),
            "queries": len(queries),
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # MORPHOLOGY METHOD
    # ============================================
    async def parse_morphology(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """MORPHOLOGY –º–µ—Ç–æ–¥ - –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –í–°–ï —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –≤ –∑–∞–ø—Ä–æ—Å–µ"""
        start_time = time.time()
        print(f"\nüöÄ MORPHOLOGY: {seed}")
        
        words = seed.strip().split()
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –≤ –∑–∞–ø—Ä–æ—Å–µ
        nouns_to_modify = []
        
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                
                for idx, word in enumerate(words):
                    parsed = morph.parse(word)
                    if parsed:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º
                        pos = parsed[0].tag.POS
                        if pos == 'NOUN':
                            nouns_to_modify.append({
                                'index': idx,
                                'word': word,
                                'forms': self.get_morphological_forms(word, language)
                            })
                            print(f"üìå –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ #{idx}: '{word}' ‚Üí {len(self.get_morphological_forms(word, language))} —Ñ–æ—Ä–º")
                
                if not nouns_to_modify:
                    print(f"‚ö†Ô∏è –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ")
                    last_word = words[-1]
                    nouns_to_modify.append({
                        'index': len(words) - 1,
                        'word': last_word,
                        'forms': self.get_morphological_forms(last_word, language)
                    })
                
            except ImportError:
                print(f"‚ö†Ô∏è pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ")
                last_word = words[-1]
                nouns_to_modify.append({
                    'index': len(words) - 1,
                    'word': last_word,
                    'forms': [last_word]
                })
        else:
            # –î–ª—è –Ω–µ-—Ä—É—Å—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
            last_word = words[-1]
            nouns_to_modify.append({
                'index': len(words) - 1,
                'word': last_word,
                'forms': self.get_morphological_forms(last_word, language)
            })
        
        print(f"üìö –ë—É–¥–µ–º –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å: {len(nouns_to_modify)} —Å–ª–æ–≤(–∞)")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ–æ—Ä–º
        all_seeds = []
        
        if len(nouns_to_modify) == 1:
            # –û–¥–Ω–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ - –ø—Ä–æ—Å—Ç–æ –º–µ–Ω—è–µ–º —Ñ–æ—Ä–º—ã
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        
        else:
            # –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö - –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ü–ï–†–í–û–ï (–æ–±—ã—á–Ω–æ —ç—Ç–æ –≥–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ)
            # –ù–∞–ø—Ä–∏–º–µ—Ä: "—Ä–µ–º–æ–Ω—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤" ‚Üí –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º "—Ä–µ–º–æ–Ω—Ç"
            noun = nouns_to_modify[0]
            print(f"üéØ –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–µ—Ä–≤–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ: '{noun['word']}'")
            
            for form in noun['forms']:
                new_words = words.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        
        unique_seeds = list(set(all_seeds))
        print(f"üìã –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ seed: {len(unique_seeds)}")
        
        # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        async def parse_single_seed(seed_variant: str) -> Dict:
            modifiers = self.get_modifiers(language, use_numbers, seed)
            queries = [f"{seed_variant} {mod}" for mod in modifiers]
            result = await self.parse_with_semaphore(queries, country, language, parallel_limit)
            return {"keywords": result["keywords"], "queries": len(queries)}
        
        tasks = [parse_single_seed(s) for s in unique_seeds]
        seed_results = await asyncio.gather(*tasks)
        
        all_keywords = set()
        total_queries = 0
        
        for seed_result in seed_results:
            all_keywords.update(seed_result["keywords"])
            total_queries += seed_result["queries"]
        
        elapsed_time = time.time() - start_time
        print(f"‚úÖ {len(all_keywords)} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "morphology",
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "forms_count": len(unique_seeds),
            "nouns_modified": len(nouns_to_modify),
            "queries": total_queries,
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # COMPARE METHOD
    # ============================================
    async def compare_all(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int, include_keywords: bool) -> Dict:
        """COMPARE: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤"""
        print(f"\nüî• COMPARE: {seed}")
        
        suffix_result = await self.parse(seed, country, language, use_numbers, parallel_limit)
        self.adaptive_delay = AdaptiveDelay()
        
        infix_result = await self.parse_infix(seed, country, language, use_numbers, parallel_limit)
        self.adaptive_delay = AdaptiveDelay()
        
        morphology_result = await self.parse_morphology(seed, country, language, use_numbers, parallel_limit)
        
        suffix_kw = set(suffix_result["keywords"])
        infix_kw = set(infix_result.get("keywords", []))
        morphology_kw = set(morphology_result["keywords"])
        all_unique = suffix_kw | infix_kw | morphology_kw
        
        response = {
            "seed": seed,
            "comparison": {
                "suffix": {
                    "count": len(suffix_kw),
                    "time": suffix_result["elapsed_time"],
                    "queries": suffix_result["queries"]
                },
                "infix": {
                    "count": len(infix_kw),
                    "time": infix_result.get("elapsed_time", 0),
                    "queries": infix_result.get("queries", 0)
                },
                "morphology": {
                    "count": len(morphology_kw),
                    "time": morphology_result["elapsed_time"],
                    "queries": morphology_result["queries"],
                    "forms": morphology_result["forms_count"]
                },
                "total_unique": len(all_unique),
                "total_time": suffix_result["elapsed_time"] + infix_result.get("elapsed_time", 0) + morphology_result["elapsed_time"]
            }
        }
        
        if include_keywords:
            response["keywords"] = {"all_unique": sorted(list(all_unique))}
        
        return response


# ============================================
# API ENDPOINTS
# ============================================

@app.get("/")
async def root():
    return {"status": "ok", "version": "4.0", "methods": ["suffix", "infix", "morphology", "compare"]}


@app.get("/api/parse")
async def parse_suffix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10)
):
    parser = KeywordParser()
    return await parser.parse(seed, country, language, False, parallel)


@app.get("/api/parse-infix")
async def parse_infix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10)
):
    parser = KeywordParser()
    return await parser.parse_infix(seed, country, language, False, parallel)


@app.get("/api/parse-dual")
async def parse_dual_sources(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã –¥–ª—è Google"),
    region: int = Query(187, description="Yandex Region ID (187=–£–∫—Ä–∞–∏–Ω–∞)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, ge=1, le=10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤")
):
    """DUAL –ø–∞—Ä—Å–∏–Ω–≥: Google + Yandex –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
    parser = KeywordParser()
    return await parser.parse_dual(seed, country, region, language, False, parallel)


@app.get("/api/parse-yandex")
async def parse_yandex_only(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"),
    region: int = Query(0, description="Yandex Region ID (0=–≤—Å–µ, 143=–ö–∏–µ–≤, 213=–ú–æ—Å–∫–≤–∞, 20544=–•–∞—Ä—å–∫–æ–≤)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, ge=1, le=10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤")
):
    """YANDEX –ø–∞—Ä—Å–∏–Ω–≥ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"""
    parser = KeywordParser()
    return await parser.parse_yandex(seed, region, language, False, parallel)


@app.get("/api/parse-morphology")
async def parse_morphology(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10)
):
    parser = KeywordParser()
    return await parser.parse_morphology(seed, country, language, False, parallel)


@app.get("/api/compare")
async def compare_methods(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10),
    include_keywords: bool = Query(True)
):
    parser = KeywordParser()
    return await parser.compare_all(seed, country, language, False, parallel, include_keywords)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
