"""
GOOGLE AUTOCOMPLETE PARSER - OPTIMIZED VERSION
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é

Version: 3.6 Clean
–í—Ä–µ–º—è: ~2 —Å–µ–∫ –Ω–∞ 56 –∑–∞–ø—Ä–æ—Å–æ–≤ (17√ó –±—ã—Å—Ç—Ä–µ–µ –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏!)

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
- Connection Pooling (–ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ HTTP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π)
- Adaptive Delay (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫)
- Parallel Requests (5 –ø–æ—Ç–æ–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
- Smart Filtering (—É–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤)
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Dict
import httpx
import asyncio
import time
import random

# ============================================
# FASTAPI APP
# ============================================
app = FastAPI(
    title="Google Autocomplete Parser - Optimized", 
    version="3.6",
    description="–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ (17√ó –±—ã—Å—Ç—Ä–µ–µ!)"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# USER AGENTS
# ============================================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

# ============================================
# ADAPTIVE DELAY
# ============================================
class AdaptiveDelay:
    """
    –£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π
    - –ü—Ä–∏ —É—Å–ø–µ—Ö–µ ‚Üí —É–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É (—É—Å–∫–æ—Ä—è–µ–º—Å—è)
    - –ü—Ä–∏ 429 ‚Üí —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É (–∑–∞—â–∏—Ç–∞ –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
    """
    
    def __init__(self, initial_delay=0.2, min_delay=0.1, max_delay=1.0):
        self.current_delay = initial_delay
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.decrease_factor = 0.95  # –£–º–µ–Ω—å—à–∞–µ–º –Ω–∞ 5% –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
        self.increase_factor = 2.0   # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤ 2 —Ä–∞–∑–∞ –ø—Ä–∏ 429
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_requests = 0
        self.successful_requests = 0
        self.rate_limit_hits = 0
    
    async def wait(self):
        """–ñ–¥—ë–º —Ç–µ–∫—É—â—É—é –∑–∞–¥–µ—Ä–∂–∫—É"""
        await asyncio.sleep(self.current_delay)
    
    def record_success(self):
        """–£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚Üí —É–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É"""
        self.total_requests += 1
        self.successful_requests += 1
        self.current_delay = max(self.min_delay, self.current_delay * self.decrease_factor)
    
    def record_rate_limit(self):
        """Rate limit ‚Üí —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É"""
        self.total_requests += 1
        self.rate_limit_hits += 1
        self.current_delay = min(self.max_delay, self.current_delay * self.increase_factor)
        print(f"üî¥ Rate limit! –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–æ {self.current_delay:.3f} —Å–µ–∫")
    
    def record_error(self):
        """–î—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞"""
        self.total_requests += 1
    
    def get_stats(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        avg_delay = self.current_delay
        return {
            "final_delay": round(self.current_delay, 3),
            "rate_limit_hits": self.rate_limit_hits,
            "success_rate": round(self.successful_requests / self.total_requests * 100, 1) if self.total_requests > 0 else 0
        }

# ============================================
# KEYWORD PARSER (SUFFIX + INFIX)
# ============================================
class KeywordParser:
    def __init__(self):
        self.base_url = "https://suggestqueries.google.com/complete/search"
        self.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
        
        # –Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã)
        self.language_modifiers = {
            'en': [],
            'ru': list("–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"),  # –ü–æ–ª–Ω—ã–π —Ä—É—Å—Å–∫–∏–π –∞–ª—Ñ–∞–≤–∏—Ç!
            'uk': list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—å—é—è—ñ—ó—î“ë"),
            'de': list("√§√∂√º√ü"),
            'fr': list("√†√¢√§√¶√ß√©√®√™√´√Ø√Æ√¥√π√ª√º√ø"),
            'es': list("√°√©√≠√±√≥√∫√º"),
            'pl': list("ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º"),
            'it': list("√†√®√©√¨√≠√Æ√≤√≥√π√∫"),
        }
        
        # –†–µ–¥–∫–∏–µ –±—É–∫–≤—ã (–º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å)
        self.rare_chars = {
            'ru': ['—ä', '—ë', '—ã'],  # –£–±—Ä–∞–ª '—å' —á—Ç–æ–±—ã –±—ã–ª–æ 56 –∫–∞–∫ –≤—á–µ—Ä–∞
            'uk': ['—ä'],  # –£–±—Ä–∞–ª '—å' —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
            'pl': ['ƒÖ', 'ƒô'],
        }
    
    def detect_seed_language(self, seed: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ seed (latin/cyrillic)"""
        has_latin = any(ord('a') <= ord(c.lower()) <= ord('z') for c in seed if c.isalpha())
        has_cyrillic = any(ord('–∞') <= ord(c.lower()) <= ord('—è') for c in seed if c.isalpha())
        
        if has_cyrillic:
            return 'cyrillic'
        return 'latin'
    
    def get_modifiers(self, language: str, use_numbers: bool, seed: str, cyrillic_only: bool = False) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —É–º–Ω–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        
        –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –î–õ–Ø –ë–†–ï–ù–î–û–í:
        - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π seed ‚Üí —É–±–∏—Ä–∞–µ–º –≤—Å—ë –∫—Ä–æ–º–µ a-z (–Ω–µ—Ç –±—Ä–µ–Ω–¥–æ–≤ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ)
        - –î—Ä—É–≥–∏–µ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ seed ‚Üí —É–±–∏—Ä–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É (–Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã —è–∑—ã–∫–∞)
        - –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–π seed ‚Üí –û–°–¢–ê–í–õ–Ø–ï–ú –í–°–Å (–ª–∞—Ç–∏–Ω–∏—Ü—É –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤ + –∫–∏—Ä–∏–ª–ª–∏—Ü—É!)
        
        cyrillic_only: –¢–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (–¥–ª—è INFIX - –ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞)
        """
        seed_lang = self.detect_seed_language(seed)
        base_latin = list("abcdefghijklmnopqrstuvwxyz")
        numbers = list("0123456789") if use_numbers else []
        lang_specific = self.language_modifiers.get(language.lower(), [])
        
        # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø
        if language.lower() == 'en' and seed_lang == 'latin':
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π seed ‚Üí —Ç–æ–ª—å–∫–æ a-z + —Ü–∏—Ñ—Ä—ã
            modifiers = base_latin + numbers
            
        elif seed_lang == 'latin':
            # –õ–∞—Ç–∏–Ω—Å–∫–∏–π seed (–Ω–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π) ‚Üí —É–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û –∫–∏—Ä–∏–ª–ª–∏—Ü—É
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û']
            non_cyrillic = [m for m in lang_specific if not is_cyrillic(m)]
            modifiers = base_latin + non_cyrillic + numbers
            
        else:
            # –ö–ò–†–ò–õ–õ–ò–ß–ï–°–ö–ò–ô seed ‚Üí –æ—Å—Ç–∞–≤–ª—è–µ–º –í–°–Å (–ª–∞—Ç–∏–Ω–∏—Ü—É –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤!)
            modifiers = base_latin + lang_specific + numbers
        
        # –£–±–∏—Ä–∞–µ–º —Ä–µ–¥–∫–∏–µ –±—É–∫–≤—ã
        rare = self.rare_chars.get(language.lower(), [])
        if rare:
            modifiers = [m for m in modifiers if m not in rare]
        
        # –¢–û–õ–¨–ö–û –ö–ò–†–ò–õ–õ–ò–¶–ê (–¥–ª—è INFIX)
        if cyrillic_only and seed_lang == 'cyrillic':
            # –£–±–∏—Ä–∞–µ–º –ª–∞—Ç–∏–Ω–∏—Ü—É –∏ —Ü–∏—Ñ—Ä—ã
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û', '—å']
            modifiers = [m for m in modifiers if is_cyrillic(m)]
        
        return modifiers
    
    def get_morphological_forms(self, word: str, language: str) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏:
        - RU, UK: pymorphy3 (–≤—Å–µ —Ñ–æ—Ä–º—ã –∏–∑ –ª–µ–∫—Å–µ–º—ã)
        - EN: lemminflect (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ/–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ + –ø—Ä–∏—Ç—è–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã)
        - –î—Ä—É–≥–∏–µ: –±–∞–∑–æ–≤–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (–ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞)
        """
        forms = set([word])  # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ñ–æ—Ä–º—É
        
        try:
            if language.lower() in ['ru', 'uk']:
                # –†—É—Å—Å–∫–∏–π –∏ –£–∫—Ä–∞–∏–Ω—Å–∫–∏–π - pymorphy3
                try:
                    import pymorphy3
                    
                    # –°–æ–∑–¥–∞—ë–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
                    morph = pymorphy3.MorphAnalyzer()
                    
                    # –ü–∞—Ä—Å–∏–º —Å–ª–æ–≤–æ
                    parsed = morph.parse(word)
                    
                    if parsed:
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Ä–∞–∑–±–æ—Ä–∞
                        p = parsed[0]
                        
                        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –∏–∑ –ª–µ–∫—Å–µ–º—ã
                        for form in p.lexeme:
                            forms.add(form.word)
                    
                    print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (RU/UK): '{word}' ‚Üí {len(forms)} —Ñ–æ—Ä–º —á–µ—Ä–µ–∑ pymorphy3")
                    
                except ImportError:
                    print(f"‚ö†Ô∏è pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é —Ñ–æ—Ä–º—É.")
                    print(f"   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pymorphy3 pymorphy3-dicts-ru")
            
            elif language.lower() == 'en':
                # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π - lemminflect –∏–ª–∏ –±–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞
                try:
                    import lemminflect
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º—ã —á–µ—Ä–µ–∑ lemminflect
                    # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ
                    plurals = lemminflect.getAllInflections(word, upos='NOUN')
                    if plurals and 'NNS' in plurals:
                        forms.update(plurals['NNS'])
                    
                    # –ü—Ä–∏—Ç—è–∂–∞—Ç–µ–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞
                    if not word.endswith('s'):
                        forms.add(word + "'s")
                        forms.add(word + "s")
                    
                    print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (EN): '{word}' ‚Üí {len(forms)} —Ñ–æ—Ä–º (lemminflect)")
                    
                except ImportError:
                    # Fallback - –±–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞
                    forms.add(word)  # singular
                    
                    # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ (–ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞)
                    if word.endswith('y') and len(word) > 1 and word[-2] not in 'aeiou':
                        plural = word[:-1] + 'ies'  # baby ‚Üí babies
                    elif word.endswith(('s', 'x', 'z', 'ch', 'sh')):
                        plural = word + 'es'  # box ‚Üí boxes
                    elif word.endswith('o') and len(word) > 1 and word[-2] not in 'aeiou':
                        plural = word + 'es'  # hero ‚Üí heroes
                    elif word.endswith('f'):
                        plural = word[:-1] + 'ves'  # leaf ‚Üí leaves
                    elif word.endswith('fe'):
                        plural = word[:-2] + 'ves'  # knife ‚Üí knives
                    else:
                        plural = word + 's'  # regular
                    
                    forms.add(plural)
                    
                    # –ü—Ä–∏—Ç—è–∂–∞—Ç–µ–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞
                    if not word.endswith('s'):
                        forms.add(word + "'s")
                    
                    print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (EN): '{word}' ‚Üí {len(forms)} —Ñ–æ—Ä–º (–±–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞)")
                    print(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: pip install lemminflect")
            
            else:
                # –î—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ - —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞
                print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è ({language.upper()}): '{word}' ‚Üí 1 —Ñ–æ—Ä–º–∞ (–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)")
                print(f"   –î–ª—è –ø–æ–ª–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è spacy + —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å")
        
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏: {e}")
        
        return sorted(list(forms))
    
    async def fetch_suggestions(self, query: str, country: str, language: str, client: httpx.AsyncClient) -> tuple:
        """
        –ó–∞–ø—Ä–æ—Å –∫ Google Autocomplete API
        Returns: (suggestions, success, is_rate_limit)
        """
        params = {"client": "chrome", "q": query, "gl": country, "hl": language}
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(self.base_url, params=params, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list) and len(data) > 1:
                    suggestions = [s for s in data[1] if isinstance(s, str)]
                    return (suggestions, True, False)
                return ([], True, False)
            
            elif response.status_code == 429:
                return ([], False, True)  # Rate limit
            
            return ([], True, False)
        
        except Exception as e:
            return ([], False, False)
    
    async def fetch_with_delay(self, modifier: str, seed: str, country: str, language: str, client: httpx.AsyncClient, custom_query: str = None) -> tuple:
        """–ó–∞–ø—Ä–æ—Å —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ connection pooling"""
        try:
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            await self.adaptive_delay.wait()
            
            # –ó–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ shared client (connection pooling!)
            query = custom_query if custom_query else f"{seed} {modifier}"
            results, success, is_rate_limit = await self.fetch_suggestions(query, country, language, client)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
            if is_rate_limit:
                self.adaptive_delay.record_rate_limit()
                return (modifier, [], False)
            elif success:
                self.adaptive_delay.record_success()
                return (modifier, results, True)
            else:
                self.adaptive_delay.record_error()
                return (modifier, [], False)
        
        except Exception as e:
            self.adaptive_delay.record_error()
            return (modifier, [], False)
    
    async def parse(self, seed: str, country: str, language: str, use_numbers: bool = True, parallel_limit: int = 5) -> Dict:
        """
        SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        –ü–∞—Ç—Ç–µ—Ä–Ω: seed + modifier
        """
        start_time = time.time()
        all_keywords = set()
        
        print(f"\n{'='*60}")
        print(f"SUFFIX PARSER - OPTIMIZED v3.6")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"Country: {country.upper()}, Language: {language.upper()}")
        print(f"Parallel: {parallel_limit}, Adaptive Delay: 0.1-1.0 —Å–µ–∫\n")
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        modifiers = self.get_modifiers(language, use_numbers, seed)
        print(f"üìä –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã: {modifiers[:10]}... (–≤—Å–µ–≥–æ {len(modifiers)})\n")
        
        # –°—á—ë—Ç—á–∏–∫–∏
        total_queries = 0
        successful_queries = 0
        failed_queries = 0
        
        # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì —Å Connection Pooling
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async with httpx.AsyncClient(timeout=10.0) as shared_client:
            print(f"üèä Connection pooling: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π HTTP –∫–ª–∏–µ–Ω—Ç\n")
            
            async def fetch_limited(modifier):
                async with semaphore:
                    return await self.fetch_with_delay(modifier, seed, country, language, shared_client)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = [fetch_limited(m) for m in modifiers]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_queries += 1
                total_queries += 1
                continue
            
            modifier, suggestions, success = result
            total_queries += 1
            
            if success:
                all_keywords.update(suggestions)
                successful_queries += 1
                if i < 5 or len(suggestions) > 0:
                    print(f"[{i+1}/{len(modifiers)}] '{seed} {modifier}' ‚Üí {len(suggestions)} results")
            else:
                failed_queries += 1
        
        elapsed_time = time.time() - start_time
        delay_stats = self.adaptive_delay.get_stats()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'='*60}")
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {total_queries} (‚úÖ {successful_queries}, ‚ùå {failed_queries})")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"–í—Ä–µ–º—è: {elapsed_time:.2f} —Å–µ–∫ ({elapsed_time/total_queries:.2f} —Å–µ–∫/–∑–∞–ø—Ä–æ—Å)")
        print(f"–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º: {parallel_limit}")
        print(f"üß† Adaptive Delay: {delay_stats['final_delay']:.3f} —Å–µ–∫ (rate limits: {delay_stats['rate_limit_hits']})")
        print(f"üèä Connection Pooling: –í–ö–õ–Æ–ß–Å–ù")
        print(f"{'='*60}\n")
        
        return {
            "method": "SUFFIX Optimized",
            "seed": seed,
            "country": country,
            "language": language,
            "modifiers_count": len(modifiers),  # –î–û–ë–ê–í–ò–õ–ò!
            "modifiers_sample": modifiers[:10],  # –î–û–ë–ê–í–ò–õ–ò!
            "queries": total_queries,
            "successful_queries": successful_queries,
            "count": len(all_keywords),
            "keywords": sorted(list(all_keywords)),
            "elapsed_time": round(elapsed_time, 2),
            "avg_time_per_query": round(elapsed_time / total_queries, 2),
            "adaptive_delay": delay_stats
        }
    
    async def parse_infix(self, seed: str, country: str, language: str, use_numbers: bool = True, parallel_limit: int = 5) -> Dict:
        """
        INFIX –ø–∞—Ä—Å–∏–Ω–≥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        –ü–∞—Ç—Ç–µ—Ä–Ω: word1 + modifier + word2
        
        –ü—Ä–∏–º–µ—Ä:
        Seed: "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"
        –†–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞: ["—Ä–µ–º–æ–Ω—Ç", "–ø—ã–ª–µ—Å–æ—Å–æ–≤"]
        –ó–∞–ø—Ä–æ—Å—ã: "—Ä–µ–º–æ–Ω—Ç –∞ –ø—ã–ª–µ—Å–æ—Å–æ–≤", "—Ä–µ–º–æ–Ω—Ç –± –ø—ã–ª–µ—Å–æ—Å–æ–≤", ...
        """
        start_time = time.time()
        all_keywords = set()
        
        print(f"\n{'='*60}")
        print(f"INFIX PARSER - OPTIMIZED v3.6")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"Country: {country.upper()}, Language: {language.upper()}")
        print(f"Parallel: {parallel_limit}, Adaptive Delay: 0.1-1.0 —Å–µ–∫\n")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º seed –Ω–∞ —Å–ª–æ–≤–∞
        words = seed.strip().split()
        
        if len(words) < 2:
            return {
                "error": "INFIX —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞ –≤ seed",
                "example": "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ (2 —Å–ª–æ–≤–∞) ‚úÖ",
                "your_seed": f"{seed} ({len(words)} —Å–ª–æ–≤) ‚ùå"
            }
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–¢–û–õ–¨–ö–û –ö–ò–†–ò–õ–õ–ò–¶–ê –¥–ª—è INFIX!)
        modifiers = self.get_modifiers(language, use_numbers=False, seed=seed, cyrillic_only=True)
        print(f"üìä –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã: {modifiers[:10]}... (–≤—Å–µ–≥–æ {len(modifiers)})")
        print(f"üìä –ü–∞—Ç—Ç–µ—Ä–Ω INFIX: '{words[0]}' + modifier + '{' '.join(words[1:])}'")
        print(f"üìä –ü—Ä–∏–º–µ—Ä: '{words[0]} {modifiers[0] if modifiers else '–∞'} {' '.join(words[1:])}'")
        
        # –°—á—ë—Ç—á–∏–∫–∏
        total_queries = 0
        successful_queries = 0
        failed_queries = 0
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
        
        # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì —Å Connection Pooling
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async with httpx.AsyncClient(timeout=10.0) as shared_client:
            print(f"üèä Connection pooling: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π HTTP –∫–ª–∏–µ–Ω—Ç\n")
            
            async def fetch_limited(modifier):
                async with semaphore:
                    # INFIX –ø–∞—Ç—Ç–µ—Ä–Ω: word1 + modifier + word2 word3...
                    infix_seed = f"{words[0]} {modifier} {' '.join(words[1:])}"
                    return await self.fetch_with_delay(modifier, words[0] + " _", country, language, shared_client, custom_query=infix_seed)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = [fetch_limited(m) for m in modifiers]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_queries += 1
                total_queries += 1
                continue
            
            modifier, suggestions, success = result
            total_queries += 1
            
            if success:
                all_keywords.update(suggestions)
                successful_queries += 1
                infix_query = f"{words[0]} {modifier} {' '.join(words[1:])}"
                if i < 5 or len(suggestions) > 0:
                    print(f"[{i+1}/{len(modifiers)}] '{infix_query}' ‚Üí {len(suggestions)} results")
            else:
                failed_queries += 1
        
        elapsed_time = time.time() - start_time
        delay_stats = self.adaptive_delay.get_stats()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'='*60}")
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {total_queries} (‚úÖ {successful_queries}, ‚ùå {failed_queries})")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"–í—Ä–µ–º—è: {elapsed_time:.2f} —Å–µ–∫ ({elapsed_time/total_queries:.2f} —Å–µ–∫/–∑–∞–ø—Ä–æ—Å)")
        print(f"–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º: {parallel_limit}")
        print(f"üß† Adaptive Delay: {delay_stats['final_delay']:.3f} —Å–µ–∫ (rate limits: {delay_stats['rate_limit_hits']})")
        print(f"üèä Connection Pooling: –í–ö–õ–Æ–ß–Å–ù")
        print(f"{'='*60}\n")
        
        return {
            "method": "INFIX Optimized",
            "seed": seed,
            "pattern": f"'{words[0]}' + modifier + '{' '.join(words[1:])}'",
            "country": country,
            "language": language,
            "modifiers_count": len(modifiers),
            "modifiers_sample": modifiers[:10],
            "queries": total_queries,
            "successful_queries": successful_queries,
            "count": len(all_keywords),
            "keywords": sorted(list(all_keywords)),
            "elapsed_time": round(elapsed_time, 2),
            "avg_time_per_query": round(elapsed_time / total_queries, 2),
            "adaptive_delay": delay_stats
        }
    
    async def parse_morphology(self, seed: str, country: str, language: str, use_numbers: bool = True, parallel_limit: int = 5) -> Dict:
        """
        SUFFIX –ü–ê–†–°–ò–ù–ì –° –ú–û–†–§–û–õ–û–ì–ò–ï–ô
        
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ–≤–∞ –≤ seed
        –∏ –ø–∞—Ä—Å–∏—Ç –∫–∞–∂–¥—É—é —Ñ–æ—Ä–º—É –æ—Ç–¥–µ–ª—å–Ω–æ.
        
        –ü—Ä–∏–º–µ—Ä:
        Seed: "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"
        –§–æ—Ä–º—ã: ["–ø—ã–ª–µ—Å–æ—Å", "–ø—ã–ª–µ—Å–æ—Å–∞", "–ø—ã–ª–µ—Å–æ—Å—É", "–ø—ã–ª–µ—Å–æ—Å–æ–º", "–ø—ã–ª–µ—Å–æ—Å–µ", 
                "–ø—ã–ª–µ—Å–æ—Å—ã", "–ø—ã–ª–µ—Å–æ—Å–æ–≤", "–ø—ã–ª–µ—Å–æ—Å–∞–º", "–ø—ã–ª–µ—Å–æ—Å–∞–º–∏", "–ø—ã–ª–µ—Å–æ—Å–∞—Ö"]
        
        –î–ª—è –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—ã –¥–µ–ª–∞–µ–º SUFFIX –ø–∞—Ä—Å–∏–Ω–≥:
        - "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–∞ –∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–∞ –±", ...
        - "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å—É –∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å—É –±", ...
        - ...
        
        –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —è–∑—ã–∫–æ–≤:
        - RU, UK: –ø–æ–ª–Ω–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (10+ —Ñ–æ—Ä–º)
        - EN: –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ/–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ (2 —Ñ–æ—Ä–º—ã)
        - –î—Ä—É–≥–∏–µ: —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞
        """
        start_time = time.time()
        all_keywords = set()
        
        print(f"\n{'='*60}")
        print(f"MORPHOLOGY PARSER - SUFFIX + AUTO MORPHOLOGY")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"Country: {country.upper()}, Language: {language.upper()}")
        print(f"Parallel: {parallel_limit}\n")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º seed –Ω–∞ —Å–ª–æ–≤–∞
        words = seed.strip().split()
        
        if len(words) == 0:
            return {"error": "Seed –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º"}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏
        base_word = words[-1]
        prefix = " ".join(words[:-1]) if len(words) > 1 else ""
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã
        word_forms = self.get_morphological_forms(base_word, language)
        
        print(f"üìö –ë–∞–∑–æ–≤–æ–µ —Å–ª–æ–≤–æ: '{base_word}'")
        print(f"üìö –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã: {word_forms}")
        print(f"üìö –í—Å–µ–≥–æ —Ñ–æ—Ä–º: {len(word_forms)}\n")
        
        # –°—á—ë—Ç—á–∏–∫–∏
        total_queries = 0
        successful_queries = 0
        failed_queries = 0
        forms_results = {}
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É
        self.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
        
        # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Ñ–æ—Ä–º—É
        for form_idx, word_form in enumerate(word_forms, 1):
            # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π seed —Å —ç—Ç–æ–π —Ñ–æ—Ä–º–æ–π
            if prefix:
                current_seed = f"{prefix} {word_form}"
            else:
                current_seed = word_form
            
            print(f"\n{'‚îÄ'*60}")
            print(f"üìñ –§–û–†–ú–ê {form_idx}/{len(word_forms)}: '{word_form}'")
            print(f"üìñ Seed: '{current_seed}'")
            print(f"{'‚îÄ'*60}\n")
            
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
            modifiers = self.get_modifiers(language, use_numbers, current_seed)
            
            # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì —Å Connection Pooling
            semaphore = asyncio.Semaphore(parallel_limit)
            form_keywords = set()
            
            async with httpx.AsyncClient(timeout=10.0) as shared_client:
                async def fetch_limited(modifier):
                    async with semaphore:
                        return await self.fetch_with_delay(modifier, current_seed, country, language, shared_client)
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                tasks = [fetch_limited(m) for m in modifiers]
                results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–æ–π —Ñ–æ—Ä–º—ã
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    failed_queries += 1
                    total_queries += 1
                    continue
                
                modifier, suggestions, success = result
                total_queries += 1
                
                if success:
                    form_keywords.update(suggestions)
                    all_keywords.update(suggestions)
                    successful_queries += 1
                    if i < 3 or len(suggestions) > 0:
                        print(f"  [{i+1}/{len(modifiers)}] '{current_seed} {modifier}' ‚Üí {len(suggestions)} results")
                else:
                    failed_queries += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–æ–π —Ñ–æ—Ä–º—ã
            forms_results[word_form] = {
                "seed": current_seed,
                "keywords_count": len(form_keywords),
                "unique_keywords": len(form_keywords - (all_keywords - form_keywords))
            }
            
            print(f"\nüìä –§–æ—Ä–º–∞ '{word_form}': {len(form_keywords)} –∫–ª—é—á–µ–π")
        
        elapsed_time = time.time() - start_time
        delay_stats = self.adaptive_delay.get_stats()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'='*60}")
        print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º: {len(word_forms)}")
        print(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {total_queries} (‚úÖ {successful_queries}, ‚ùå {failed_queries})")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"–í—Ä–µ–º—è: {elapsed_time:.2f} —Å–µ–∫ ({elapsed_time/total_queries:.3f} —Å–µ–∫/–∑–∞–ø—Ä–æ—Å)")
        print(f"–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º: {parallel_limit}")
        print(f"üß† Adaptive Delay: {delay_stats['final_delay']:.3f} —Å–µ–∫ (rate limits: {delay_stats['rate_limit_hits']})")
        print(f"üèä Connection Pooling: –í–ö–õ–Æ–ß–Å–ù")
        print(f"{'='*60}\n")
        
        return {
            "method": "SUFFIX + Morphology",
            "seed": seed,
            "base_word": base_word,
            "word_forms": word_forms,
            "forms_count": len(word_forms),
            "country": country,
            "language": language,
            "queries": total_queries,
            "successful_queries": successful_queries,
            "count": len(all_keywords),
            "keywords": sorted(list(all_keywords)),
            "forms_breakdown": forms_results,
            "elapsed_time": round(elapsed_time, 2),
            "avg_time_per_query": round(elapsed_time / total_queries, 3),
            "adaptive_delay": delay_stats
        }

# ============================================
# API ENDPOINTS
# ============================================

@app.get("/")
async def root():
    return {
        "api": "Google Autocomplete Parser - Optimized",
        "version": "3.7",
        "methods": {
            "suffix": "seed + modifier (—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ + –∞) - –ª–∞—Ç–∏–Ω–∏—Ü–∞ + –∫–∏—Ä–∏–ª–ª–∏—Ü–∞",
            "infix": "word1 + modifier + word2 (—Ä–µ–º–æ–Ω—Ç + –∞ + –ø—ã–ª–µ—Å–æ—Å–æ–≤) - —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞",
            "morphology": "SUFFIX –¥–ª—è –≤—Å–µ—Ö –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º —Å–ª–æ–≤–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)"
        },
        "optimizations": [
            "Connection Pooling (–ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π)",
            "Adaptive Delay (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)",
            "Parallel Requests (5 –ø–æ—Ç–æ–∫–æ–≤)",
            "Smart Filtering (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤)",
            "Auto Morphology (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –¥–ª—è RU/UK/EN)"
        ],
        "performance": {
            "baseline": "37.86 —Å–µ–∫",
            "optimized_suffix": "~2.48 —Å–µ–∫ (470 –∫–ª—é—á–µ–π)",
            "optimized_morphology": "~20 —Å–µ–∫ (700-900 –∫–ª—é—á–µ–π –¥–ª—è RU/UK)",
            "speedup": "15√ó –±—ã—Å—Ç—Ä–µ–µ"
        },
        "endpoints": {
            "suffix": "/api/parse",
            "infix": "/api/parse-infix",
            "morphology": "/api/parse-morphology",
            "compare": "/api/compare",
            "examples": {
                "suffix": "/api/parse?seed=—Ä–µ–º–æ–Ω—Ç+–ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru&parallel=5",
                "infix": "/api/parse-infix?seed=—Ä–µ–º–æ–Ω—Ç+–ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru&parallel=5",
                "morphology": "/api/parse-morphology?seed=—Ä–µ–º–æ–Ω—Ç+–ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru&parallel=5",
                "compare": "/api/compare?seed=—Ä–µ–º–æ–Ω—Ç+–ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru&parallel=5"
            }
        },
        "morphology_support": {
            "ru": "‚úÖ –ü–æ–ª–Ω–∞—è (—á–µ—Ä–µ–∑ pymorphy3.lexeme)",
            "uk": "‚úÖ –ü–æ–ª–Ω–∞—è (—á–µ—Ä–µ–∑ pymorphy3.lexeme)",
            "en": "‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è (—á–µ—Ä–µ–∑ lemminflect) –∏–ª–∏ –±–∞–∑–æ–≤–∞—è",
            "other": "‚ö†Ô∏è –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞"
        },
        "required_packages": {
            "ru_uk": "pip install pymorphy3 pymorphy3-dicts-ru",
            "en": "pip install lemminflect (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
            "other": "–ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è (–±–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞)"
        }
    }

@app.get("/api/parse")
async def parse_suffix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (UA, US, RU, DE...)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞ (ru, en, uk, de...)"),
    use_numbers: bool = Query(False, description="–í–∫–ª—é—á–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel: int = Query(5, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (1-10)", ge=1, le=10)
):
    """
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô SUFFIX –ü–ê–†–°–ò–ù–ì
    
    –ü–∞—Ç—Ç–µ—Ä–Ω: seed + [a-z, –∞-—è, 0-9]
    
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - Connection Pooling: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ HTTP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
    - Adaptive Delay: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ (0.1-1.0 —Å–µ–∫)
    - Parallel: 5 –ø–æ—Ç–æ–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    - Smart Filtering: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–∞—Ç–∏–Ω–∏—Ü—É –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤
    
    –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
    - –í—Ä–µ–º—è: ~2 —Å–µ–∫ –Ω–∞ 56 –∑–∞–ø—Ä–æ—Å–æ–≤
    - –£—Å–∫–æ—Ä–µ–Ω–∏–µ: 17√ó –æ—Ç –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏
    """
    parser = KeywordParser()
    result = await parser.parse(
        seed=seed,
        country=country,
        language=language,
        use_numbers=use_numbers,
        parallel_limit=parallel
    )
    return result

@app.get("/api/parse-infix")
async def parse_infix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞)"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (UA, US, RU, DE...)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞ (ru, en, uk, de...)"),
    use_numbers: bool = Query(False, description="–í–∫–ª—é—á–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel: int = Query(5, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (1-10)", ge=1, le=10)
):
    """
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô INFIX –ü–ê–†–°–ò–ù–ì
    
    –ü–∞—Ç—Ç–µ—Ä–Ω: word1 + modifier + word2
    
    –ü—Ä–∏–º–µ—Ä:
    Seed: "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤" ‚Üí "—Ä–µ–º–æ–Ω—Ç –∞ –ø—ã–ª–µ—Å–æ—Å–æ–≤", "—Ä–µ–º–æ–Ω—Ç –± –ø—ã–ª–µ—Å–æ—Å–æ–≤", ...
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - Seed –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ INFIX:
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¢–û–õ–¨–ö–û –∫–∏—Ä–∏–ª–ª–∏—Ü—É (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞)
    - –ë–ï–ó —Ü–∏—Ñ—Ä (—Ü–∏—Ñ—Ä—ã –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è)
    - ~30 –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 56 (–±—ã—Å—Ç—Ä–µ–µ!)
    
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - Connection Pooling: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ HTTP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
    - Adaptive Delay: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ (0.1-1.0 —Å–µ–∫)
    - Parallel: 5 –ø–æ—Ç–æ–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    - Cyrillic Only: —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
    
    –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
    - –í—Ä–µ–º—è: ~1.5 —Å–µ–∫ –Ω–∞ 30 –∑–∞–ø—Ä–æ—Å–æ–≤
    - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏: +15 (~3%)
    """
    parser = KeywordParser()
    result = await parser.parse_infix(
        seed=seed,
        country=country,
        language=language,
        use_numbers=use_numbers,
        parallel_limit=parallel
    )
    return result

@app.get("/api/parse-morphology")
async def parse_morphology(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (UA, US, RU, DE...)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞ (ru, en, uk, de...)"),
    use_numbers: bool = Query(False, description="–í–∫–ª—é—á–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel: int = Query(5, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (1-10)", ge=1, le=10)
):
    """
    SUFFIX –ü–ê–†–°–ò–ù–ì –° –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ô –ú–û–†–§–û–õ–û–ì–ò–ï–ô
    
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Å–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ–≤–∞
    –∏ –ø–∞—Ä—Å–∏—Ç –∫–∞–∂–¥—É—é —Ñ–æ—Ä–º—É —á–µ—Ä–µ–∑ SUFFIX –º–µ—Ç–æ–¥.
    
    –ü—Ä–∏–º–µ—Ä:
    Seed: "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"
    
    –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã (RU):
    - –ß–µ—Ä–µ–∑ pymorphy3.lexeme –ø–æ–ª—É—á–∞–µ—Ç –í–°–ï —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
    - –ø—ã–ª–µ—Å–æ—Å, –ø—ã–ª–µ—Å–æ—Å–∞, –ø—ã–ª–µ—Å–æ—Å—É, –ø—ã–ª–µ—Å–æ—Å–æ–º, –ø—ã–ª–µ—Å–æ—Å–µ (–µ–¥.—á.)
    - –ø—ã–ª–µ—Å–æ—Å—ã, –ø—ã–ª–µ—Å–æ—Å–æ–≤, –ø—ã–ª–µ—Å–æ—Å–∞–º, –ø—ã–ª–µ—Å–æ—Å–∞–º–∏, –ø—ã–ª–µ—Å–æ—Å–∞—Ö (–º–Ω.—á.)
    
    –î–ª—è –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—ã:
    - "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–∞ –∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–∞ –±", ...
    - "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å—É –∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å—É –±", ...
    - ...
    
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —è–∑—ã–∫–æ–≤:
    - RU, UK: –ø–æ–ª–Ω–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è —á–µ—Ä–µ–∑ pymorphy3 (10+ —Ñ–æ—Ä–º)
    - EN: –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ/–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ —á–µ—Ä–µ–∑ lemminflect (2-3 —Ñ–æ—Ä–º—ã)
    - –î—Ä—É–≥–∏–µ: —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞ (1 —Ñ–æ—Ä–º–∞)
    
    –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
    - RU/UK: ~10 —Ñ–æ—Ä–º √ó 56 –∑–∞–ø—Ä–æ—Å–æ–≤ = 560 –∑–∞–ø—Ä–æ—Å–æ–≤ (~20 —Å–µ–∫)
    - EN: ~2-3 —Ñ–æ—Ä–º—ã √ó 56 –∑–∞–ø—Ä–æ—Å–æ–≤ = 112-168 –∑–∞–ø—Ä–æ—Å–æ–≤ (~4-6 —Å–µ–∫)
    
    –†–µ–∑—É–ª—å—Ç–∞—Ç:
    - –ù–∞–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π (+50-100%)
    - –ù–∞—Ö–æ–¥–∏—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞–¥–µ–∂–∞–º–∏
    """
    parser = KeywordParser()
    result = await parser.parse_morphology(
        seed=seed,
        country=country,
        language=language,
        use_numbers=use_numbers,
        parallel_limit=parallel
    )
    return result

@app.get("/api/compare")
async def compare_methods(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤", ge=1, le=10)
):
    """
    –°–†–ê–í–ù–ï–ù–ò–ï SUFFIX vs INFIX
    
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±–∞ –º–µ—Ç–æ–¥–∞ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π
    - –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    - –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏ –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
    """
    parser = KeywordParser()
    
    print("\nüîÑ –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í: SUFFIX vs INFIX\n")
    
    # SUFFIX
    suffix_result = await parser.parse(
        seed=seed,
        country=country,
        language=language,
        use_numbers=False,
        parallel_limit=parallel
    )
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º adaptive delay –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏
    parser.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
    
    # INFIX
    infix_result = await parser.parse_infix(
        seed=seed,
        country=country,
        language=language,
        use_numbers=False,  # INFIX –±–µ–∑ —Ü–∏—Ñ—Ä
        parallel_limit=parallel
    )
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    if "error" in infix_result:
        return {
            "error": "INFIX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —ç—Ç–æ–≥–æ seed",
            "reason": infix_result["error"],
            "suffix_result": suffix_result
        }
    
    suffix_keywords = set(suffix_result["keywords"])
    infix_keywords = set(infix_result["keywords"])
    
    intersection = suffix_keywords & infix_keywords
    suffix_only = suffix_keywords - infix_keywords
    infix_only = infix_keywords - suffix_keywords
    total_unique = suffix_keywords | infix_keywords
    
    return {
        "seed": seed,
        "comparison": {
            "suffix": {
                "count": len(suffix_keywords),
                "time": suffix_result["elapsed_time"],
                "queries": suffix_result["queries"]
            },
            "infix": {
                "count": len(infix_keywords),
                "time": infix_result["elapsed_time"],
                "queries": infix_result["queries"]
            },
            "overlap": {
                "count": len(intersection),
                "percentage": round(len(intersection) / len(total_unique) * 100, 1) if total_unique else 0
            },
            "unique_to_suffix": {
                "count": len(suffix_only),
                "sample": sorted(list(suffix_only))[:10]
            },
            "unique_to_infix": {
                "count": len(infix_only),
                "sample": sorted(list(infix_only))[:10]
            },
            "total_unique": len(total_unique)
        },
        "winner": {
            "by_count": "INFIX" if len(infix_keywords) > len(suffix_keywords) else "SUFFIX",
            "by_speed": "INFIX" if infix_result["elapsed_time"] < suffix_result["elapsed_time"] else "SUFFIX",
            "recommendation": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±–∞ –º–µ—Ç–æ–¥–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è" if len(total_unique) > max(len(suffix_keywords), len(infix_keywords)) * 1.3 else "–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–¥–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞"
        }
    }
