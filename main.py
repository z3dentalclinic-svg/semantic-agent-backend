
"""
FGS Parser API - Version 5.4.2 PRODUCTION
Deployed: 2026-01-10

–†–ê–ó–î–ï–õ–¨–ù–´–ô –í–´–í–û–î –Ø–ö–û–†–ï–ô:
- –í—Å–µ –º–µ—Ç–æ–¥—ã —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç: keywords + anchors –æ—Ç–¥–µ–ª—å–Ω–æ
- Frontend –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —è–∫–æ—Ä—è –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å–µ–∫—Ü–∏–µ–π
- Export –≤ CSV —Å –ø–æ–º–µ—Ç–∫–æ–π —Ç–∏–ø–∞ (–ö–ª—é—á/–Ø–∫–æ—Ä—å)

API Response format:
{
  "keywords": [...],     // –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏
  "anchors": [...],      // –°–æ–∑–¥–∞–Ω–Ω—ã–µ —è–∫–æ—Ä—è
  "count": 10,           // –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–π
  "anchors_count": 5     // –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–∫–æ—Ä–µ–π
}

Previous (v5.4.1):
- –°—É–ø–µ—Ä-–æ—á–∏—Å—Ç–∏—Ç–µ–ª—å strip_geo_to_anchor()
- –Ø–∫–æ—Ä—è —Å–º–µ—à–∏–≤–∞–ª–∏—Å—å —Å –∫–ª—é—á–∞–º–∏
"""



from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from typing import List, Dict
import httpx
import asyncio
import time
import random
import re
import logging
from difflib import SequenceMatcher

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ Pre-filter
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# NLTK –¥–ª—è —Å—Ç–µ–º–º–∏–Ω–≥–∞ (v5.2.0)
import nltk
from nltk.stem import SnowballStemmer

# Natasha –¥–ª—è NER (v5.2.4)
try:
    from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsNERTagger, Doc
    NATASHA_AVAILABLE = True
except ImportError:
    NATASHA_AVAILABLE = False
    print("‚ö†Ô∏è Natasha –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. EntityLogicManager –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å –∂—ë—Å—Ç–∫–∏–º –∫–µ—à–µ–º.")

try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except Exception as e:
    print(f"Warning: Could not download NLTK data: {e}")

# Pymorphy3 –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ RU/UK (v5.2.0)
import pymorphy3

app = FastAPI(
    title="FGS Parser API",
    version="5.4.2",
    description="6 –º–µ—Ç–æ–¥–æ–≤ | 3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞ | Separate anchors output + Frontend support | Level 2"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

# WHITELIST –î–õ–Ø –ü–†–ï-–§–ò–õ–¨–¢–†–ê (v5.3.0)

WHITELIST_TOKENS = {
    "—Ñ–∏–ª–∏–ø—Å", "philips",
    "—Å–∞–º—Å—É–Ω–≥", "samsung",
    "–±–æ—à", "bosch",
    "lg",
    "electrolux", "—ç–ª–µ–∫—Ç—Ä–æ–ª—é–∫—Å",
    "dyson", "–¥–∞–π—Å–æ–Ω",
    "xiaomi", "—Å—è–æ–º–∏",
    "karcher", "–∫–µ—Ä—Ö–µ—Ä",
    "tefal", "—Ç–µ—Ñ–∞–ª—å",
    "rowenta", "—Ä–æ–≤–µ–Ω—Ç–∞",

    "–∂–µ–ª—Ç—ã–µ –≤–æ–¥—ã", "–∂—ë–ª—Ç—ã–µ –≤–æ–¥—ã", "zhovti vody",
    "–Ω–æ–≤–æ–º–æ—Å–∫–æ–≤—Å–∫", "–Ω–æ–≤–æ–º–æ—Å–∫–æ–≤—Å—å–∫",  # –£–∫—Ä–∞–∏–Ω–∞, –ù–ï –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ!
}

# GEO BLACKLIST –î–õ–Ø –ü–†–ï-–§–ò–õ–¨–¢–†–ê (v5.3.0)

MANUAL_RARE_CITIES = {
    "ua": {
        "—â—ë–ª–∫–∏–Ω–æ", "—â–µ–ª–∫ino", "shcholkino",
        "–∞—Ä–º—è–Ω—Å–∫", "–∞—Ä–ºjansk",
        "–∫—Ä–∞—Å–Ω–æ–ø–µ—Ä–µ–∫–æ–ø—Å–∫", "krasnoperekopsk",
        "–¥–∂–∞–Ω–∫–æ–π", "dzhankoi",

        "–∫–æ–º–º—É–Ω–∞—Ä–∫–∞", "kommunarka",
        "–º–æ—Å–∫–æ–≤—Å–∫–∏–π", "moskovskiy",
    },

    "ru": {
        "–∂—ë–ª—Ç—ã–µ –≤–æ–¥—ã", "–∂–µ–ª—Ç—ã–µ –≤–æ–¥—ã", "zhovti vody",
        "–≤–æ–∑–Ω–µ—Å–µ–Ω—Å–∫", "voznesensk",
    },

    "by": set(),

    "kz": set(),
}

def generate_geo_blacklist_full():
    """
    v5.4.0: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ì–µ–æ-–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
    
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç:
    - all_cities_global: {–≥–æ—Ä–æ–¥: –∫–æ–¥_—Å—Ç—Ä–∞–Ω—ã} - –í–°–ï –≥–æ—Ä–æ–¥–∞ –º–∏—Ä–∞
    - –£–±—Ä–∞–Ω—ã —Å—Ç–∞—Ç–∏—á–Ω—ã–µ blacklist –∏ ua_cities
    
    Returns:
        all_cities_global: dict - {–≥–æ—Ä–æ–¥: –∫–æ–¥_—Å—Ç—Ä–∞–Ω—ã}
    """
    try:
        from geonamescache import GeonamesCache

        gc = GeonamesCache()
        cities = gc.get_cities()

        all_cities_global = {}  # {–≥–æ—Ä–æ–¥: –∫–æ–¥_—Å—Ç—Ä–∞–Ω—ã}

        for city_id, city_data in cities.items():
            country = city_data['countrycode'].lower()  # 'RU', 'UA', 'BY' ‚Üí 'ru', 'ua', 'by'

            # –û—Å–Ω–æ–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
            name = city_data['name'].lower()
            all_cities_global[name] = country

            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            for alt in city_data.get('alternatenames', []):
                if ' ' in alt:
                    continue

                if not (3 <= len(alt) <= 30):
                    continue

                if not any(c.isalpha() for c in alt):
                    continue

                alt_clean = alt.replace('-', '').replace("'", "")
                if alt_clean.isalpha():
                    is_latin_cyrillic = all(
                        ('\u0000' <= c <= '\u007F') or  # ASCII (–ª–∞—Ç–∏–Ω–∏—Ü–∞)
                        ('\u0400' <= c <= '\u04FF') or  # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞
                        c in ['-', "'"]
                        for c in alt
                    )

                    if is_latin_cyrillic:
                        alt_lower = alt.lower()
                        # –ï—Å–ª–∏ –≥–æ—Ä–æ–¥ —É–∂–µ –µ—Å—Ç—å, –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ)
                        if alt_lower not in all_cities_global:
                            all_cities_global[alt_lower] = country

        print("‚úÖ v5.4.0: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ì–µ–æ-–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        print(f"   ALL_CITIES_GLOBAL: {len(all_cities_global)} –≥–æ—Ä–æ–¥–æ–≤ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ —Å—Ç—Ä–∞–Ω–∞–º")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
        from collections import Counter
        country_stats = Counter(all_cities_global.values())
        print(f"   –¢–æ–ø-5 —Å—Ç—Ä–∞–Ω: {dict(country_stats.most_common(5))}")

        return all_cities_global

    except ImportError:
        print("‚ö†Ô∏è geonamescache –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å")
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π fallback —Å–ª–æ–≤–∞—Ä—å
        all_cities_global = {
            # –†–æ—Å—Å–∏—è
            '–º–æ—Å–∫–≤–∞': 'ru', '–º—Å–∫': 'ru', '—Å–ø–±': 'ru', '–ø–∏—Ç–µ—Ä': 'ru', 
            '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥': 'ru', '–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': 'ru', '–∫–∞–∑–∞–Ω—å': 'ru',
            '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫': 'ru', '—á–µ–ª—è–±–∏–Ω—Å–∫': 'ru', '–æ–º—Å–∫': 'ru',
            # –ë–µ–ª–∞—Ä—É—Å—å
            '–º–∏–Ω—Å–∫': 'by', '–≥–æ–º–µ–ª—å': 'by', '–≤–∏—Ç–µ–±—Å–∫': 'by', '–º–æ–≥–∏–ª–µ–≤': 'by',
            # –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω
            '–∞–ª–º–∞—Ç—ã': 'kz', '–∞—Å—Ç–∞–Ω–∞': 'kz', '–∫–∞—Ä–∞–≥–∞–Ω–¥–∞': 'kz',
            # –£–∫—Ä–∞–∏–Ω–∞
            '–∫–∏–µ–≤': 'ua', '—Ö–∞—Ä—å–∫–æ–≤': 'ua', '–æ–¥–µ—Å—Å–∞': 'ua', '–¥–Ω–µ–ø—Ä': 'ua',
            '–ª—å–≤–æ–≤': 'ua', '–∑–∞–ø–æ—Ä–æ–∂—å–µ': 'ua', '–∫—Ä–∏–≤–æ–π —Ä–æ–≥': 'ua',
            '–Ω–∏–∫–æ–ª–∞–µ–≤': 'ua', '–≤–∏–Ω–Ω–∏—Ü–∞': 'ua', '—Ö–µ—Ä—Å–æ–Ω': 'ua',
            '–ø–æ–ª—Ç–∞–≤–∞': 'ua', '—á–µ—Ä–Ω–∏–≥–æ–≤': 'ua', '—á–µ—Ä–∫–∞—Å—Å—ã': 'ua',
            '–¥–Ω–µ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å–∫': 'ua', 'kyiv': 'ua', 'kiev': 'ua',
            'kharkiv': 'ua', 'odessa': 'ua', 'lviv': 'ua', 'dnipro': 'ua',
        }
        
        return all_cities_global

ALL_CITIES_GLOBAL = generate_geo_blacklist_full()

class AdaptiveDelay:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""

    def __init__(self, initial_delay: float = 0.2, min_delay: float = 0.1, max_delay: float = 1.0):
        self.delay = initial_delay
        self.min_delay = min_delay
        self.max_delay = max_delay

    def get_delay(self) -> float:
        return self.delay

    def on_success(self):
        self.delay = max(self.min_delay, self.delay * 0.95)

    def on_rate_limit(self):
        self.delay = min(self.max_delay, self.delay * 1.5)

# ENTITY CONFLICT DETECTION (v5.2.4)
class EntityLogicManager:
    """
    """

    def __init__(self):
        self.cache = {}

        # Pymorphy3 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤ (v5.2.5)
        try:
            import pymorphy3
            self.morph_ru = pymorphy3.MorphAnalyzer(lang='ru')
            self.morph_uk = pymorphy3.MorphAnalyzer(lang='uk')
            self.morph_available = True
        except Exception as e:
            print(f"‚ö†Ô∏è Pymorphy3 –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è EntityLogicManager: {e}")
            self.morph_available = False

        self.hard_cache = {
            'LOC': {
                "–∫–∏–µ–≤", "–¥–Ω–µ–ø—Ä", "–¥–Ω–µ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å–∫", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–ª—å–≤–æ–≤", 
                "–∑–∞–ø–æ—Ä–æ–∂—å–µ", "–¥–æ–Ω–µ—Ü–∫", "–∫—Ä–∏–≤–æ–π —Ä–æ–≥", "–Ω–∏–∫–æ–ª–∞–µ–≤", "–ª—É–≥–∞–Ω—Å–∫", 
                "–≤–∏–Ω–Ω–∏—Ü–∞", "—Ö–µ—Ä—Å–æ–Ω", "–ø–æ–ª—Ç–∞–≤–∞", "—á–µ—Ä–Ω–∏–≥–æ–≤", "—á–µ—Ä–∫–∞—Å—Å—ã",
                "–∂–∏—Ç–æ–º–∏—Ä", "—Å—É–º—ã", "—Ö–º–µ–ª—å–Ω–∏—Ü–∫–∏–π", "—Ä–æ–≤–Ω–æ", "–∏–≤–∞–Ω–æ-—Ñ—Ä–∞–Ω–∫–æ–≤—Å–∫",
                "—Ç–µ—Ä–Ω–æ–ø–æ–ª—å", "–ª—É—Ü–∫", "—É–∂–≥–æ—Ä–æ–¥", "—á–µ—Ä–Ω–æ–≤—Ü—ã",
                # –£–∫—Ä–∞–∏–Ω–∞ - –æ–±–ª–∞—Å—Ç–∏ –∏ —Ä–µ–≥–∏–æ–Ω—ã (v5.2.6)
                "–¥–Ω–µ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–¥–Ω–µ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å–∫–∞—è",
                "—Ö–∞—Ä—å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "—Ö–∞—Ä—å–∫–æ–≤—Å–∫–∞—è",
                "–∫–∏–µ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–∫–∏–µ–≤—Å–∫–∞—è",
                "–æ–¥–µ—Å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–æ–¥–µ—Å—Å–∫–∞—è",
                "–ª—å–≤–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ª—å–≤–æ–≤—Å–∫–∞—è",
                "–∑–∞–ø–æ—Ä–æ–∂—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–∑–∞–ø–æ—Ä–æ–∂—Å–∫–∞—è",
                "–¥–æ–Ω–µ—Ü–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–¥–æ–Ω–µ—Ü–∫–∞—è",
                "–ª—É–≥–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ª—É–≥–∞–Ω—Å–∫–∞—è",
                "–∫—Ä—ã–º", "–¥–æ–Ω–±–∞—Å—Å", "–∑–∞–∫–∞—Ä–ø–∞—Ç—å–µ",
                "–º–æ—Å–∫–≤–∞", "—Å–ø–±", "–ø–∏—Ç–µ—Ä", "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", 
                "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–∫–∞–∑–∞–Ω—å", "–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä", "–≤–æ—Ä–æ–Ω–µ–∂", "—Å–∞–º–∞—Ä–∞", 
                "—Ä–æ—Å—Ç–æ–≤", "—É—Ñ–∞", "—á–µ–ª—è–±–∏–Ω—Å–∫", "–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥", "–æ–º—Å–∫",
                "–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫", "–ø–µ—Ä–º—å", "–≤–æ–ª–≥–æ–≥—Ä–∞–¥", "—Å–∞—Ä–∞—Ç–æ–≤", "—Ç—é–º–µ–Ω—å",
                # –†–æ—Å—Å–∏—è - —Ä–µ–≥–∏–æ–Ω—ã (v5.2.6)
                "–º–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–º–æ—Å–∫–æ–≤—Å–∫–∞—è",
                "–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è",
                "–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä—Å–∫–∏–π –∫—Ä–∞–π", "–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä—Å–∫–∏–π",
                "–ø–æ–¥–º–æ—Å–∫–æ–≤—å–µ",
                "–º–∏–Ω—Å–∫", "–≥–æ–º–µ–ª—å", "–º–æ–≥–∏–ª–µ–≤", "–≤–∏—Ç–µ–±—Å–∫", "–≥—Ä–æ–¥–Ω–æ", "–±—Ä–µ—Å—Ç",
                "–∞—Å—Ç–∞–Ω–∞", "–∞–ª–º–∞—Ç—ã", "—Ç–∞—à–∫–µ–Ω—Ç", "—Ç–±–∏–ª–∏—Å–∏", "–µ—Ä–µ–≤–∞–Ω", "–±–∞–∫—É",
                "–∫–∏—à–∏–Ω–µ–≤", "—Ä–∏–≥–∞", "—Ç–∞–ª–ª–∏–Ω", "–≤–∏–ª—å–Ω—é—Å", "–ø—Ä–∞–≥–∞", "–≤–∞—Ä—à–∞–≤–∞",
                "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—è–ª—Ç–∞", "–µ–≤–ø–∞—Ç–æ—Ä–∏—è", "–∫–µ—Ä—á—å", "—Ñ–µ–æ–¥–æ—Å–∏—è",
                "–ª–∏–º–∞—Å—Å–æ–ª", "–Ω–∏–∫–æ—Å–∏—è", "–ª–∞—Ä–Ω–∞–∫–∞", "–ø–∞—Ñ–æ—Å"
            },
            'ORG': {
                "apple", "samsung", "xiaomi", "lg", "sony", "bosch",
                "philips", "panasonic", "nokia", "huawei", "lenovo",
                "dell", "hp", "asus", "acer", "msi", "intel", "amd",
                "dyson", "karcher", "thomas", "electrolux", "siemens",
                "ariston", "indesit", "candy", "zanussi", "beko",
                "gorenje", "whirlpool", "hotpoint", "miele", "aeg"
            }
        }

        self.natasha_available = NATASHA_AVAILABLE
        if self.natasha_available:
            try:
                self.segmenter = Segmenter()
                self.morph_vocab = MorphVocab()
                self.emb = NewsEmbedding()
                self.ner_tagger = NewsNERTagger(self.emb)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Natasha: {e}")
                self.natasha_available = False

    def get_entities(self, text: str, lang: str = 'ru') -> Dict[str, set]:
        """
        Args:
        Returns:
        """
        cache_key = f"{text}_{lang}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        text_lower = text.lower()
        entities = {'LOC': set(), 'ORG': set()}

        words_original = set(re.findall(r'\w+', text_lower))

        for category, items in self.hard_cache.items():
            intersection = words_original & items
            if intersection:
                entities[category].update(intersection)

        if not any(entities.values()) and self.morph_available and lang in ['ru', 'uk']:
            morph = self.morph_ru if lang == 'ru' else self.morph_uk
            words_normalized = set()

            for word in words_original:
                try:
                    parsed = morph.parse(word)
                    if parsed:
                        words_normalized.add(parsed[0].normal_form)
                except:
                    words_normalized.add(word)

            for category, items in self.hard_cache.items():
                intersection = words_normalized & items
                if intersection:
                    entities[category].update(intersection)

        if not any(entities.values()) and lang == 'ru' and self.natasha_available:
            try:
                doc = Doc(text)
                doc.segment(self.segmenter)
                doc.tag_ner(self.ner_tagger)

                for span in doc.spans:
                    if span.type == 'LOC':
                        entities['LOC'].add(span.text.lower())
                    elif span.type == 'ORG':
                        entities['ORG'].add(span.text.lower())
            except Exception as e:
                pass  # NER –º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º —Ç–µ–∫—Å—Ç–µ

        self.cache[cache_key] = entities
        return entities

    def check_conflict(self, seed: str, keyword: str, lang: str = 'ru') -> bool:
        """
        Args:
        Returns:
        """
        seed_entities = self.get_entities(seed, lang)
        kw_entities = self.get_entities(keyword, lang)

        for entity_type in ['LOC', 'ORG']:
            seed_set = seed_entities[entity_type]
            kw_set = kw_entities[entity_type]

            if seed_set and (kw_set - seed_set):
                return True  # –ö–û–ù–§–õ–ò–ö–¢!

        return False  # –ù–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞

class GoogleAutocompleteParser:
    def __init__(self):
        self.adaptive_delay = AdaptiveDelay()

        # ENTITY CONFLICT DETECTION (v5.2.4)
        self.entity_manager = EntityLogicManager()

        # –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–õ–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–ò (v5.2.0)

        self.morph_ru = pymorphy3.MorphAnalyzer(lang='ru')
        self.morph_uk = pymorphy3.MorphAnalyzer(lang='uk')

        self.stemmers = {
            'en': SnowballStemmer("english"),
            'de': SnowballStemmer("german"),
            'fr': SnowballStemmer("french"),
            'es': SnowballStemmer("spanish"),
            'it': SnowballStemmer("italian"),
        }

        self.stop_words = {
            'ru': {'–∏', '–≤', '–≤–æ', '–Ω–µ', '–Ω–∞', '—Å', '–æ—Ç', '–¥–ª—è', '–ø–æ', '–æ', '–æ–±', '–∫', '—É', '–∑–∞', 
                   '–∏–∑', '—Å–æ', '–¥–æ', '–ø—Ä–∏', '–±–µ–∑', '–Ω–∞–¥', '–ø–æ–¥', '–∞', '–Ω–æ', '–¥–∞', '–∏–ª–∏', '—á—Ç–æ–±—ã', 
                   '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '–ø–æ—á–µ–º—É'},
            'uk': {'—ñ', '–≤', '–Ω–∞', '–∑', '–≤—ñ–¥', '–¥–ª—è', '–ø–æ', '–æ', '–¥–æ', '–ø—Ä–∏', '–±–µ–∑', '–Ω–∞–¥', '–ø—ñ–¥', 
                   '–∞', '–∞–ª–µ', '—Ç–∞', '–∞–±–æ', '—â–æ', '—è–∫', '–¥–µ', '–∫–æ–ª–∏', '–∫—É–¥–∏', '–∑–≤—ñ–¥–∫–∏', '—á–æ–º—É'},
            'en': {'a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'o', 'with', 'by', 'from', 
                   'up', 'about', 'into', 'through', 'during', 'and', 'or', 'but', 'i', 'when', 
                   'where', 'how', 'why', 'what'},
            'de': {'der', 'die', 'das', 'den', 'dem', 'des', 'ein', 'eine', 'einen', 'einem', 
                   'und', 'oder', 'aber', 'in', 'au', 'von', 'zu', 'mit', 'f√ºr', 'bei', 'nach',
                   'wie', 'wo', 'wann', 'warum', 'was', 'wer'},
            'fr': {'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'et', 'ou', 'mais', 'dans',
                   'sur', 'avec', 'pour', 'par', '√†', 'en', 'au', 'aux', 'ce', 'qui', 'que',
                   'comment', 'o√π', 'quand', 'pourquoi', 'quoi'},
            'es': {'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'de', 'del', 'y', 'o',
                   'pero', 'en', 'con', 'por', 'para', 'a', 'al', 'como', 'que', 'quien',
                   'donde', 'cuando', 'porque', 'qu√©'},
            'it': {'il', 'lo', 'la', 'i', 'gli', 'le', 'un', 'uno', 'una', 'di', 'da', 'e', 'o',
                   'ma', 'in', 'su', 'con', 'per', 'a', 'come', 'che', 'chi', 'dove', 'quando',
                   'perch√©', 'cosa'},
            'pl': {'i', 'w', 'na', 'z', 'do', 'dla', 'po', 'o', 'przy', 'bez', 'nad', 'pod',
                   'a', 'ale', 'lub', 'czy', '≈ºe', 'jak', 'gdzie', 'kiedy', 'dlaczego', 'co'}
        }

    def is_city_allowed(self, word: str, target_country: str) -> bool:
        """
        v5.4.0: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ä–æ–¥–∞
        
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ª–∏ –≥–æ—Ä–æ–¥ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–µ.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å ALL_CITIES_GLOBAL {–≥–æ—Ä–æ–¥: –∫–æ–¥_—Å—Ç—Ä–∞–Ω—ã}
        
        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            target_country: –ö–æ–¥ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω—ã ('ua', 'ru', 'by', 'kz')
        
        Returns:
            True –µ—Å–ª–∏ –≥–æ—Ä–æ–¥ —Ä–∞–∑—Ä–µ—à—ë–Ω (–ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç target_country –∏–ª–∏ –Ω–µ –≥–æ—Ä–æ–¥)
            False –µ—Å–ª–∏ –≥–æ—Ä–æ–¥ –∑–∞–ø—Ä–µ—â—ë–Ω (–ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –¥—Ä—É–≥–æ–π —Å—Ç—Ä–∞–Ω–µ)
        
        –ü—Ä–∏–º–µ—Ä—ã:
            >>> is_city_allowed('–∫–∏–µ–≤', 'ua')
            True  # –ö–∏–µ–≤ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç UA
            
            >>> is_city_allowed('–º–æ—Å–∫–≤–∞', 'ua')
            False  # –ú–æ—Å–∫–≤–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç RU
            
            >>> is_city_allowed('—Ä–µ–º–æ–Ω—Ç', 'ua')
            True  # "—Ä–µ–º–æ–Ω—Ç" –Ω–µ –≥–æ—Ä–æ–¥ - —Ä–∞–∑—Ä–µ—à–∞–µ–º
        """
        try:
            parsed = self.morph_ru.parse(word.lower())[0]
            lemma = parsed.normal_form
        except:
            lemma = word.lower()
        
        # –ï—Å–ª–∏ —Å–ª–æ–≤–∞ –Ω–µ—Ç –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –±–∞–∑–µ –≥–æ—Ä–æ–¥–æ–≤ ‚Äî –æ–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ
        if lemma not in ALL_CITIES_GLOBAL:
            return True
        
        # –ï—Å–ª–∏ –≥–æ—Ä–æ–¥ –µ—Å—Ç—å –≤ –±–∞–∑–µ, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ–≥–æ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å
        city_country = ALL_CITIES_GLOBAL.get(lemma)  # –ø–æ–ª—É—á–∞–µ–º –∫–æ–¥ —Å—Ç—Ä–∞–Ω—ã (–Ω–∞–ø—Ä. 'ru', 'kz', 'ua')
        
        if city_country == target_country.lower():
            return True  # –ì–æ—Ä–æ–¥ –Ω–∞—à–µ–π —Å—Ç—Ä–∞–Ω—ã ‚Äî —Ä–∞–∑—Ä–µ—à–∞–µ–º
        
        return False  # –ì–æ—Ä–æ–¥ —á—É–∂–æ–π —Å—Ç—Ä–∞–Ω—ã ‚Äî –±–ª–æ–∫–∏—Ä—É–µ–º
    
    def strip_geo_to_anchor(self, text: str, target_country: str) -> str:
        """
        v5.4.0: –°—É–ø–µ—Ä-–æ—á–∏—Å—Ç–∏—Ç–µ–ª—å –≥–µ–æ-–º—É—Å–æ—Ä–∞
        
        –£–¥–∞–ª—è–µ—Ç –∏–∑ —Ñ—Ä–∞–∑—ã –í–°–ï –≥–æ—Ä–æ–¥–∞ –∫—Ä–æ–º–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω—ã.
        –°–æ–∑–¥–∞—ë—Ç "—è–∫–æ—Ä—å" - —á–∏—Å—Ç—É—é —Ñ—Ä–∞–∑—É –±–µ–∑ –≥–µ–æ-–ø—Ä–∏–≤—è–∑–æ–∫.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω–∞—è —Ñ—Ä–∞–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –º–æ—Å–∫–≤–∞")
            target_country: –¶–µ–ª–µ–≤–∞—è —Å—Ç—Ä–∞–Ω–∞ ('ua', 'ru', 'by', 'kz')
        
        Returns:
            –û—á–∏—â–µ–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤")
        
        –ü—Ä–∏–º–µ—Ä—ã:
            >>> strip_geo_to_anchor("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –º–æ—Å–∫–≤–∞", "ua")
            "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"
            
            >>> strip_geo_to_anchor("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –∫–∏–µ–≤", "ua")
            "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –∫–∏–µ–≤"  # –ö–∏–µ–≤ - –Ω–∞—à –≥–æ—Ä–æ–¥, –æ—Å—Ç–∞–≤–ª—è–µ–º
            
            >>> strip_geo_to_anchor("—Ä–µ–º–æ–Ω—Ç –≤ –π–æ—à–∫–∞—Ä-–æ–ª–∞", "ua")
            "—Ä–µ–º–æ–Ω—Ç –≤"  # –ô–æ—à–∫–∞—Ä-–û–ª–∞ —É–¥–∞–ª—ë–Ω —Ü–µ–ª–∏–∫–æ–º (—Å –¥–µ—Ñ–∏—Å–æ–º)
        """
        import re
        
        # –û—á–∏—â–∞–µ–º –æ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ –∏ –¥–µ–ª–∏–º –Ω–∞ —Å–ª–æ–≤–∞ (–≤–∫–ª—é—á–∞—è –¥–µ—Ñ–∏—Å!)
        words = re.findall(r'[–∞-—è—ëa-z0-9-]+', text.lower())
        clean_words = []
        
        for w in words:
            if len(w) < 3:
                clean_words.append(w)
                continue
            
            try:
                parsed = self.morph_ru.parse(w)[0]
                lemma = parsed.normal_form
            except:
                lemma = w
            
            # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –∏–ª–∏ –ª–µ–º–º–∞ –µ—Å—Ç—å –≤ –ì–õ–û–ë–ê–õ–¨–ù–û–ô –±–∞–∑–µ –≥–æ—Ä–æ–¥–æ–≤
            is_city = lemma in ALL_CITIES_GLOBAL or w in ALL_CITIES_GLOBAL
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—à –ª–∏ —ç—Ç–æ –≥–æ—Ä–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, UA)
            is_our_city = False
            if is_city:
                city_info = ALL_CITIES_GLOBAL.get(lemma) or ALL_CITIES_GLOBAL.get(w)
                if city_info == target_country.lower():
                    is_our_city = True
            
            # –ï—Å–ª–∏ —ç—Ç–æ –ß–£–ñ–û–ô –≥–æ—Ä–æ–¥ ‚Äî –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–æ–≤–æ (—É–¥–∞–ª—è–µ–º)
            if is_city and not is_our_city:
                logger.info(f"üßº STRIPPED: '{w}' from '{text}' (city of {ALL_CITIES_GLOBAL.get(lemma) or ALL_CITIES_GLOBAL.get(w)})")
                continue
            
            clean_words.append(w)
        
        cleaned = " ".join(clean_words).strip()
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å
        if cleaned != text.lower():
            logger.warning(f"üßº ANCHOR CREATED: '{text}' ‚Üí '{cleaned}'")
        
        return cleaned

    def detect_seed_language(self, seed: str) -> str:
        """–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ seed"""
        if any('\u0400' <= char <= '\u04FF' for char in seed):
            if any(char in '—ñ—ó—î“ë' for char in seed.lower()):
                return 'uk'
            return 'ru'
        return 'en'

    def get_modifiers(self, language: str, use_numbers: bool, seed: str, cyrillic_only: bool = False) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —è–∑—ã–∫–∞ —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        modifiers = []

        seed_lower = seed.lower()
        has_cyrillic = any('\u0400' <= c <= '\u04FF' for c in seed_lower)
        has_latin = any('a' <= c <= 'z' for c in seed_lower)

        if language.lower() == 'ru':
            modifiers.extend(list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ç—é—è"))
        elif language.lower() == 'uk':
            modifiers.extend(list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—é—è—ñ—ó—î“ë"))

        if not cyrillic_only:
            if has_cyrillic and not has_latin and language.lower() not in ['en', 'de', 'fr', 'es', 'pl']:
                pass
            else:
                modifiers.extend(list("abcdefghijklmnopqrstuvwxyz"))

        if use_numbers:
            modifiers.extend([str(i) for i in range(10)])

        return modifiers

    def get_morphological_forms(self, word: str, language: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ pymorphy3"""
        forms = set([word])

        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                parsed = morph.parse(word)

                if parsed:
                    for form in parsed[0].lexeme:
                        pos = form.tag.POS
                        if pos not in ['PRTS', 'PRTF', 'GRND']:
                            forms.add(form.word)
            except:
                pass

        return sorted(list(forms))

    def _normalize_with_pymorphy(self, text: str, language: str) -> set:
        """
        Args:
        Returns:
        """
        morph = self.morph_ru if language == 'ru' else self.morph_uk

        stop_words = self.stop_words.get(language, self.stop_words['ru'])

        words = re.findall(r'\w+', text.lower())

        meaningful = [w for w in words if w not in stop_words and len(w) > 1]

        lemmas = set()
        for word in meaningful:
            try:
                parsed = morph.parse(word)
                if parsed:
                    lemmas.add(parsed[0].normal_form)
            except:
                lemmas.add(word)

        return lemmas

    def _normalize_with_snowball(self, text: str, language: str) -> set:
        """
        Args:
        Returns:
        """
        stemmer = self.stemmers.get(language, self.stemmers['en'])

        stop_words = self.stop_words.get(language, self.stop_words['en'])

        words = re.findall(r'\w+', text.lower())

        meaningful = [w for w in words if w not in stop_words and len(w) > 1]

        stems = {stemmer.stem(w) for w in meaningful}

        return stems

    def _are_words_similar(self, word1: str, word2: str, threshold: float = 0.85) -> bool:
        """
        Args:
        Returns:
        """
        if len(word1) <= 4 or len(word2) <= 4:
            return False

        similarity = SequenceMatcher(None, word1, word2).ratio()

        return similarity >= threshold

    def _normalize(self, text: str, language: str = 'ru') -> set:
        """
        Args:
        Returns:
        """

        if language in ['ru', 'uk']:
            return self._normalize_with_pymorphy(text, language)

        elif language in ['en', 'de', 'fr', 'es', 'it']:
            return self._normalize_with_snowball(text, language)

        else:
            words = re.findall(r'\w+', text.lower())
            stop_words = self.stop_words.get('en', set())  # fallback –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ
            meaningful = [w for w in words if w not in stop_words and len(w) > 1]
            return set(meaningful)

    def is_grammatically_valid(self, seed_word: str, kw_word: str, language: str = 'ru') -> bool:
        """
        Args:
        Returns:
        """
        if language not in ['ru', 'uk']:
            return True

        try:
            morph = self.morph_ru if language == 'ru' else self.morph_uk

            parsed_seed = morph.parse(seed_word)
            parsed_kw = morph.parse(kw_word)

            if not parsed_seed or not parsed_kw:
                return True  # –ï—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–æ—Å—å - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º

            seed_form = parsed_seed[0]
            kw_form = parsed_kw[0]

            if seed_form.normal_form != kw_form.normal_form:
                return True  # –†–∞–∑–Ω—ã–µ —Å–ª–æ–≤–∞ - –Ω–µ –Ω–∞—à–∞ –ø—Ä–æ–±–ª–µ–º–∞

            invalid_tags = {'datv', 'ablt', 'loct'}

            if 'plur' in kw_form.tag and any(tag in kw_form.tag for tag in invalid_tags):
                return False  # –û—Ç—Å–µ–∏–≤–∞–µ–º –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä!

            return True  # –§–æ—Ä–º–∞ –¥–æ–ø—É—Å—Ç–∏–º–∞—è

        except Exception as e:
            return True

    def is_query_allowed(self, query: str, seed: str, country: str) -> bool:
        """
        –ü—Ä–µ-—Ñ–∏–ª—å—Ç—Ä v5.4.0: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ì–µ–æ-–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç is_city_allowed() –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.
        –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –õ–Æ–ë–û–ô —Å—Ç—Ä–∞–Ω—ã –±–µ–∑ —Ä—É—á–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤.
        """
        import re
        
        q_lower = query.lower().strip()
        
        # 1. Whitelist –±—Ä–µ–Ω–¥–æ–≤ - —Ä–∞–∑—Ä–µ—à–∞–µ–º —Å—Ä–∞–∑—É
        if any(white in q_lower for white in WHITELIST_TOKENS):
            logger.info(f"‚úÖ ALLOWED (whitelist): {query}")
            return True
        
        # 2. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r'[–∞-—è—ëa-z0-9]+', q_lower)
        
        for word in words:
            if len(word) < 3:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ (–ø—Ä–µ–¥–ª–æ–≥–∏)
                continue
            
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ is_city_allowed
            if not self.is_city_allowed(word, country):
                logger.warning(f"üö´ BLOCKED (v5.4.0): {query} | City '{word}' not allowed for {country.upper()}")
                return False
        
        logger.info(f"‚úÖ ALLOWED: {query}")
        return True
    
    def post_filter_cities(self, keywords: set, country: str) -> set:
        """
        POST-FILTER v5.4.0: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —á–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç is_city_allowed() –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –∫–ª—é—á–µ.
        –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –õ–Æ–ë–û–ô —Å—Ç—Ä–∞–Ω—ã.
        
        Args:
            keywords: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –æ—Ç Google
            country: –ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (ua, ru, by, kz)
        
        Returns:
            –û—á–∏—â–µ–Ω–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        """
        import re
        
        cleaned = set()
        removed_count = 0
        
        for keyword in keywords:
            should_remove = False
            kw_lower = keyword.lower()
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
            words = re.findall(r'[–∞-—è—ëa-z0-9]+', kw_lower)
            
            for word in words:
                if len(word) < 3:
                    continue
                
                # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ is_city_allowed
                if not self.is_city_allowed(word, country):
                    logger.info(f"üßπ POST-FILTER removed (v5.4.0): '{keyword}' | City '{word}' not allowed for {country.upper()}")
                    should_remove = True
                    removed_count += 1
                    break
            
            if not should_remove:
                cleaned.add(keyword)
        
        if removed_count > 0:
            logger.warning(f"üßπ POST-FILTER: Removed {removed_count} keywords with non-{country.upper()} cities")
        
        return cleaned

    async def autocorrect_text(self, text: str, language: str) -> Dict:
        """–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ Yandex Speller (ru/uk/en) –∏–ª–∏ LanguageTool (–æ—Å—Ç–∞–ª—å–Ω—ã–µ)"""

        if language.lower() in ['ru', 'uk', 'en']:
            url = "https://speller.yandex.net/services/spellservice.json/checkText"
            lang_map = {'ru': 'ru', 'uk': 'uk', 'en': 'en'}
            yandex_lang = lang_map.get(language.lower(), 'ru')

            params = {"text": text, "lang": yandex_lang, "options": 0}

            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get(url, params=params)

                    if response.status_code == 200:
                        errors = response.json()

                        if not errors:
                            return {"original": text, "corrected": text, "corrections": [], "has_errors": False}

                        corrected = text
                        corrections = []
                        errors_sorted = sorted(errors, key=lambda x: x.get('pos', 0), reverse=True)

                        for error in errors_sorted:
                            word = error.get('word', '')
                            suggestions = error.get('s', [])

                            if suggestions:
                                suggestion = suggestions[0]
                                pos = error.get('pos', 0)
                                corrected = corrected[:pos] + suggestion + corrected[pos + len(word):]
                                corrections.append({"word": word, "suggestion": suggestion})

                        return {
                            "original": text,
                            "corrected": corrected,
                            "corrections": corrections,
                            "has_errors": True
                        }
            except:
                pass

        return await self.autocorrect_languagetool(text, language)

    async def autocorrect_languagetool(self, text: str, language: str) -> Dict:
        """–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ LanguageTool API (30+ —è–∑—ã–∫–æ–≤)"""
        url = "https://api.languagetool.org/v2/check"

        data = {
            "text": text,
            "language": language.lower(),
            "enabledOnly": "false"
        }

        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(url, data=data)

                if response.status_code == 200:
                    result = response.json()
                    matches = result.get('matches', [])

                    if not matches:
                        return {"original": text, "corrected": text, "corrections": [], "has_errors": False}

                    corrected = text
                    corrections = []

                    for match in reversed(matches):
                        offset = match.get('offset', 0)
                        length = match.get('length', 0)
                        replacements = match.get('replacements', [])

                        if replacements:
                            suggestion = replacements[0].get('value', '')
                            word = text[offset:offset+length]
                            corrected = corrected[:offset] + suggestion + corrected[offset+length:]
                            corrections.append({"word": word, "suggestion": suggestion})

                    return {
                        "original": text,
                        "corrected": corrected,
                        "corrections": corrections,
                        "has_errors": True
                    }
        except:
            pass

        return {"original": text, "corrected": text, "corrections": [], "has_errors": False}

    async def filter_infix_results(self, keywords: List[str], language: str) -> List[str]:
        """–§–∏–ª—å—Ç—Ä INFIX —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: —É–±–∏—Ä–∞–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ –æ–¥–∏–Ω–æ—á–Ω—ã–µ –±—É–∫–≤—ã"""

        if language.lower() == 'ru':
            valid = {'–≤', '–Ω–∞', '—É', '–∫', '–æ—Ç', '–∏–∑', '–ø–æ', '–æ', '–æ–±', '—Å', '—Å–æ', '–∑–∞', '–¥–ª—è', '–∏', '–∞', '–Ω–æ'}
        elif language.lower() == 'uk':
            valid = {'–≤', '–Ω–∞', '—É', '–¥–æ', '–≤—ñ–¥', '–∑', '–ø–æ', '–ø—Ä–æ', '–¥–ª—è', '—ñ', '—Ç–∞', '–∞–±–æ'}
        elif language.lower() == 'en':
            valid = {'in', 'on', 'at', 'to', 'from', 'with', 'for', 'by', 'o', 'and', 'or', 'a', 'i'}
        else:
            valid = set()

        filtered = []

        for keyword in keywords:
            keyword_lower = keyword.lower()
            words = keyword_lower.split()

            has_garbage = False
            for i in range(1, len(words)):
                word = words[i]
                if len(word) == 1 and word not in valid:
                    has_garbage = True
                    break

            if not has_garbage:
                filtered.append(keyword)

        return filtered

    async def filter_relevant_keywords(self, keywords: List[str], seed: str, language: str = 'ru') -> List[str]:
        """
        """

        seed_lemmas = self._normalize(seed, language)

        if not seed_lemmas:
            return keywords

        seed_lower = seed.lower()
        seed_words_original = [w.lower() for w in re.findall(r'\w+', seed) if len(w) > 2]

        stop_words = self.stop_words.get(language, self.stop_words['ru'])

        seed_important_words = [w for w in seed_words_original if w not in stop_words]

        if not seed_important_words:
            seed_important_words = seed_words_original

        filtered = []

        for keyword in keywords:
            kw_lower = keyword.lower()

            kw_lemmas = self._normalize(keyword, language)
            if not seed_lemmas.issubset(kw_lemmas):
                continue  # –ù–µ –ø—Ä–æ —Ç–æ - –æ—Ç—Å–µ–∏–≤–∞–µ–º

            kw_words = kw_lower.split()
            matches = 0
            grammatically_valid = True

            for seed_word in seed_important_words:
                found_match = False

                for kw_word in kw_words:
                    if seed_word in kw_word:
                        # –ü–†–û–í–ï–†–ö–ê 2.5: –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å (v5.2.3 - Gemini)
                        if self.is_grammatically_valid(seed_word, kw_word, language):
                            found_match = True
                            break
                        else:
                            grammatically_valid = False
                            break

                if found_match:
                    matches += 1

            if not grammatically_valid:
                continue

            if len(seed_important_words) > 0:
                match_ratio = matches / len(seed_important_words)
                if match_ratio < 1.0:  # –ï—Å–ª–∏ –ù–ï 100% - –æ—Ç—Å–µ–∏–≤–∞–µ–º
                    continue

            first_seed_word = seed_important_words[0]
            first_word_position = -1

            for i, kw_word in enumerate(kw_words):
                if first_seed_word in kw_word:
                    first_word_position = i
                    break

            if first_word_position > 1:
                continue  # "–¥–æ–º—É —Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤" ‚Üí –ø–æ–∑–∏—Ü–∏—è 1, –Ω–æ —ç—Ç–æ —É–∂–µ –º—É—Å–æ—Ä

            last_index = -1
            order_correct = True

            for seed_word in seed_important_words:
                found_at = -1
                for i, kw_word in enumerate(kw_words):
                    if i > last_index and seed_word in kw_word:
                        found_at = i
                        break

                if found_at == -1:
                    order_correct = False
                    break

                last_index = found_at

            if order_correct:
                filtered.append(keyword)

        # –≠–¢–ê–ü 2: ENTITY CONFLICTS (v5.2.4)
        filtered_final = []

        for keyword in filtered:
            is_conflict = await asyncio.to_thread(
                self.entity_manager.check_conflict,
                seed,
                keyword,
                language
            )

            if not is_conflict:
                filtered_final.append(keyword)

        return filtered_final

    async def fetch_suggestions(self, query: str, country: str, language: str, client: httpx.AsyncClient) -> List[str]:
        """Google Autocomplete"""
        url = "https://www.google.com/complete/search"
        params = {"q": query, "client": "firefox", "hl": language, "gl": country}
        headers = {"User-Agent": random.choice(USER_AGENTS)}

        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)

            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []

            self.adaptive_delay.on_success()

            if response.status_code == 200:
                data = response.json()
                return data[1] if len(data) > 1 else []
        except:
            pass

        return []

    async def fetch_suggestions_yandex(self, query: str, language: str, region_id: int, client: httpx.AsyncClient) -> List[str]:
        """Yandex Suggest"""
        url = "https://suggest-maps.yandex.ru/suggest-geo"

        params = {
            "v": "9",
            "search_type": "tp",
            "part": query,
            "lang": language,
            "n": "10",
            "geo": str(region_id),
            "fullpath": "1"
        }

        headers = {"User-Agent": random.choice(USER_AGENTS)}

        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)

            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []

            self.adaptive_delay.on_success()

            if response.status_code == 200:
                data = response.json()
                results = data.get('results', [])
                return [item.get('text', '') for item in results if item.get('text')]
        except:
            pass

        return []

    async def fetch_suggestions_bing(self, query: str, language: str, country: str, client: httpx.AsyncClient) -> List[str]:
        """Bing Autosuggest"""
        url = "https://www.bing.com/AS/Suggestions"

        params = {
            "q": query,
            "mkt": f"{language}-{country}",
            "cvid": "0",
            "qry": query
        }

        headers = {"User-Agent": random.choice(USER_AGENTS)}

        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)

            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []

            self.adaptive_delay.on_success()

            if response.status_code == 200:
                data = response.json()
                suggestion_groups = data.get('AS', {}).get('Results', [])

                suggestions = []
                for group in suggestion_groups:
                    for item in group.get('Suggests', []):
                        text = item.get('Txt', '')
                        if text:
                            suggestions.append(text)

                return suggestions
        except:
            pass

        return []

    async def parse_with_semaphore(self, queries: List[str], country: str, language: str, 
                                   parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""

        semaphore = asyncio.Semaphore(parallel_limit)
        all_keywords = set()
        success_count = 0
        failed_count = 0

        async def fetch_with_limit(query: str, client: httpx.AsyncClient):
            nonlocal success_count, failed_count

            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())

                if source == "google":
                    results = await self.fetch_suggestions(query, country, language, client)
                elif source == "yandex":
                    results = await self.fetch_suggestions_yandex(query, language, region_id, client)
                elif source == "bing":
                    results = await self.fetch_suggestions_bing(query, language, country, client)
                else:
                    results = []

                if results:
                    all_keywords.update(results)
                    success_count += 1
                else:
                    failed_count += 1

                return results

        async with httpx.AsyncClient() as client:
            tasks = [fetch_with_limit(q, client) for q in queries]
            await asyncio.gather(*tasks)

        return {
            "keywords": sorted(list(all_keywords)),
            "success": success_count,
            "failed": failed_count
        }

    async def parse_suffix(self, seed: str, country: str, language: str, use_numbers: bool, 
                          parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """SUFFIX –º–µ—Ç–æ–¥: seed + –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä"""
        start_time = time.time()

        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]

        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)

        # POST-FILTER: –ß–∏—Å—Ç–∫–∞ –æ—Ç –Ω–µ—Ü–µ–ª–µ–≤—ã—Ö –≥–æ—Ä–æ–¥–æ–≤
        cleaned_keywords = self.post_filter_cities(set(result_raw['keywords']), country)
        
        # SUPER-CLEANER v5.4.1: –°–æ–∑–¥–∞—ë–º —è–∫–æ—Ä—è
        anchors_created = set()
        for keyword in cleaned_keywords:
            anchor = self.strip_geo_to_anchor(keyword, country)
            if anchor and len(anchor) > 5 and anchor != keyword.lower():
                anchors_created.add(anchor)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        all_with_anchors = cleaned_keywords | anchors_created
        
        # –§–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (v5.2.0: subset matching)
        filtered = await self.filter_relevant_keywords(list(all_with_anchors), seed, language)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
        filtered_set = set(filtered)
        final_keywords = sorted(list(cleaned_keywords & filtered_set))
        final_anchors = sorted(list(anchors_created & filtered_set))

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "suffix",
            "source": source,
            "keywords": final_keywords,
            "anchors": final_anchors,
            "count": len(final_keywords),
            "anchors_count": len(final_anchors),
            "queries": len(queries),
            "elapsed_time": round(elapsed, 2)
        }

    async def parse_infix(self, seed: str, country: str, language: str, use_numbers: bool, 
                         parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """INFIX –º–µ—Ç–æ–¥: –≤—Å—Ç–∞–≤–∫–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏"""
        start_time = time.time()

        words = seed.strip().split()

        if len(words) < 2:
            return {"error": "INFIX —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞", "seed": seed}

        modifiers = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
        queries = []

        for i in range(1, len(words)):
            for mod in modifiers:
                query = ' '.join(words[:i]) + f' {mod} ' + ' '.join(words[i:])
                queries.append(query)

        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)

        # POST-FILTER: –ß–∏—Å—Ç–∫–∞ –æ—Ç –Ω–µ—Ü–µ–ª–µ–≤—ã—Ö –≥–æ—Ä–æ–¥–æ–≤
        cleaned_keywords = self.post_filter_cities(set(result_raw['keywords']), country)
        
        # SUPER-CLEANER v5.4.1: –°–æ–∑–¥–∞—ë–º —è–∫–æ—Ä—è
        anchors_created = set()
        for keyword in cleaned_keywords:
            anchor = self.strip_geo_to_anchor(keyword, country)
            if anchor and len(anchor) > 5 and anchor != keyword.lower():
                anchors_created.add(anchor)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        all_with_anchors = cleaned_keywords | anchors_created
        
        filtered_1 = await self.filter_infix_results(list(all_with_anchors), language)

        # –§–∏–ª—å—Ç—Ä 2: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (v5.2.0: subset matching)
        filtered_2 = await self.filter_relevant_keywords(filtered_1, seed, language)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
        filtered_set = set(filtered_2)
        final_keywords = sorted(list(cleaned_keywords & filtered_set))
        final_anchors = sorted(list(anchors_created & filtered_set))

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "infix",
            "source": source,
            "keywords": final_keywords,
            "anchors": final_anchors,
            "count": len(final_keywords),
            "anchors_count": len(final_anchors),
            "queries": len(queries),
            "elapsed_time": round(elapsed, 2)
        }

    async def parse_morphology(self, seed: str, country: str, language: str, use_numbers: bool, 
                               parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """MORPHOLOGY –º–µ—Ç–æ–¥: –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–æ—Ä–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö"""
        start_time = time.time()

        words = seed.strip().split()

        nouns_to_modify = []

        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                for idx, word in enumerate(words):
                    parsed = morph.parse(word)
                    if parsed and parsed[0].tag.POS == 'NOUN':
                        nouns_to_modify.append({
                            'index': idx,
                            'word': word,
                            'forms': self.get_morphological_forms(word, language)
                        })

                if not nouns_to_modify:
                    last_word = words[-1]
                    nouns_to_modify.append({
                        'index': len(words) - 1,
                        'word': last_word,
                        'forms': self.get_morphological_forms(last_word, language)
                    })
            except:
                last_word = words[-1]
                nouns_to_modify.append({
                    'index': len(words) - 1,
                    'word': last_word,
                    'forms': self.get_morphological_forms(last_word, language)
                })
        else:
            last_word = words[-1]
            nouns_to_modify.append({
                'index': len(words) - 1,
                'word': last_word,
                'forms': self.get_morphological_forms(last_word, language)
            })

        all_seeds = []
        if len(nouns_to_modify) >= 1:
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))

        unique_seeds = list(set(all_seeds))

        all_keywords = set()
        modifiers = self.get_modifiers(language, use_numbers, seed)

        for seed_variant in unique_seeds:
            queries = [f"{seed_variant} {mod}" for mod in modifiers]
            result = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)
            all_keywords.update(result['keywords'])

        # POST-FILTER: –ß–∏—Å—Ç–∫–∞ –æ—Ç –Ω–µ—Ü–µ–ª–µ–≤—ã—Ö –≥–æ—Ä–æ–¥–æ–≤
        all_keywords = self.post_filter_cities(all_keywords, country)
        
        # SUPER-CLEANER v5.4.1: –°–æ–∑–¥–∞—ë–º —è–∫–æ—Ä—è
        anchors_created = set()
        for keyword in all_keywords:
            anchor = self.strip_geo_to_anchor(keyword, country)
            if anchor and len(anchor) > 5 and anchor != keyword.lower():
                anchors_created.add(anchor)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        all_with_anchors = all_keywords | anchors_created
        
        filtered = await self.filter_relevant_keywords(sorted(list(all_with_anchors)), seed, language)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
        filtered_set = set(filtered)
        final_keywords = sorted(list(all_keywords & filtered_set))
        final_anchors = sorted(list(anchors_created & filtered_set))

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "morphology",
            "source": source,
            "keywords": final_keywords,
            "anchors": final_anchors,
            "count": len(final_keywords),
            "anchors_count": len(final_anchors),
            "elapsed_time": round(elapsed, 2)
        }

    async def parse_light_search(self, seed: str, country: str, language: str, use_numbers: bool, 
                                 parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """LIGHT SEARCH: –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ (SUFFIX + INFIX)"""
        start_time = time.time()

        suffix_result = await self.parse_suffix(seed, country, language, use_numbers, parallel_limit, source, region_id)
        infix_result = await self.parse_infix(seed, country, language, use_numbers, parallel_limit, source, region_id)

        all_keywords = set(suffix_result["keywords"]) | set(infix_result.get("keywords", []))

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "light_search",
            "source": source,
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "suffix_count": len(suffix_result["keywords"]),
            "infix_count": len(infix_result.get("keywords", [])),
            "elapsed_time": round(elapsed, 2)
        }

    async def parse_adaptive_prefix(self, seed: str, country: str, language: str, use_numbers: bool, 
                                    parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """ADAPTIVE PREFIX –º–µ—Ç–æ–¥: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ –∏–∑ SUFFIX + PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞"""
        start_time = time.time()

        seed_words = set(seed.lower().split())

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        prefixes = ["", "–∫—É–ø–∏—Ç—å", "—Ü–µ–Ω–∞", "–æ—Ç–∑—ã–≤—ã"]
        queries = []
        for p in prefixes:
            q = f"{p} {seed}".strip()
            if self.is_query_allowed(q, seed, country):
                queries.append(q)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–ª—Ñ–∞–≤–∏—Ç–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        alphabet = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
        for char in alphabet:
            q_ext = f"{seed} {char}".strip()
            if self.is_query_allowed(q_ext, seed, country):
                queries.append(q_ext)

        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)

        from collections import Counter
        word_counter = Counter()

        for result in result_raw['keywords']:
            result_words = result.lower().split()
            for word in result_words:
                if word not in seed_words and len(word) > 2:
                    word_counter[word] += 1

        candidates = {w for w, count in word_counter.items() if count >= 2}

        all_keywords = set()
        verified_prefixes = []

        for candidate in sorted(candidates):
            query = f"{candidate} {seed}"

            if not self.is_query_allowed(query, seed, country):
                continue

            result = await self.parse_with_semaphore([query], country, language, parallel_limit, source, region_id)
            if result['keywords']:
                all_keywords.update(result['keywords'])
                verified_prefixes.append(candidate)

        # POST-FILTER: –ß–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç –Ω–µ—Ü–µ–ª–µ–≤—ã—Ö –≥–æ—Ä–æ–¥–æ–≤
        cleaned_keywords = self.post_filter_cities(all_keywords, country)
        
        # SUPER-CLEANER v5.4.1: –°–æ–∑–¥–∞—ë–º —è–∫–æ—Ä—è
        anchors_created = set()
        for keyword in cleaned_keywords:
            anchor = self.strip_geo_to_anchor(keyword, country)
            if anchor and len(anchor) > 5 and anchor != keyword.lower():
                anchors_created.add(anchor)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        all_with_anchors = cleaned_keywords | anchors_created
        
        filtered = await self.filter_relevant_keywords(sorted(list(all_with_anchors)), seed, language)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
        filtered_set = set(filtered)
        final_keywords = sorted(list(cleaned_keywords & filtered_set))
        final_anchors = sorted(list(anchors_created & filtered_set))

        elapsed = time.time() - start_time

        return {
            "seed": seed,
            "method": "adaptive_prefix",
            "source": source,
            "keywords": final_keywords,
            "anchors": final_anchors,
            "count": len(final_keywords),
            "anchors_count": len(final_anchors),
            "candidates_found": len(candidates),
            "verified_prefixes": verified_prefixes,
            "elapsed_time": round(elapsed, 2)
        }

    async def parse_deep_search(self, seed: str, country: str, region_id: int, language: str, 
                                use_numbers: bool, parallel_limit: int, include_keywords: bool, 
                                source: str = "google") -> Dict:
        """DEEP SEARCH: –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ (–≤—Å–µ 4 –º–µ—Ç–æ–¥–∞)"""

        correction = await self.autocorrect_text(seed, language)
        original_seed = seed

        if correction.get("has_errors"):
            seed = correction["corrected"]

        start_time = time.time()

        suffix_result = await self.parse_suffix(seed, country, language, use_numbers, parallel_limit, source, region_id)
        infix_result = await self.parse_infix(seed, country, language, use_numbers, parallel_limit, source, region_id)
        morph_result = await self.parse_morphology(seed, country, language, use_numbers, parallel_limit, source, region_id)
        prefix_result = await self.parse_adaptive_prefix(seed, country, language, use_numbers, parallel_limit, source, region_id)

        suffix_kw = set(suffix_result["keywords"])
        infix_kw = set(infix_result.get("keywords", []))
        morph_kw = set(morph_result["keywords"])
        prefix_kw = set(prefix_result["keywords"])
        
        # –°–æ–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è
        suffix_anchors = set(suffix_result.get("anchors", []))
        infix_anchors = set(infix_result.get("anchors", []))
        morph_anchors = set(morph_result.get("anchors", []))
        prefix_anchors = set(prefix_result.get("anchors", []))

        all_unique = suffix_kw | infix_kw | morph_kw | prefix_kw
        all_anchors = suffix_anchors | infix_anchors | morph_anchors | prefix_anchors

        elapsed = time.time() - start_time

        response = {
            "seed": original_seed,
            "corrected_seed": seed if correction.get("has_errors") else None,
            "corrections": correction.get("corrections", []) if correction.get("has_errors") else [],
            "source": source,
            "total_unique_keywords": len(all_unique),
            "total_anchors": len(all_anchors),
            "methods": {
                "suffix": {"count": len(suffix_kw), "anchors_count": len(suffix_anchors)},
                "infix": {"count": len(infix_kw), "anchors_count": len(infix_anchors)},
                "morphology": {"count": len(morph_kw), "anchors_count": len(morph_anchors)},
                "adaptive_prefix": {"count": len(prefix_kw), "anchors_count": len(prefix_anchors)}
            },
            "elapsed_time": round(elapsed, 2)
        }

        if include_keywords:
            response["keywords"] = {
                "all": sorted(list(all_unique)),
                "suffix": sorted(list(suffix_kw)),
                "infix": sorted(list(infix_kw)),
                "morphology": sorted(list(morph_kw)),
                "adaptive_prefix": sorted(list(prefix_kw))
            }
            response["anchors"] = {
                "all": sorted(list(all_anchors)),
                "suffix": sorted(list(suffix_anchors)),
                "infix": sorted(list(infix_anchors)),
                "morphology": sorted(list(morph_anchors)),
                "adaptive_prefix": sorted(list(prefix_anchors))
            }

        return response

parser = GoogleAutocompleteParser()

@app.get("/")
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return FileResponse('static/index.html')

@app.get("/api/light-search")
async def light_search_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """LIGHT SEARCH: –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ (SUFFIX + INFIX)"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_light_search(seed, country, language, use_numbers, parallel_limit, source, region_id)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return result

@app.get("/api/deep-search")
async def deep_search_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (ua/us/de...)"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex (143=–ö–∏–µ–≤)"),
    language: str = Query("auto", description="–Ø–∑—ã–∫ (auto/ru/uk/en)"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    include_keywords: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–π"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """DEEP SEARCH: –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ (–≤—Å–µ 4 –º–µ—Ç–æ–¥–∞)"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    return await parser.parse_deep_search(seed, country, region_id, language, use_numbers, parallel_limit, include_keywords, source)

@app.get("/api/compare")
async def compare_methods(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (ua/us/de...)"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex (143=–ö–∏–µ–≤)"),
    language: str = Query("auto", description="–Ø–∑—ã–∫ (auto/ru/uk/en)"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    include_keywords: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–π"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """[DEPRECATED] –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /api/deep-search"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    return await parser.parse_deep_search(seed, country, region_id, language, use_numbers, parallel_limit, include_keywords, source)

@app.get("/api/parse/suffix")
async def parse_suffix_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """–¢–æ–ª—å–∫–æ SUFFIX –º–µ—Ç–æ–¥"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_suffix(seed, country, language, use_numbers, parallel_limit, source, region_id)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return result

@app.get("/api/parse/infix")
async def parse_infix_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞)"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """–¢–æ–ª—å–∫–æ INFIX –º–µ—Ç–æ–¥"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_infix(seed, country, language, use_numbers, parallel_limit, source, region_id)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return result

@app.get("/api/parse/morphology")
async def parse_morphology_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """–¢–æ–ª—å–∫–æ MORPHOLOGY –º–µ—Ç–æ–¥"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_morphology(seed, country, language, use_numbers, parallel_limit, source, region_id)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return result

@app.get("/api/parse/adaptive-prefix")
async def parse_adaptive_prefix_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """ADAPTIVE PREFIX –º–µ—Ç–æ–¥ (–Ω–∞—Ö–æ–¥–∏—Ç PREFIX –∑–∞–ø—Ä–æ—Å—ã —Ç–∏–ø–∞ '–∫–∏–µ–≤ —Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤')"""

    if language == "auto":
        language = parser.detect_seed_language(seed)

    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]

    result = await parser.parse_adaptive_prefix(seed, country, language, use_numbers, parallel_limit, source, region_id)

    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])

    return result

