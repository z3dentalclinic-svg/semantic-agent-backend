"""
GOOGLE AUTOCOMPLETE PARSER - OPTIMIZED VERSION
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é

Version: 3.6 Clean
–í—Ä–µ–º—è: ~2 —Å–µ–∫ –Ω–∞ 56 –∑–∞–ø—Ä–æ—Å–æ–≤ (17√ó –±—ã—Å—Ç—Ä–µ–µ –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏!)

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
- Connection Pooling (–ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ HTTP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π)
- Adaptive Delay (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫)
- Parallel Requests (5 –ø–æ—Ç–æ–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
- Smart Filtering (—É–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤)
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Dict
import httpx
import asyncio
import time
import random

# ============================================
# FASTAPI APP
# ============================================
app = FastAPI(
    title="Google Autocomplete Parser - Optimized", 
    version="3.6",
    description="–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ (17√ó –±—ã—Å—Ç—Ä–µ–µ!)"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# USER AGENTS
# ============================================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

# ============================================
# ADAPTIVE DELAY
# ============================================
class AdaptiveDelay:
    """
    –£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π
    - –ü—Ä–∏ —É—Å–ø–µ—Ö–µ ‚Üí —É–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É (—É—Å–∫–æ—Ä—è–µ–º—Å—è)
    - –ü—Ä–∏ 429 ‚Üí —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É (–∑–∞—â–∏—Ç–∞ –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
    """
    
    def __init__(self, initial_delay=0.2, min_delay=0.1, max_delay=1.0):
        self.current_delay = initial_delay
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.decrease_factor = 0.95  # –£–º–µ–Ω—å—à–∞–µ–º –Ω–∞ 5% –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
        self.increase_factor = 2.0   # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤ 2 —Ä–∞–∑–∞ –ø—Ä–∏ 429
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_requests = 0
        self.successful_requests = 0
        self.rate_limit_hits = 0
    
    async def wait(self):
        """–ñ–¥—ë–º —Ç–µ–∫—É—â—É—é –∑–∞–¥–µ—Ä–∂–∫—É"""
        await asyncio.sleep(self.current_delay)
    
    def record_success(self):
        """–£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚Üí —É–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É"""
        self.total_requests += 1
        self.successful_requests += 1
        self.current_delay = max(self.min_delay, self.current_delay * self.decrease_factor)
    
    def record_rate_limit(self):
        """Rate limit ‚Üí —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É"""
        self.total_requests += 1
        self.rate_limit_hits += 1
        self.current_delay = min(self.max_delay, self.current_delay * self.increase_factor)
        print(f"üî¥ Rate limit! –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–æ {self.current_delay:.3f} —Å–µ–∫")
    
    def record_error(self):
        """–î—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞"""
        self.total_requests += 1
    
    def get_stats(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        avg_delay = self.current_delay
        return {
            "final_delay": round(self.current_delay, 3),
            "rate_limit_hits": self.rate_limit_hits,
            "success_rate": round(self.successful_requests / self.total_requests * 100, 1) if self.total_requests > 0 else 0
        }

# ============================================
# KEYWORD PARSER (SUFFIX + INFIX)
# ============================================
class KeywordParser:
    def __init__(self):
        self.base_url = "https://suggestqueries.google.com/complete/search"
        self.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
        
        # –Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã)
        self.language_modifiers = {
            'en': [],
            'ru': list("–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"),  # –ü–æ–ª–Ω—ã–π —Ä—É—Å—Å–∫–∏–π –∞–ª—Ñ–∞–≤–∏—Ç!
            'uk': list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—å—é—è—ñ—ó—î“ë"),
            'de': list("√§√∂√º√ü"),
            'fr': list("√†√¢√§√¶√ß√©√®√™√´√Ø√Æ√¥√π√ª√º√ø"),
            'es': list("√°√©√≠√±√≥√∫√º"),
            'pl': list("ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º"),
            'it': list("√†√®√©√¨√≠√Æ√≤√≥√π√∫"),
        }
        
        # –†–µ–¥–∫–∏–µ –±—É–∫–≤—ã (–º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å)
        self.rare_chars = {
            'ru': ['—ä', '—ë', '—ã'],  # –£–±—Ä–∞–ª '—å' —á—Ç–æ–±—ã –±—ã–ª–æ 56 –∫–∞–∫ –≤—á–µ—Ä–∞
            'uk': ['—ä'],  # –£–±—Ä–∞–ª '—å' —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
            'pl': ['ƒÖ', 'ƒô'],
        }
    
    def detect_seed_language(self, seed: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ seed (latin/cyrillic)"""
        has_latin = any(ord('a') <= ord(c.lower()) <= ord('z') for c in seed if c.isalpha())
        has_cyrillic = any(ord('–∞') <= ord(c.lower()) <= ord('—è') for c in seed if c.isalpha())
        
        if has_cyrillic:
            return 'cyrillic'
        return 'latin'
    
    def get_modifiers(self, language: str, use_numbers: bool, seed: str, cyrillic_only: bool = False) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —É–º–Ω–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        
        –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –î–õ–Ø –ë–†–ï–ù–î–û–í:
        - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π seed ‚Üí —É–±–∏—Ä–∞–µ–º –≤—Å—ë –∫—Ä–æ–º–µ a-z (–Ω–µ—Ç –±—Ä–µ–Ω–¥–æ–≤ –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ)
        - –î—Ä—É–≥–∏–µ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ seed ‚Üí —É–±–∏—Ä–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É (–Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã —è–∑—ã–∫–∞)
        - –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–π seed ‚Üí –û–°–¢–ê–í–õ–Ø–ï–ú –í–°–Å (–ª–∞—Ç–∏–Ω–∏—Ü—É –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤ + –∫–∏—Ä–∏–ª–ª–∏—Ü—É!)
        
        cyrillic_only: –¢–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (–¥–ª—è INFIX - –ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞)
        """
        seed_lang = self.detect_seed_language(seed)
        base_latin = list("abcdefghijklmnopqrstuvwxyz")
        numbers = list("0123456789") if use_numbers else []
        lang_specific = self.language_modifiers.get(language.lower(), [])
        
        # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø
        if language.lower() == 'en' and seed_lang == 'latin':
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π seed ‚Üí —Ç–æ–ª—å–∫–æ a-z + —Ü–∏—Ñ—Ä—ã
            modifiers = base_latin + numbers
            
        elif seed_lang == 'latin':
            # –õ–∞—Ç–∏–Ω—Å–∫–∏–π seed (–Ω–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π) ‚Üí —É–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û –∫–∏—Ä–∏–ª–ª–∏—Ü—É
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û']
            non_cyrillic = [m for m in lang_specific if not is_cyrillic(m)]
            modifiers = base_latin + non_cyrillic + numbers
            
        else:
            # –ö–ò–†–ò–õ–õ–ò–ß–ï–°–ö–ò–ô seed ‚Üí –æ—Å—Ç–∞–≤–ª—è–µ–º –í–°–Å (–ª–∞—Ç–∏–Ω–∏—Ü—É –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤!)
            modifiers = base_latin + lang_specific + numbers
        
        # –£–±–∏—Ä–∞–µ–º —Ä–µ–¥–∫–∏–µ –±—É–∫–≤—ã
        rare = self.rare_chars.get(language.lower(), [])
        if rare:
            modifiers = [m for m in modifiers if m not in rare]
        
        # –¢–û–õ–¨–ö–û –ö–ò–†–ò–õ–õ–ò–¶–ê (–¥–ª—è INFIX)
        if cyrillic_only and seed_lang == 'cyrillic':
            # –£–±–∏—Ä–∞–µ–º –ª–∞—Ç–∏–Ω–∏—Ü—É –∏ —Ü–∏—Ñ—Ä—ã
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û', '—å']
            modifiers = [m for m in modifiers if is_cyrillic(m)]
        
        return modifiers
    
    def get_morphological_forms(self, word: str, language: str) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏:
        - RU, UK: pymorphy3 (–≤—Å–µ —Ñ–æ—Ä–º—ã –∏–∑ –ª–µ–∫—Å–µ–º—ã)
        - EN: lemminflect (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ/–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ + –ø—Ä–∏—Ç—è–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã)
        - –î—Ä—É–≥–∏–µ: –±–∞–∑–æ–≤–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (–ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞)
        """
        forms = set([word])  # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ñ–æ—Ä–º—É
        
        if language.lower() in ['ru', 'uk']:
            # –†—É—Å—Å–∫–∏–π –∏ –£–∫—Ä–∞–∏–Ω—Å–∫–∏–π - pymorphy3
            try:
                import pymorphy3
                
                # –°–æ–∑–¥–∞—ë–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
                morph = pymorphy3.MorphAnalyzer()
                
                # –ü–∞—Ä—Å–∏–º —Å–ª–æ–≤–æ
                parsed = morph.parse(word)
                
                if parsed:
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Ä–∞–∑–±–æ—Ä–∞
                    p = parsed[0]
                    
                    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –∏–∑ –ª–µ–∫—Å–µ–º—ã
                    for form in p.lexeme:
                        forms.add(form.word)
                
                print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (RU/UK): '{word}' ‚Üí {len(forms)} —Ñ–æ—Ä–º —á–µ—Ä–µ–∑ pymorphy3")
                
            except ImportError:
                print(f"‚ö†Ô∏è pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é —Ñ–æ—Ä–º—É.")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ pymorphy3: {e}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º–æ–π
        
        elif language.lower() == 'en':
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π - lemminflect –∏–ª–∏ –±–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞
            try:
                import lemminflect
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º—ã —á–µ—Ä–µ–∑ lemminflect
                # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ
                plurals = lemminflect.getAllInflections(word, upos='NOUN')
                if plurals and 'NNS' in plurals:
                    forms.update(plurals['NNS'])
                
                # –ü—Ä–∏—Ç—è–∂–∞—Ç–µ–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞
                if not word.endswith('s'):
                    forms.add(word + "'s")
                    forms.add(word + "s")
                
                print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (EN): '{word}' ‚Üí {len(forms)} —Ñ–æ—Ä–º (lemminflect)")
                
            except ImportError:
                # Fallback - –±–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞
                forms.add(word)  # singular
                
                # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ (–ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞)
                if word.endswith('y') and len(word) > 1 and word[-2] not in 'aeiou':
                    plural = word[:-1] + 'ies'  # baby ‚Üí babies
                elif word.endswith(('s', 'x', 'z', 'ch', 'sh')):
                    plural = word + 'es'  # box ‚Üí boxes
                elif word.endswith('o') and len(word) > 1 and word[-2] not in 'aeiou':
                    plural = word + 'es'  # hero ‚Üí heroes
                elif word.endswith('f'):
                    plural = word[:-1] + 'ves'  # leaf ‚Üí leaves
                elif word.endswith('fe'):
                    plural = word[:-2] + 'ves'  # knife ‚Üí knives
                else:
                    plural = word + 's'  # regular
                
                forms.add(plural)
                
                # –ü—Ä–∏—Ç—è–∂–∞—Ç–µ–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞
                if not word.endswith('s'):
                    forms.add(word + "'s")
                
                print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (EN): '{word}' ‚Üí {len(forms)} —Ñ–æ—Ä–º (–±–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞)")
        
        else:
            # –î—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ - —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞
            print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è ({language.upper()}): '{word}' ‚Üí 1 —Ñ–æ—Ä–º–∞ (–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)")
        
        return sorted(list(forms))
    
    async def parse_single_morphological_form(self, form: str, prefix: str, language: str, use_numbers: bool, country: str, parallel_limit: int) -> dict:
        """
        –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ø–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—ã
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ parse_morphology –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Ñ–æ—Ä–º
        """
        # –°–æ–∑–¥–∞—ë–º seed –¥–ª—è —ç—Ç–æ–π —Ñ–æ—Ä–º—ã
        if prefix:
            current_seed = f"{prefix} {form}"
        else:
            current_seed = form
        
        print(f"üìñ –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–æ—Ä–º—ã: '{form}' (seed: '{current_seed}')")
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        modifiers = self.get_modifiers(language, use_numbers, current_seed)
        
        # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì —Å Connection Pooling
        semaphore = asyncio.Semaphore(parallel_limit)
        form_keywords = set()
        
        async with httpx.AsyncClient(timeout=10.0) as shared_client:
            async def fetch_limited(modifier):
                async with semaphore:
                    return await self.fetch_with_delay(modifier, current_seed, country, language, shared_client)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = [fetch_limited(m) for m in modifiers]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        successful = 0
        failed = 0
        
        for result in results:
            if isinstance(result, Exception):
                failed += 1
                continue
            
            modifier, suggestions, success = result
            
            if success:
                form_keywords.update(suggestions)
                successful += 1
            else:
                failed += 1
        
        print(f"‚úÖ –§–æ—Ä–º–∞ '{form}': {len(form_keywords)} –∫–ª—é—á–µ–π (–∑–∞–ø—Ä–æ—Å–æ–≤: {len(modifiers)}, ‚úÖ {successful}, ‚ùå {failed})")
        
        return {
            "form": form,
            "seed": current_seed,
            "keywords": form_keywords,
            "queries": len(modifiers),
            "successful": successful,
            "failed": failed
        }

    
    async def fetch_suggestions(self, query: str, country: str, language: str, client: httpx.AsyncClient) -> tuple:
        """
        –ó–∞–ø—Ä–æ—Å –∫ Google Autocomplete API
        Returns: (suggestions, success, is_rate_limit)
        """
        params = {"client": "chrome", "q": query, "gl": country, "hl": language}
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(self.base_url, params=params, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list) and len(data) > 1:
                    suggestions = [s for s in data[1] if isinstance(s, str)]
                    return (suggestions, True, False)
                return ([], True, False)
            
            elif response.status_code == 429:
                return ([], False, True)  # Rate limit
            
            return ([], True, False)
        
        except Exception as e:
            return ([], False, False)
    
    async def fetch_with_delay(self, modifier: str, seed: str, country: str, language: str, client: httpx.AsyncClient, custom_query: str = None) -> tuple:
        """–ó–∞–ø—Ä–æ—Å —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ connection pooling"""
        try:
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            await self.adaptive_delay.wait()
            
            # –ó–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ shared client (connection pooling!)
            query = custom_query if custom_query else f"{seed} {modifier}"
            results, success, is_rate_limit = await self.fetch_suggestions(query, country, language, client)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
            if is_rate_limit:
                self.adaptive_delay.record_rate_limit()
                return (modifier, [], False)
            elif success:
                self.adaptive_delay.record_success()
                return (modifier, results, True)
            else:
                self.adaptive_delay.record_error()
                return (modifier, [], False)
        
        except Exception as e:
            self.adaptive_delay.record_error()
            return (modifier, [], False)
    
    async def parse(self, seed: str, country: str, language: str, use_numbers: bool = True, parallel_limit: int = 5) -> Dict:
        """
        SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        –ü–∞—Ç—Ç–µ—Ä–Ω: seed + modifier
        """
        start_time = time.time()
        all_keywords = set()
        
        print(f"\n{'='*60}")
        print(f"SUFFIX PARSER - OPTIMIZED v3.6")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"Country: {country.upper()}, Language: {language.upper()}")
        print(f"Parallel: {parallel_limit}, Adaptive Delay: 0.1-1.0 —Å–µ–∫\n")
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        modifiers = self.get_modifiers(language, use_numbers, seed)
        print(f"üìä –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã: {modifiers[:10]}... (–≤—Å–µ–≥–æ {len(modifiers)})\n")
        
        # –°—á—ë—Ç—á–∏–∫–∏
        total_queries = 0
        successful_queries = 0
        failed_queries = 0
        
        # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì —Å Connection Pooling
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async with httpx.AsyncClient(timeout=10.0) as shared_client:
            print(f"üèä Connection pooling: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π HTTP –∫–ª–∏–µ–Ω—Ç\n")
            
            async def fetch_limited(modifier):
                async with semaphore:
                    return await self.fetch_with_delay(modifier, seed, country, language, shared_client)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = [fetch_limited(m) for m in modifiers]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_queries += 1
                total_queries += 1
                continue
            
            modifier, suggestions, success = result
            total_queries += 1
            
            if success:
                all_keywords.update(suggestions)
                successful_queries += 1
                if i < 5 or len(suggestions) > 0:
                    print(f"[{i+1}/{len(modifiers)}] '{seed} {modifier}' ‚Üí {len(suggestions)} results")
            else:
                failed_queries += 1
        
        elapsed_time = time.time() - start_time
        delay_stats = self.adaptive_delay.get_stats()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'='*60}")
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {total_queries} (‚úÖ {successful_queries}, ‚ùå {failed_queries})")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"–í—Ä–µ–º—è: {elapsed_time:.2f} —Å–µ–∫ ({elapsed_time/total_queries:.2f} —Å–µ–∫/–∑–∞–ø—Ä–æ—Å)")
        print(f"–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º: {parallel_limit}")
        print(f"üß† Adaptive Delay: {delay_stats['final_delay']:.3f} —Å–µ–∫ (rate limits: {delay_stats['rate_limit_hits']})")
        print(f"üèä Connection Pooling: –í–ö–õ–Æ–ß–Å–ù")
        print(f"{'='*60}\n")
        
        return {
            "method": "SUFFIX Optimized",
            "seed": seed,
            "country": country,
            "language": language,
            "modifiers_count": len(modifiers),  # –î–û–ë–ê–í–ò–õ–ò!
            "modifiers_sample": modifiers[:10],  # –î–û–ë–ê–í–ò–õ–ò!
            "queries": total_queries,
            "successful_queries": successful_queries,
            "count": len(all_keywords),
            "keywords": sorted(list(all_keywords)),
            "elapsed_time": round(elapsed_time, 2),
            "avg_time_per_query": round(elapsed_time / total_queries, 2),
            "adaptive_delay": delay_stats
        }
    
    async def parse_infix(self, seed: str, country: str, language: str, use_numbers: bool = True, parallel_limit: int = 5) -> Dict:
        """
        INFIX –ø–∞—Ä—Å–∏–Ω–≥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        –ü–∞—Ç—Ç–µ—Ä–Ω: word1 + modifier + word2
        
        –ü—Ä–∏–º–µ—Ä:
        Seed: "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"
        –†–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞: ["—Ä–µ–º–æ–Ω—Ç", "–ø—ã–ª–µ—Å–æ—Å–æ–≤"]
        –ó–∞–ø—Ä–æ—Å—ã: "—Ä–µ–º–æ–Ω—Ç –∞ –ø—ã–ª–µ—Å–æ—Å–æ–≤", "—Ä–µ–º–æ–Ω—Ç –± –ø—ã–ª–µ—Å–æ—Å–æ–≤", ...
        """
        start_time = time.time()
        all_keywords = set()
        
        print(f"\n{'='*60}")
        print(f"INFIX PARSER - OPTIMIZED v3.6")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"Country: {country.upper()}, Language: {language.upper()}")
        print(f"Parallel: {parallel_limit}, Adaptive Delay: 0.1-1.0 —Å–µ–∫\n")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º seed –Ω–∞ —Å–ª–æ–≤–∞
        words = seed.strip().split()
        
        if len(words) < 2:
            return {
                "error": "INFIX —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞ –≤ seed",
                "example": "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ (2 —Å–ª–æ–≤–∞) ‚úÖ",
                "your_seed": f"{seed} ({len(words)} —Å–ª–æ–≤) ‚ùå"
            }
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–¢–û–õ–¨–ö–û –ö–ò–†–ò–õ–õ–ò–¶–ê –¥–ª—è INFIX!)
        modifiers = self.get_modifiers(language, use_numbers=False, seed=seed, cyrillic_only=True)
        print(f"üìä –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã: {modifiers[:10]}... (–≤—Å–µ–≥–æ {len(modifiers)})")
        print(f"üìä –ü–∞—Ç—Ç–µ—Ä–Ω INFIX: '{words[0]}' + modifier + '{' '.join(words[1:])}'")
        print(f"üìä –ü—Ä–∏–º–µ—Ä: '{words[0]} {modifiers[0] if modifiers else '–∞'} {' '.join(words[1:])}'")
        
        # –°—á—ë—Ç—á–∏–∫–∏
        total_queries = 0
        successful_queries = 0
        failed_queries = 0
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
        
        # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì —Å Connection Pooling
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async with httpx.AsyncClient(timeout=10.0) as shared_client:
            print(f"üèä Connection pooling: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π HTTP –∫–ª–∏–µ–Ω—Ç\n")
            
            async def fetch_limited(modifier):
                async with semaphore:
                    # INFIX –ø–∞—Ç—Ç–µ—Ä–Ω: word1 + modifier + word2 word3...
                    infix_seed = f"{words[0]} {modifier} {' '.join(words[1:])}"
                    return await self.fetch_with_delay(modifier, words[0] + " _", country, language, shared_client, custom_query=infix_seed)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = [fetch_limited(m) for m in modifiers]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_queries += 1
                total_queries += 1
                continue
            
            modifier, suggestions, success = result
            total_queries += 1
            
            if success:
                all_keywords.update(suggestions)
                successful_queries += 1
                infix_query = f"{words[0]} {modifier} {' '.join(words[1:])}"
                if i < 5 or len(suggestions) > 0:
                    print(f"[{i+1}/{len(modifiers)}] '{infix_query}' ‚Üí {len(suggestions)} results")
            else:
                failed_queries += 1
        
        elapsed_time = time.time() - start_time
        delay_stats = self.adaptive_delay.get_stats()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'='*60}")
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {total_queries} (‚úÖ {successful_queries}, ‚ùå {failed_queries})")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"–í—Ä–µ–º—è: {elapsed_time:.2f} —Å–µ–∫ ({elapsed_time/total_queries:.2f} —Å–µ–∫/–∑–∞–ø—Ä–æ—Å)")
        print(f"–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º: {parallel_limit}")
        print(f"üß† Adaptive Delay: {delay_stats['final_delay']:.3f} —Å–µ–∫ (rate limits: {delay_stats['rate_limit_hits']})")
        print(f"üèä Connection Pooling: –í–ö–õ–Æ–ß–Å–ù")
        print(f"{'='*60}\n")
        
        return {
            "method": "INFIX Optimized",
            "seed": seed,
            "pattern": f"'{words[0]}' + modifier + '{' '.join(words[1:])}'",
            "country": country,
            "language": language,
            "modifiers_count": len(modifiers),
            "modifiers_sample": modifiers[:10],
            "queries": total_queries,
            "successful_queries": successful_queries,
            "count": len(all_keywords),
            "keywords": sorted(list(all_keywords)),
            "elapsed_time": round(elapsed_time, 2),
            "avg_time_per_query": round(elapsed_time / total_queries, 2),
            "adaptive_delay": delay_stats
        }
    
    async def parse_morphology(self, seed: str, country: str, language: str, use_numbers: bool = True, parallel_limit: int = 5) -> Dict:
        """
        SUFFIX –ü–ê–†–°–ò–ù–ì –° –ú–û–†–§–û–õ–û–ì–ò–ï–ô
        
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ–≤–∞ –≤ seed
        –∏ –ø–∞—Ä—Å–∏—Ç –∫–∞–∂–¥—É—é —Ñ–æ—Ä–º—É –æ—Ç–¥–µ–ª—å–Ω–æ.
        
        –ü—Ä–∏–º–µ—Ä:
        Seed: "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"
        –§–æ—Ä–º—ã: ["–ø—ã–ª–µ—Å–æ—Å", "–ø—ã–ª–µ—Å–æ—Å–∞", "–ø—ã–ª–µ—Å–æ—Å—É", "–ø—ã–ª–µ—Å–æ—Å–æ–º", "–ø—ã–ª–µ—Å–æ—Å–µ", 
                "–ø—ã–ª–µ—Å–æ—Å—ã", "–ø—ã–ª–µ—Å–æ—Å–æ–≤", "–ø—ã–ª–µ—Å–æ—Å–∞–º", "–ø—ã–ª–µ—Å–æ—Å–∞–º–∏", "–ø—ã–ª–µ—Å–æ—Å–∞—Ö"]
        
        –î–ª—è –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—ã –¥–µ–ª–∞–µ–º SUFFIX –ø–∞—Ä—Å–∏–Ω–≥:
        - "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–∞ –∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–∞ –±", ...
        - "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å—É –∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å—É –±", ...
        - ...
        
        –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —è–∑—ã–∫–æ–≤:
        - RU, UK: –ø–æ–ª–Ω–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (10+ —Ñ–æ—Ä–º)
        - EN: –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ/–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ (2 —Ñ–æ—Ä–º—ã)
        - –î—Ä—É–≥–∏–µ: —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞
        """
        start_time = time.time()
        all_keywords = set()
        
        print(f"\n{'='*60}")
        print(f"MORPHOLOGY PARSER - SUFFIX + AUTO MORPHOLOGY")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"Country: {country.upper()}, Language: {language.upper()}")
        print(f"Parallel: {parallel_limit}\n")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º seed –Ω–∞ —Å–ª–æ–≤–∞
        words = seed.strip().split()
        
        if len(words) == 0:
            return {"error": "Seed –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º"}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏
        base_word = words[-1]
        prefix = " ".join(words[:-1]) if len(words) > 1 else ""
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã
        word_forms = self.get_morphological_forms(base_word, language)
        
        print(f"üìö –ë–∞–∑–æ–≤–æ–µ —Å–ª–æ–≤–æ: '{base_word}'")
        print(f"üìö –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã: {word_forms}")
        print(f"üìö –í—Å–µ–≥–æ —Ñ–æ—Ä–º: {len(word_forms)}\n")
        
        # –£–ë–ò–†–ê–ï–ú –î–£–ë–õ–ò–ö–ê–¢–´ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
        unique_forms = list(set(word_forms))
        if len(unique_forms) < len(word_forms):
            print(f"üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —É–±—Ä–∞–Ω–æ {len(word_forms) - len(unique_forms)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ñ–æ—Ä–º")
            print(f"üìö –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º: {len(unique_forms)}\n")
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É
        self.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
        
        # ============================================
        # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –í–°–ï–• –§–û–†–ú! üöÄ
        # ============================================
        print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–£–Æ –æ–±—Ä–∞–±–æ—Ç–∫—É {len(unique_forms)} —Ñ–æ—Ä–º...\n")
        
        # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—ã
        tasks = []
        for word_form in unique_forms:
            task = self.parse_single_morphological_form(
                form=word_form,
                prefix=prefix,
                language=language,
                use_numbers=use_numbers,
                country=country,
                parallel_limit=parallel_limit
            )
            tasks.append(task)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –í–°–ï —Ñ–æ—Ä–º—ã –û–î–ù–û–í–†–ï–ú–ï–ù–ù–û!
        form_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        total_queries = 0
        successful_queries = 0
        failed_queries = 0
        forms_breakdown = {}
        
        for result in form_results:
            if isinstance(result, Exception):
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ä–º—ã: {result}")
                continue
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–ª—é—á–∏
            all_keywords.update(result["keywords"])
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_queries += result["queries"]
            successful_queries += result["successful"]
            failed_queries += result["failed"]
            
            # –î–µ—Ç–∞–ª–∏ –ø–æ —Ñ–æ—Ä–º–µ
            forms_breakdown[result["form"]] = {
                "seed": result["seed"],
                "keywords_count": len(result["keywords"]),
                "queries": result["queries"]
            }
        
        elapsed_time = time.time() - start_time
        delay_stats = self.adaptive_delay.get_stats()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'='*60}")
        print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º: {len(word_forms)}")
        print(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {total_queries} (‚úÖ {successful_queries}, ‚ùå {failed_queries})")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"–í—Ä–µ–º—è: {elapsed_time:.2f} —Å–µ–∫ ({elapsed_time/total_queries:.3f} —Å–µ–∫/–∑–∞–ø—Ä–æ—Å)")
        print(f"–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º: {parallel_limit}")
        print(f"üß† Adaptive Delay: {delay_stats['final_delay']:.3f} —Å–µ–∫ (rate limits: {delay_stats['rate_limit_hits']})")
        print(f"üèä Connection Pooling: –í–ö–õ–Æ–ß–Å–ù")
        print(f"{'='*60}\n")
        
        return {
            "method": "SUFFIX + Morphology (Parallel)",
            "seed": seed,
            "base_word": base_word,
            "word_forms": word_forms,
            "unique_forms": unique_forms,
            "forms_count": len(word_forms),
            "unique_forms_count": len(unique_forms),
            "country": country,
            "language": language,
            "queries": total_queries,
            "successful_queries": successful_queries,
            "count": len(all_keywords),
            "keywords": sorted(list(all_keywords)),
            "forms_breakdown": forms_breakdown,
            "elapsed_time": round(elapsed_time, 2),
            "avg_time_per_query": round(elapsed_time / total_queries, 3) if total_queries > 0 else 0,
            "adaptive_delay": delay_stats,
            "optimization": {
                "parallel_forms": True,
                "duplicate_removal": len(word_forms) - len(unique_forms),
                "speedup": f"{len(unique_forms) * 2 / elapsed_time:.1f}√ó" if elapsed_time > 0 else "N/A"
            }
        }

# ============================================
# API ENDPOINTS
# ============================================

@app.get("/")
async def root():
    return {
        "api": "Google Autocomplete Parser - Optimized",
        "version": "4.0",
        "methods": {
            "suffix": "seed + modifier (—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ + –∞) - –ª–∞—Ç–∏–Ω–∏—Ü–∞ + –∫–∏—Ä–∏–ª–ª–∏—Ü–∞",
            "infix": "word1 + modifier + word2 (—Ä–µ–º–æ–Ω—Ç + –∞ + –ø—ã–ª–µ—Å–æ—Å–æ–≤) - —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞",
            "morphology": "SUFFIX –¥–ª—è –≤—Å–µ—Ö –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º —Å–ª–æ–≤–∞ (–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û!)"
        },
        "optimizations": [
            "Connection Pooling (–ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π)",
            "Adaptive Delay (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)",
            "Parallel Requests (5 –ø–æ—Ç–æ–∫–æ–≤)",
            "Smart Filtering (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤)",
            "Auto Morphology (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –¥–ª—è RU/UK/EN)",
            "Parallel Forms (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º) üöÄ"
        ],
        "performance": {
            "baseline": "37.86 —Å–µ–∫",
            "optimized_suffix": "~2.48 —Å–µ–∫ (470 –∫–ª—é—á–µ–π)",
            "optimized_morphology_old": "~21 —Å–µ–∫ (1112 –∫–ª—é—á–µ–π)",
            "optimized_morphology_new": "~3-4 —Å–µ–∫ (1112 –∫–ª—é—á–µ–π) üöÄ",
            "speedup_suffix": "15√ó –±—ã—Å—Ç—Ä–µ–µ",
            "speedup_morphology": "5-7√ó –±—ã—Å—Ç—Ä–µ–µ (vs –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è)"
        },
        "endpoints": {
            "suffix": "/api/parse",
            "infix": "/api/parse-infix",
            "morphology": "/api/parse-morphology",
            "compare": "/api/compare",
            "examples": {
                "suffix": "/api/parse?seed=—Ä–µ–º–æ–Ω—Ç+–ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru&parallel=5",
                "infix": "/api/parse-infix?seed=—Ä–µ–º–æ–Ω—Ç+–ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru&parallel=5",
                "morphology": "/api/parse-morphology?seed=—Ä–µ–º–æ–Ω—Ç+–ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru&parallel=5",
                "compare": "/api/compare?seed=—Ä–µ–º–æ–Ω—Ç+–ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru&parallel=5"
            }
        },
        "morphology_support": {
            "ru": "‚úÖ –ü–æ–ª–Ω–∞—è (—á–µ—Ä–µ–∑ pymorphy2.lexeme)",
            "uk": "‚úÖ –ü–æ–ª–Ω–∞—è (—á–µ—Ä–µ–∑ pymorphy2.lexeme)",
            "en": "‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è (—á–µ—Ä–µ–∑ lemminflect) –∏–ª–∏ –±–∞–∑–æ–≤–∞—è",
            "other": "‚ö†Ô∏è –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞"
        },
        "required_packages": {
            "ru_uk": "pip install pymorphy2",
            "en": "pip install lemminflect (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
            "other": "–ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è (–±–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞)"
        }
    }

@app.get("/api/parse")
async def parse_suffix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (UA, US, RU, DE...)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞ (ru, en, uk, de...)"),
    use_numbers: bool = Query(False, description="–í–∫–ª—é—á–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel: int = Query(5, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (1-10)", ge=1, le=10)
):
    """
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô SUFFIX –ü–ê–†–°–ò–ù–ì
    
    –ü–∞—Ç—Ç–µ—Ä–Ω: seed + [a-z, –∞-—è, 0-9]
    
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - Connection Pooling: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ HTTP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
    - Adaptive Delay: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ (0.1-1.0 —Å–µ–∫)
    - Parallel: 5 –ø–æ—Ç–æ–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    - Smart Filtering: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–∞—Ç–∏–Ω–∏—Ü—É –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤
    
    –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
    - –í—Ä–µ–º—è: ~2 —Å–µ–∫ –Ω–∞ 56 –∑–∞–ø—Ä–æ—Å–æ–≤
    - –£—Å–∫–æ—Ä–µ–Ω–∏–µ: 17√ó –æ—Ç –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏
    """
    parser = KeywordParser()
    result = await parser.parse(
        seed=seed,
        country=country,
        language=language,
        use_numbers=use_numbers,
        parallel_limit=parallel
    )
    return result

@app.get("/api/parse-infix")
async def parse_infix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞)"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (UA, US, RU, DE...)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞ (ru, en, uk, de...)"),
    use_numbers: bool = Query(False, description="–í–∫–ª—é—á–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel: int = Query(5, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (1-10)", ge=1, le=10)
):
    """
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô INFIX –ü–ê–†–°–ò–ù–ì
    
    –ü–∞—Ç—Ç–µ—Ä–Ω: word1 + modifier + word2
    
    –ü—Ä–∏–º–µ—Ä:
    Seed: "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤" ‚Üí "—Ä–µ–º–æ–Ω—Ç –∞ –ø—ã–ª–µ—Å–æ—Å–æ–≤", "—Ä–µ–º–æ–Ω—Ç –± –ø—ã–ª–µ—Å–æ—Å–æ–≤", ...
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - Seed –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ INFIX:
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¢–û–õ–¨–ö–û –∫–∏—Ä–∏–ª–ª–∏—Ü—É (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞)
    - –ë–ï–ó —Ü–∏—Ñ—Ä (—Ü–∏—Ñ—Ä—ã –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è)
    - ~30 –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 56 (–±—ã—Å—Ç—Ä–µ–µ!)
    
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - Connection Pooling: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ HTTP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
    - Adaptive Delay: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ (0.1-1.0 —Å–µ–∫)
    - Parallel: 5 –ø–æ—Ç–æ–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    - Cyrillic Only: —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
    
    –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
    - –í—Ä–µ–º—è: ~1.5 —Å–µ–∫ –Ω–∞ 30 –∑–∞–ø—Ä–æ—Å–æ–≤
    - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏: +15 (~3%)
    """
    parser = KeywordParser()
    result = await parser.parse_infix(
        seed=seed,
        country=country,
        language=language,
        use_numbers=use_numbers,
        parallel_limit=parallel
    )
    return result

@app.get("/api/parse-morphology")
async def parse_morphology(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (UA, US, RU, DE...)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞ (ru, en, uk, de...)"),
    use_numbers: bool = Query(False, description="–í–∫–ª—é—á–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel: int = Query(5, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (1-10)", ge=1, le=10)
):
    """
    SUFFIX –ü–ê–†–°–ò–ù–ì –° –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ô –ú–û–†–§–û–õ–û–ì–ò–ï–ô (–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô)
    
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Å–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ–≤–∞
    –∏ –ø–∞—Ä—Å–∏—Ç –∫–∞–∂–¥—É—é —Ñ–æ—Ä–º—É –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û —á–µ—Ä–µ–∑ SUFFIX –º–µ—Ç–æ–¥.
    
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:
    - ‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ä–º (–≤—Å–µ —Ñ–æ—Ä–º—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ!)
    - ‚úÖ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ñ–æ—Ä–º
    - ‚úÖ Connection Pooling
    - ‚úÖ Adaptive Delay
    
    –ü—Ä–∏–º–µ—Ä:
    Seed: "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"
    
    –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã (RU):
    - –ß–µ—Ä–µ–∑ pymorphy2.lexeme –ø–æ–ª—É—á–∞–µ—Ç –í–°–ï —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
    - –ø—ã–ª–µ—Å–æ—Å, –ø—ã–ª–µ—Å–æ—Å–∞, –ø—ã–ª–µ—Å–æ—Å—É, –ø—ã–ª–µ—Å–æ—Å–æ–º, –ø—ã–ª–µ—Å–æ—Å–µ (–µ–¥.—á.)
    - –ø—ã–ª–µ—Å–æ—Å—ã, –ø—ã–ª–µ—Å–æ—Å–æ–≤, –ø—ã–ª–µ—Å–æ—Å–∞–º, –ø—ã–ª–µ—Å–æ—Å–∞–º–∏, –ø—ã–ª–µ—Å–æ—Å–∞—Ö (–º–Ω.—á.)
    
    –í—Å–µ —Ñ–æ—Ä–º—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û:
    - "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–∞ –∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–∞ –±", ... (56 –∑–∞–ø—Ä–æ—Å–æ–≤)
    - "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å—É –∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å—É –±", ... (56 –∑–∞–ø—Ä–æ—Å–æ–≤)
    - ... (–≤—Å–µ —Ñ–æ—Ä–º—ã –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è –û–î–ù–û–í–†–ï–ú–ï–ù–ù–û!)
    
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —è–∑—ã–∫–æ–≤:
    - RU, UK: –ø–æ–ª–Ω–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è —á–µ—Ä–µ–∑ pymorphy2 (10+ —Ñ–æ—Ä–º)
    - EN: –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ/–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ —á–µ—Ä–µ–∑ lemminflect (2-3 —Ñ–æ—Ä–º—ã)
    - –î—Ä—É–≥–∏–µ: —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞ (1 —Ñ–æ—Ä–º–∞)
    
    –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
    - –ë–´–õ–û (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ): ~20 —Å–µ–∫ –¥–ª—è RU/UK
    - –°–¢–ê–õ–û (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ): ~3-4 —Å–µ–∫ –¥–ª—è RU/UK (–≤ 5-7√ó –±—ã—Å—Ç—Ä–µ–µ!) üöÄ
    - RU/UK: ~10 —Ñ–æ—Ä–º √ó 56 –∑–∞–ø—Ä–æ—Å–æ–≤ = 560 –∑–∞–ø—Ä–æ—Å–æ–≤ (~3-4 —Å–µ–∫)
    - EN: ~2-3 —Ñ–æ—Ä–º—ã √ó 56 –∑–∞–ø—Ä–æ—Å–æ–≤ = 112-168 –∑–∞–ø—Ä–æ—Å–æ–≤ (~1-2 —Å–µ–∫)
    
    –†–µ–∑—É–ª—å—Ç–∞—Ç:
    - –ù–∞–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π (+50-100%)
    - –ù–∞—Ö–æ–¥–∏—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞–¥–µ–∂–∞–º–∏
    - –í 5-7√ó –±—ã—Å—Ç—Ä–µ–µ —á–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    """
    parser = KeywordParser()
    result = await parser.parse_morphology(
        seed=seed,
        country=country,
        language=language,
        use_numbers=use_numbers,
        parallel_limit=parallel
    )
    return result

@app.get("/api/compare")
async def compare_methods(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤", ge=1, le=10)
):
    """
    –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –¢–†–Å–• –ú–ï–¢–û–î–û–í: SUFFIX vs INFIX vs MORPHOLOGY
    
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ –º–µ—Ç–æ–¥—ã –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π
    - –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    - –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏ –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
    """
    parser = KeywordParser()
    
    print("\nüîÑ –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í: SUFFIX vs INFIX vs MORPHOLOGY\n")
    
    # SUFFIX
    print("‚ö° –ó–∞–ø—É—Å–∫ SUFFIX...")
    suffix_result = await parser.parse(
        seed=seed,
        country=country,
        language=language,
        use_numbers=False,
        parallel_limit=parallel
    )
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º adaptive delay –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏
    parser.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
    
    # INFIX
    print("\nüîÑ –ó–∞–ø—É—Å–∫ INFIX...")
    infix_result = await parser.parse_infix(
        seed=seed,
        country=country,
        language=language,
        use_numbers=False,  # INFIX –±–µ–∑ —Ü–∏—Ñ—Ä
        parallel_limit=parallel
    )
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º adaptive delay –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏
    parser.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
    
    # MORPHOLOGY
    print("\nüöÄ –ó–∞–ø—É—Å–∫ MORPHOLOGY...")
    morphology_result = await parser.parse_morphology(
        seed=seed,
        country=country,
        language=language,
        use_numbers=False,
        parallel_limit=parallel
    )
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ INFIX
    if "error" in infix_result:
        print("‚ö†Ô∏è INFIX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —ç—Ç–æ–≥–æ seed")
        infix_keywords = set()
    else:
        infix_keywords = set(infix_result["keywords"])
    
    # –°–æ–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∫–ª—é—á–µ–π
    suffix_keywords = set(suffix_result["keywords"])
    morphology_keywords = set(morphology_result["keywords"])
    
    # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
    all_three = suffix_keywords & infix_keywords & morphology_keywords
    suffix_infix = suffix_keywords & infix_keywords
    suffix_morphology = suffix_keywords & morphology_keywords
    infix_morphology = infix_keywords & morphology_keywords
    
    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
    suffix_only = suffix_keywords - infix_keywords - morphology_keywords
    infix_only = infix_keywords - suffix_keywords - morphology_keywords
    morphology_only = morphology_keywords - suffix_keywords - infix_keywords
    
    # –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
    total_unique = suffix_keywords | infix_keywords | morphology_keywords
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
    counts = {
        "SUFFIX": len(suffix_keywords),
        "INFIX": len(infix_keywords),
        "MORPHOLOGY": len(morphology_keywords)
    }
    
    times = {
        "SUFFIX": suffix_result["elapsed_time"],
        "INFIX": infix_result.get("elapsed_time", 0),
        "MORPHOLOGY": morphology_result["elapsed_time"]
    }
    
    winner_count = max(counts, key=counts.get)
    winner_speed = min(times, key=times.get)
    
    return {
        "seed": seed,
        "comparison": {
            "suffix": {
                "count": len(suffix_keywords),
                "time": suffix_result["elapsed_time"],
                "queries": suffix_result["queries"]
            },
            "infix": {
                "count": len(infix_keywords),
                "time": infix_result.get("elapsed_time", 0),
                "queries": infix_result.get("queries", 0)
            },
            "morphology": {
                "count": len(morphology_keywords),
                "time": morphology_result["elapsed_time"],
                "queries": morphology_result["queries"],
                "forms": morphology_result.get("forms_count", 0)
            },
            "total_unique": len(total_unique),
            "total_time": sum(times.values()),
            "intersections": {
                "all_three": len(all_three),
                "suffix_infix": len(suffix_infix),
                "suffix_morphology": len(suffix_morphology),
                "infix_morphology": len(infix_morphology)
            },
            "unique_only": {
                "suffix": {
                    "count": len(suffix_only),
                    "sample": sorted(list(suffix_only))[:5]
                },
                "infix": {
                    "count": len(infix_only),
                    "sample": sorted(list(infix_only))[:5]
                },
                "morphology": {
                    "count": len(morphology_only),
                    "sample": sorted(list(morphology_only))[:5]
                }
            }
        },
        "winner": {
            "by_count": winner_count,
            "by_speed": winner_speed,
            "recommendation": f"üèÜ {winner_count} –¥–∞—ë—Ç –º–∞–∫—Å–∏–º—É–º –∫–ª—é—á–µ–π ({counts[winner_count]}), {winner_speed} —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π ({times[winner_speed]:.1f}—Å)"
        },
        "summary": f"–ù–∞–π–¥–µ–Ω–æ {len(total_unique)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π –∑–∞ {sum(times.values()):.1f} —Å–µ–∫"
    }
