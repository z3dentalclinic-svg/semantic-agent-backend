import asyncio
import logging
import random
import re
import time
from typing import List, Dict, Set, Optional
import httpx
import pymorphy3
from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse

# --- CONFIG ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("FGS_PARSER")

# –ë–∞–∑–∞ –≥–æ—Ä–æ–¥–æ–≤ –∏ –±—Ä–µ–Ω–¥–æ–≤ (Whitelist)
WHITELIST_TOKENS = {"—Ñ–∏–ª–∏–ø—Å", "philips", "—Å–∞–º—Å—É–Ω–≥", "samsung", "–±–æ—à", "bosch", "lg", "dyson", "–∂–µ–ª—Ç—ã–µ –≤–æ–¥—ã"}
GEO_BLACKLIST = {
    "ua": {"–º–æ—Å–∫–≤–∞", "—Å–ø–±", "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–º–∏–Ω—Å–∫", "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "–∫–∞–∑–∞–Ω—å", "—Ä–æ—Å—Ç–æ–≤"},
    "ru": {"–∫–∏–µ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–¥–Ω–µ–ø—Ä", "–æ–¥–µ—Å—Å–∞", "–ª—å–≤–æ–≤", "–∑–∞–ø–æ—Ä–æ–∂—å–µ", "–≤–∏–Ω–Ω–∏—Ü–∞", "–∫—Ä–∏–≤–æ–π —Ä–æ–≥"}
}

class GoogleAutocompleteParser:
    def __init__(self):
        self.morph = pymorphy3.MorphAnalyzer()
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]

    # --- –§–ò–õ–¨–¢–†–´ v5.3.0 ---
    def is_query_safe_pre(self, query: str, country: str) -> bool:
        q_lower = query.lower()
        if any(w in q_lower for w in WHITELIST_TOKENS): return True
        
        words = re.findall(r'\b\w+\b', q_lower)
        blacklist = GEO_BLACKLIST.get(country.lower(), set())
        
        for w in words:
            lemma = self.morph.parse(w)[0].normal_form
            if (w in blacklist or lemma in blacklist) and lemma not in WHITELIST_TOKENS:
                logger.warning(f"üö´ [PRE] Blocked: '{query}' (Found: {lemma})")
                return False
        return True

    async def filter_results_post(self, keywords: List[str], country: str) -> List[str]:
        clean = []
        blacklist = GEO_BLACKLIST.get(country.lower(), set())
        for kw in keywords:
            kw_l = kw.lower()
            if any(w in kw_l for w in WHITELIST_TOKENS):
                clean.append(kw)
                continue
            if any(city in kw_l for city in blacklist):
                logger.info(f"‚ö†Ô∏è [POST] Cleaned: '{kw}'")
                continue
            clean.append(kw)
        return clean

    # --- –°–ë–û–† –î–ê–ù–ù–´–• ---
    async def fetch(self, query: str, lang: str, country: str, client: httpx.AsyncClient) -> List[str]:
        url = "https://www.google.com/complete/search"
        params = {"q": query, "client": "firefox", "hl": lang, "gl": country}
        try:
            resp = await client.get(url, params=params, timeout=5)
            return resp.json()[1] if resp.status_code == 200 else []
        except: return []

    async def parse_adaptive_prefix(self, seed: str, country: str, lang: str, use_numbers: bool, limit: int):
        start_time = time.time()
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø—É–ª–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (—á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å –æ–±—ä–µ–º –∫–ª—é—á–µ–π)
        prefixes = ["", "–∫—É–ø–∏—Ç—å ", "—Ü–µ–Ω–∞ ", "–æ—Ç–∑—ã–≤—ã ", "—Ä–µ–º–æ–Ω—Ç ", "–≤ "]
        alphabet = "–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ç—é—è"
        if lang == "en": alphabet = "abcdefghijklmnopqrstuvwxyz"
        
        queries = [f"{p}{seed}".strip() for p in prefixes]
        for char in alphabet:
            queries.append(f"{seed} {char}")
        if use_numbers:
            for n in range(10): queries.append(f"{seed} {n}")

        results = set()
        semaphore = asyncio.Semaphore(limit)

        async def worker(q, client):
            async with semaphore:
                if self.is_query_safe_pre(q, country):
                    data = await self.fetch(q, lang, country, client)
                    results.update(data)

        async with httpx.AsyncClient(headers={"User-Agent": random.choice(self.user_agents)}) as client:
            await asyncio.gather(*[worker(q, client) for q in queries])

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        final_list = await self.filter_results_post(list(results), country)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—ã–π –æ–±—ä–µ–∫—Ç –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        return {
            "seed": seed,
            "keywords": sorted(final_list),
            "count": len(final_list),
            "time": round(time.time() - start_time, 2),
            "method": "adaptive-prefix",
            "source": "google"
        }

# --- API ---
app = FastAPI()
parser = GoogleAutocompleteParser()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/api/parse/adaptive-prefix")
async def api_adaptive_prefix(
    seed: str = Query(...), 
    country: str = "ua", 
    lang: str = "ru", 
    use_numbers: bool = False,
    parallel_limit: int = 10
):
    return await parser.parse_adaptive_prefix(seed, country, lang, use_numbers, parallel_limit)

@app.get("/")
def root():
    return FileResponse('static/index.html')
