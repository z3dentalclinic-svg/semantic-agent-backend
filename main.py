"""
ChatGPT PPM TEST - PREFIX Projection Method
–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è PREFIX —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ n-–≥—Ä–∞–º–º
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List
from collections import Counter
import os
import httpx
import asyncio
import time
import random

app = FastAPI(title="ChatGPT PPM Test", version="1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AutocompleteParser:
    def __init__(self):
        self.base_url = "https://suggestqueries.google.com/complete/search"
    
    async def fetch_suggestions(self, query: str, country: str, language: str) -> List[str]:
        params = {"client": "chrome", "q": query, "gl": country, "hl": language}
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(self.base_url, params=params)
                if response.status_code == 200:
                    data = response.json()
                    if isinstance(data, list) and len(data) > 1:
                        return [s for s in data[1] if isinstance(s, str)]
                return []
        except Exception as e:
            print(f"Error: {e}")
            return []
    
    async def chatgpt_ppm_test(self, seed: str, country: str, language: str) -> List[str]:
        all_keywords = set()
        seed_words = set(seed.lower().split())
        
        print(f"\n{'='*60}")
        print(f"üî¨ ChatGPT PPM - PREFIX Projection Method")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"–ú–µ—Ç–æ–¥: –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —á–µ—Ä–µ–∑ n-–≥—Ä–∞–º–º—ã\n")
        
        # ========================================
        # –≠–¢–ê–ü 1: –ë–∞–∑–æ–≤—ã–π SUFFIX –ø–∞—Ä—Å–∏–Ω–≥
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 1: –ë–∞–∑–æ–≤—ã–π SUFFIX –ø–∞—Ä—Å–∏–Ω–≥")
        print(f"{'='*60}\n")
        
        alphabet = "–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"
        suffix_results = []
        
        for letter in alphabet:
            query = f"{seed} {letter}"
            results = await self.fetch_suggestions(query, country, language)
            suffix_results.extend(results)
            await asyncio.sleep(random.uniform(0.3, 0.6))
        
        print(f"–ë–∞–∑–æ–≤—ã–π SUFFIX: 29 –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"–ü–æ–ª—É—á–µ–Ω–æ SUFFIX –∫–ª—é—á–µ–π: {len(suffix_results)}\n")
        
        # ========================================
        # –≠–¢–ê–ü 2: –û—Ç–±–æ—Ä —Ç–æ–ø-30 SUFFIX –¥–ª—è –≤—Ç–æ—Ä–∏—á–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 2: –û—Ç–±–æ—Ä —Ç–æ–ø-30 SUFFIX")
        print(f"{'='*60}\n")
        
        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 30 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ)
        top_suffix = suffix_results[:30] if len(suffix_results) >= 30 else suffix_results
        
        print(f"–û—Ç–æ–±—Ä–∞–Ω–æ –¥–ª—è –≤—Ç–æ—Ä–∏—á–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: {len(top_suffix)}")
        print(f"–ü—Ä–∏–º–µ—Ä—ã:")
        for s in top_suffix[:5]:
            print(f"  ‚Ä¢ {s}")
        print()
        
        # ========================================
        # –≠–¢–ê–ü 3: –í—Ç–æ—Ä–∏—á–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ (–ö–õ–Æ–ß–ï–í–û–ô –≠–¢–ê–ü!)
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 3: –í—Ç–æ—Ä–∏—á–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ç–æ–ø SUFFIX")
        print(f"{'='*60}")
        print(f"–¶–µ–ª—å: –Ω–∞–π—Ç–∏ –î–õ–ò–ù–ù–´–ï —Ü–µ–ø–æ—á–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤\n")
        
        # –¶–µ–ª–µ–≤—ã–µ –±—É–∫–≤—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è (—Ç–æ–ø-8 –ø–æ —á–∞—Å—Ç–æ—Ç–µ)
        expansion_letters = ["–∞", "–±", "–≤", "–≥", "—Å", "–º", "–Ω", "–∫"]
        all_expansions = []
        expansion_count = 0
        
        for suffix_key in top_suffix:
            for letter in expansion_letters:
                query = f"{suffix_key} {letter}"
                results = await self.fetch_suggestions(query, country, language)
                all_expansions.extend(results)
                expansion_count += 1
                await asyncio.sleep(random.uniform(0.3, 0.6))
        
        print(f"–í—Ç–æ—Ä–∏—á–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {expansion_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"–ü–æ–ª—É—á–µ–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_expansions)}\n")
        
        # ========================================
        # –≠–¢–ê–ü 4: –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ n-–≥—Ä–∞–º–º
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 4: –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ n-–≥—Ä–∞–º–º")
        print(f"{'='*60}\n")
        
        bigrams = Counter()
        trigrams = Counter()
        
        for result in all_expansions:
            words = result.lower().split()
            
            # –ë–∏–≥—Ä–∞–º–º—ã
            for i in range(len(words) - 1):
                bigram = f"{words[i]} {words[i+1]}"
                # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï —á–∞—Å—Ç—å seed
                if words[i] not in seed_words and words[i+1] not in seed_words:
                    bigrams[bigram] += 1
            
            # –¢—Ä–∏–≥—Ä–∞–º–º—ã
            for i in range(len(words) - 2):
                trigram = f"{words[i]} {words[i+1]} {words[i+2]}"
                if words[i] not in seed_words:
                    trigrams[trigram] += 1
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞—Å—Ç–æ—Ç–Ω—ã–µ
        frequent_bigrams = {k: v for k, v in bigrams.items() if v >= 3}
        frequent_trigrams = {k: v for k, v in trigrams.items() if v >= 2}
        
        print(f"–ù–∞–π–¥–µ–Ω–æ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –±–∏–≥—Ä–∞–º–º (‚â•3 —Ä–∞–∑): {len(frequent_bigrams)}")
        print(f"–¢–æ–ø-10 –±–∏–≥—Ä–∞–º–º:")
        for bigram, freq in sorted(frequent_bigrams.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  ‚Ä¢ '{bigram}' ({freq} —Ä–∞–∑)")
        
        print(f"\n–ù–∞–π–¥–µ–Ω–æ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Ç—Ä–∏–≥—Ä–∞–º–º (‚â•2 —Ä–∞–∑): {len(frequent_trigrams)}")
        print(f"–¢–æ–ø-10 —Ç—Ä–∏–≥—Ä–∞–º–º:")
        for trigram, freq in sorted(frequent_trigrams.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  ‚Ä¢ '{trigram}' ({freq} —Ä–∞–∑)")
        print()
        
        # ========================================
        # –≠–¢–ê–ü 5: –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è PREFIX
        # ========================================
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 5: –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è PREFIX")
        print(f"{'='*60}\n")
        
        prefix_candidates = set()
        projection_count = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∏–≥—Ä–∞–º–º—ã
        for ngram in frequent_bigrams.keys():
            test_query = f"{ngram} {seed}"
            results = await self.fetch_suggestions(test_query, country, language)
            projection_count += 1
            
            if results:
                prefix_candidates.add(ngram)
                print(f"‚úÖ –ë–∏–≥—Ä–∞–º–º–∞ '{ngram}' ‚Üí PREFIX –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω")
            
            await asyncio.sleep(random.uniform(0.3, 0.6))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã
        for ngram in list(frequent_trigrams.keys())[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–æ–ø-20
            test_query = f"{ngram} {seed}"
            results = await self.fetch_suggestions(test_query, country, language)
            projection_count += 1
            
            if results:
                prefix_candidates.add(ngram)
                print(f"‚úÖ –¢—Ä–∏–≥—Ä–∞–º–º–∞ '{ngram}' ‚Üí PREFIX –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω")
            
            await asyncio.sleep(random.uniform(0.3, 0.6))
        
        print(f"\n–ü—Ä–æ–µ–∫—Ü–∏—è: {projection_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"–ù–∞–π–¥–µ–Ω–æ PREFIX –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(prefix_candidates)}\n")
        
        # ========================================
        # –≠–¢–ê–ü 6: –°–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö PREFIX –∫–ª—é—á–µ–π
        # ========================================
        if len(prefix_candidates) > 0:
            print(f"{'='*60}")
            print(f"–≠–¢–ê–ü 6: –°–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö PREFIX –∫–ª—é—á–µ–π")
            print(f"{'='*60}\n")
            
            for candidate in prefix_candidates:
                query = f"{candidate} {seed}"
                results = await self.fetch_suggestions(query, country, language)
                
                if results:
                    all_keywords.update(results)
                    print(f"'{candidate}' ‚Üí {len(results)} –∫–ª—é—á–µ–π")
                
                await asyncio.sleep(random.uniform(0.3, 0.6))
        
        # ========================================
        # –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        # ========================================
        total_queries = 29 + expansion_count + projection_count
        
        print(f"\n{'='*60}")
        print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê PPM")
        print(f"{'='*60}")
        print(f"–ë–∞–∑–æ–≤—ã–π SUFFIX: 29 –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"–í—Ç–æ—Ä–∏—á–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {expansion_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è: {projection_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        print(f"–í–°–ï–ì–û –∑–∞–ø—Ä–æ—Å–æ–≤: {total_queries}")
        print(f"")
        print(f"–ù–∞–π–¥–µ–Ω–æ PREFIX –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(prefix_candidates)}")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã—Ö PREFIX –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"")
        
        if len(all_keywords) > 0:
            print(f"üéâ PPM –†–ê–ë–û–¢–ê–ï–¢!")
            print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–∞—à–ª–∞ PREFIX –∑–∞–ø—Ä–æ—Å—ã!")
        else:
            print(f"‚ùå PPM –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            print(f"–ß–∞—Å—Ç–æ—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç PREFIX —Å–ª–æ–≤")
        
        print(f"{'='*60}\n")
        
        return list(all_keywords)


@app.get("/api/test-parser/chatgpt-ppm")
async def test_chatgpt_ppm(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru")
):
    parser = AutocompleteParser()
    start = time.time()
    keywords = await parser.chatgpt_ppm_test(seed, country, language)
    return {
        "seed": seed,
        "method": "ChatGPT PPM",
        "keywords": keywords,
        "count": len(keywords),
        "time": round(time.time() - start, 2)
    }


@app.get("/")
async def root():
    return {
        "api": "ChatGPT PPM Test",
        "url": "/api/test-parser/chatgpt-ppm?seed=—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
