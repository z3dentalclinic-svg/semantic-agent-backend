"""
MORPHOLOGICAL ADAPTIVE TEST - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
–° User-Agent —Ä–æ—Ç–∞—Ü–∏–µ–π –∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List
from collections import Counter
import os
import httpx
import asyncio
import time
import random

app = FastAPI(title="Morphological ADAPTIVE Test Fixed", version="1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# User-Agent —Ä–æ—Ç–∞—Ü–∏—è
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

class AutocompleteParser:
    def __init__(self):
        self.base_url = "https://suggestqueries.google.com/complete/search"
    
    async def fetch_suggestions(self, query: str, country: str, language: str) -> List[str]:
        params = {"client": "chrome", "q": query, "gl": country, "hl": language}
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(self.base_url, params=params, headers=headers)
                if response.status_code == 200:
                    data = response.json()
                    if isinstance(data, list) and len(data) > 1:
                        return [s for s in data[1] if isinstance(s, str)]
                return []
        except Exception as e:
            print(f"Error: {e}")
            return []
    
    async def morphological_adaptive_test(self, seed: str, country: str, language: str) -> List[str]:
        all_keywords = set()
        seed_words = set(seed.lower().split())
        
        print(f"\n{'='*60}")
        print(f"üî¨ MORPHOLOGICAL ADAPTIVE (FIXED)")
        print(f"{'='*60}")
        print(f"Seed: '{seed}'")
        print(f"‚úÖ User-Agent —Ä–æ—Ç–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞")
        print(f"‚úÖ –ó–∞–¥–µ—Ä–∂–∫–∏ 1-2 —Å–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏\n")
        
        # –≠–¢–ê–ü 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º")
        print(f"{'='*60}\n")
        
        # –î–ª—è "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤" —Å–æ–∑–¥–∞—ë–º —Ñ–æ—Ä–º—ã –≤—Ä—É—á–Ω—É—é
        forms = [
            seed,                           # "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"
            "—Ä–µ–º–æ–Ω—Ç–∞ –ø—ã–ª–µ—Å–æ—Å–æ–≤",           # —Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π
            "–ø–æ —Ä–µ–º–æ–Ω—Ç—É –ø—ã–ª–µ—Å–æ—Å–æ–≤"         # –ø—Ä–µ–¥–ª–æ–≥ + –¥–∞—Ç–µ–ª—å–Ω—ã–π
        ]
        
        print(f"–§–æ—Ä–º: {len(forms)}")
        for i, form in enumerate(forms, 1):
            print(f"  {i}. '{form}'")
        print()
        
        # –≠–¢–ê–ü 2: SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –∫–∞–∂–¥–æ–π —Ñ–æ—Ä–º—ã
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 2: SUFFIX –ø–∞—Ä—Å–∏–Ω–≥")
        print(f"{'='*60}\n")
        
        alphabet = "–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"
        all_suffix_results = []
        suffix_count = 0
        
        for form_idx, form in enumerate(forms, 1):
            print(f"--- –§–æ—Ä–º–∞ {form_idx}: '{form}' ---")
            form_results = []
            
            for letter in alphabet:
                query = f"{form} {letter}"
                results = await self.fetch_suggestions(query, country, language)
                form_results.extend(results)
                all_suffix_results.extend(results)
                suffix_count += 1
                await asyncio.sleep(random.uniform(1.0, 2.0))
            
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(form_results)}")
            if form_results:
                for r in form_results[:3]:
                    print(f"  ‚Ä¢ {r}")
            print()
        
        print(f"SUFFIX –∑–∞–ø—Ä–æ—Å–æ–≤: {suffix_count}")
        print(f"–í—Å–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_suffix_results)}\n")
        
        # –≠–¢–ê–ü 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        print(f"{'='*60}\n")
        
        word_counter = Counter()
        
        for result in all_suffix_results:
            words = result.lower().split()
            for word in words:
                if word not in seed_words and len(word) > 2:
                    word_counter[word] += 1
        
        # –ß–∞—Å—Ç–æ—Ç–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        all_candidates = {w for w, count in word_counter.items() if count >= 2}
        
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(word_counter)}")
        print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (‚â•2): {len(all_candidates)}")
        print(f"\n–¢–æ–ø-20:")
        for word, count in word_counter.most_common(20):
            print(f"  ‚Ä¢ '{word}' ({count})")
        print()
        
        # –≠–¢–ê–ü 4: –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤—ã—Ö —Å–ª–æ–≤
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 4: –ê–Ω–∞–ª–∏–∑ –ù–û–í–´–• —Å–ª–æ–≤ –æ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏")
        print(f"{'='*60}\n")
        
        base_form_words = set()
        morpho_form_words = set()
        
        # –°–ª–æ–≤–∞ –æ—Ç –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º—ã (–ø–µ—Ä–≤–∞—è —Ç—Ä–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
        base_count = len(all_suffix_results) // 3
        for result in all_suffix_results[:base_count]:
            words = result.lower().split()
            for word in words:
                if word not in seed_words and len(word) > 2:
                    base_form_words.add(word)
        
        # –°–ª–æ–≤–∞ –æ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º
        for result in all_suffix_results[base_count:]:
            words = result.lower().split()
            for word in words:
                if word not in seed_words and len(word) > 2:
                    morpho_form_words.add(word)
        
        new_from_morphology = morpho_form_words - base_form_words
        
        print(f"–û—Ç –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º—ã: {len(base_form_words)}")
        print(f"–û—Ç –º–æ—Ä—Ñ–æ —Ñ–æ—Ä–º: {len(morpho_form_words)}")
        print(f"–ù–û–í–´–• –æ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏: {len(new_from_morphology)}")
        
        if new_from_morphology:
            print(f"\n–ù–æ–≤—ã–µ —Å–ª–æ–≤–∞:")
            for word in sorted(list(new_from_morphology)[:20]):
                print(f"  ‚Ä¢ {word}")
        print()
        
        # –≠–¢–ê–ü 5: PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞
        print(f"{'='*60}")
        print(f"–≠–¢–ê–ü 5: PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞")
        print(f"{'='*60}\n")
        
        prefix_count = 0
        verified_count = 0
        
        for candidate in sorted(all_candidates):
            query = f"{candidate} {seed}"
            results = await self.fetch_suggestions(query, country, language)
            prefix_count += 1
            
            if results:
                all_keywords.update(results)
                verified_count += 1
                if verified_count <= 10:
                    print(f"‚úÖ '{query}' ‚Üí {len(results)}")
            
            await asyncio.sleep(random.uniform(1.0, 2.0))
        
        print(f"\n–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: {prefix_count}")
        print(f"–í–∞–ª–∏–¥–Ω—ã—Ö PREFIX: {verified_count}")
        print()
        
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
        total_queries = suffix_count + prefix_count
        
        print(f"{'='*60}")
        print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"SUFFIX: {suffix_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"  - –ë–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞: 29")
        print(f"  - –ú–æ—Ä—Ñ–æ —Ñ–æ—Ä–º—ã: {suffix_count - 29}")
        print(f"PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞: {prefix_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        print(f"–í–°–ï–ì–û: {total_queries} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"")
        print(f"–ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_candidates)}")
        print(f"–í–∞–ª–∏–¥–Ω—ã—Ö PREFIX: {verified_count}")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π: {len(all_keywords)}")
        print(f"")
        
        if len(all_candidates) > 0:
            print(f"–≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨:")
            print(f"  –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {len(all_candidates)/suffix_count:.2f}")
            print(f"  –í–∞–ª–∏–¥–∞—Ü–∏—è: {verified_count}/{len(all_candidates)} = {verified_count/len(all_candidates)*100:.1f}%")
            print(f"  –ö–ª—é—á–µ–π –Ω–∞ –∑–∞–ø—Ä–æ—Å: {len(all_keywords)/total_queries:.2f}")
        print(f"")
        
        if len(new_from_morphology) > 0:
            print(f"‚úÖ –ú–û–†–§–û–õ–û–ì–ò–Ø –î–ê–õ–ê –†–ï–ó–£–õ–¨–¢–ê–¢!")
            print(f"–ù–æ–≤—ã—Ö —Å–ª–æ–≤: {len(new_from_morphology)} (+{len(new_from_morphology)/len(base_form_words)*100:.1f}%)")
        else:
            print(f"‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –Ω–µ –¥–∞–ª–∞ –Ω–æ–≤—ã—Ö —Å–ª–æ–≤")
        
        print(f"{'='*60}\n")
        
        return list(all_keywords)


@app.get("/api/test-parser/morphology")
async def test_morphology(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru")
):
    parser = AutocompleteParser()
    start = time.time()
    keywords = await parser.morphological_adaptive_test(seed, country, language)
    return {
        "seed": seed,
        "method": "Morphological ADAPTIVE (Fixed)",
        "keywords": keywords,
        "count": len(keywords),
        "time": round(time.time() - start, 2)
    }


@app.get("/")
async def root():
    return {
        "api": "Morphological ADAPTIVE Test (Fixed)",
        "url": "/api/test-parser/morphology?seed=—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤&country=UA&language=ru"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
