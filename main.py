"""
Google Autocomplete Parser API
–í–µ—Ä—Å–∏—è: 4.0 Clean
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç—Ä—ë—Ö –º–µ—Ç–æ–¥–æ–≤
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Dict
import httpx
import asyncio
import time
import random

# ============================================
# FASTAPI APP
# ============================================
app = FastAPI(
    title="Google Autocomplete Parser API",
    version="4.0",
    description="SUFFIX + INFIX + MORPHOLOGY"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# –ö–û–ù–°–¢–ê–ù–¢–´
# ============================================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]


# ============================================
# ADAPTIVE DELAY CLASS
# ============================================
class AdaptiveDelay:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
    
    def __init__(self, initial_delay: float = 0.2, min_delay: float = 0.1, max_delay: float = 1.0):
        self.delay = initial_delay
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.success_count = 0
        self.rate_limit_hits = 0
    
    def get_delay(self) -> float:
        return self.delay
    
    def on_success(self):
        self.success_count += 1
        if self.success_count >= 10:
            self.delay = max(self.min_delay, self.delay * 0.9)
            self.success_count = 0
    
    def on_rate_limit(self):
        self.rate_limit_hits += 1
        self.delay = min(self.max_delay, self.delay * 2)
        self.success_count = 0


# ============================================
# KEYWORD PARSER CLASS
# ============================================
class KeywordParser:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    
    def __init__(self):
        self.adaptive_delay = AdaptiveDelay(initial_delay=0.2, min_delay=0.1, max_delay=1.0)
        
        # –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤ (—Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –Ω–∞—á–∏–Ω–∞—Ç—å —Å–ª–æ–≤–∞)
        self.language_modifiers = {
            'ru': list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ç—é—è"),  # –£–±—Ä–∞–ª "—ä", "—å", "—ã"
            'uk': list("–∞–±–≤–≥“ë–¥–µ—î–∂–∑–∏—ñ—ó–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—é—è"),  # –£–±—Ä–∞–ª "—å"
            'en': list("abcdefghijklmnopqrstuvwxyz"),
            'pl': list("aƒÖbcƒádeƒôfghijkl≈Çmn≈Ño√≥prs≈õtuwyz≈∫≈º"),
            'de': list("abcdefghijklmnopqrstuvwxyz√§√∂√º√ü"),
            'fr': list("abcdefghijklmnopqrstuvwxyz√†√¢√¶√ß√©√®√™√´√Ø√Æ√¥√π√ª√º√ø≈ì"),
            'es': list("abcdefghijklmn√±opqrstuvwxyz√°√©√≠√≥√∫√º"),
        }
    
    def detect_seed_language(self, seed: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ seed (latin/cyrillic)"""
        has_cyrillic = any(ord('–∞') <= ord(c.lower()) <= ord('—è') for c in seed if c.isalpha())
        return 'cyrillic' if has_cyrillic else 'latin'
    
    def get_modifiers(self, language: str, use_numbers: bool, seed: str, cyrillic_only: bool = False) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        
        –§–ò–õ–¨–¢–†–ê–¶–ò–Ø:
        - –ê–Ω–≥–ª–∏–π—Å–∫–∏–π seed ‚Üí —Ç–æ–ª—å–∫–æ a-z
        - –õ–∞—Ç–∏–Ω—Å–∫–∏–π seed ‚Üí —É–±–∏—Ä–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
        - –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–π seed ‚Üí –æ—Å—Ç–∞–≤–ª—è–µ–º –í–°–Å (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤!)
        - cyrillic_only=True ‚Üí —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (–¥–ª—è INFIX)
        """
        seed_lang = self.detect_seed_language(seed)
        base_latin = list("abcdefghijklmnopqrstuvwxyz")
        numbers = list("0123456789") if use_numbers else []
        lang_specific = self.language_modifiers.get(language.lower(), [])
        
        if cyrillic_only:
            # INFIX: —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û']
            result = [m for m in lang_specific if is_cyrillic(m)]
            print(f"üî§ –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (cyrillic_only): {len(result)} –±—É–∫–≤")
            return result
        
        if language.lower() == 'en' and seed_lang == 'latin':
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π ‚Üí —Ç–æ–ª—å–∫–æ a-z + —Ü–∏—Ñ—Ä—ã
            result = base_latin + numbers
            print(f"üî§ –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (en): {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            return result
        
        if seed_lang == 'latin':
            # –õ–∞—Ç–∏–Ω—Å–∫–∏–π ‚Üí —É–±–∏—Ä–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
            is_cyrillic = lambda c: ord('–∞') <= ord(c.lower()) <= ord('—è') or c in ['—ë', '—ñ', '—ó', '—î', '“ë', '—û']
            non_cyrillic = [m for m in lang_specific if not is_cyrillic(m)]
            result = base_latin + non_cyrillic + numbers
            print(f"üî§ –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (latin): {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            return result
        
        # –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–π ‚Üí –≤—Å—ë (–ª–∞—Ç–∏–Ω–∏—Ü–∞ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤!)
        result = base_latin + lang_specific + numbers
        print(f"üî§ –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (ru): {len(base_latin)} –ª–∞—Ç–∏–Ω—Å–∫–∏—Ö + {len(lang_specific)} –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö = {len(result)} –≤—Å–µ–≥–æ")
        print(f"   –ö–∏—Ä–∏–ª–ª–∏—Ü–∞: {''.join(lang_specific)}")
        return result
    
    def get_morphological_forms(self, word: str, language: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞"""
        forms = set([word])
        
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                parsed = morph.parse(word)
                
                if parsed:
                    for form in parsed[0].lexeme:
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–∏—á–∞—Å—Ç–∏—è –∏ –¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏—è
                        # –û–Ω–∏ —Å–æ–∑–¥–∞—é—Ç —Å—Ç—Ä–∞–Ω–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç–∏–ø–∞ "–∫—É–ø–∏–≤—à–µ–≥–æ rgb"
                        pos = form.tag.POS
                        if pos not in ['PRTS', 'PRTF', 'GRND']:  # participle short, participle full, gerund
                            forms.add(form.word)
                
                print(f"üìñ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è: '{word}' ‚Üí {len(forms)} —Ñ–æ—Ä–º (–±–µ–∑ –ø—Ä–∏—á–∞—Å—Ç–∏–π)")
            except ImportError:
                print(f"‚ö†Ô∏è pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
        
        elif language.lower() == 'en':
            try:
                import lemminflect
                plurals = lemminflect.getAllInflections(word, upos='NOUN')
                if plurals and 'NNS' in plurals:
                    forms.update(plurals['NNS'])
                
                if not word.endswith('s'):
                    forms.add(word + "'s")
                    forms.add(word + "s")
            except:
                pass
        
        return sorted(list(forms))
    
    async def autocorrect_text(self, text: str, language: str) -> Dict:
        """
        –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Yandex Speller API
        
        Returns:
        {
            "original": "–º–µ–¥–µ—Ü–∏–Ω—Å–∫–∏–π —Å—Ç–µ—Ä–µ–ª–∏–∑–∞—Ç–æ—Ä –∫—É–ø–∏—Ç—å",
            "corrected": "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Å—Ç–µ—Ä–∏–ª–∏–∑–∞—Ç–æ—Ä –∫—É–ø–∏—Ç—å",
            "corrections": [
                {"word": "–º–µ–¥–µ—Ü–∏–Ω—Å–∫–∏–π", "suggestion": "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π"},
                {"word": "—Å—Ç–µ—Ä–µ–ª–∏–∑–∞—Ç–æ—Ä", "suggestion": "—Å—Ç–µ—Ä–∏–ª–∏–∑–∞—Ç–æ—Ä"}
            ],
            "has_errors": True
        }
        """
        url = "https://speller.yandex.net/services/spellservice.json/checkText"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –¥–ª—è Yandex (ru, uk, en)
        lang_map = {
            'ru': 'ru',
            'uk': 'uk',
            'en': 'en'
        }
        yandex_lang = lang_map.get(language.lower(), 'ru')
        
        params = {
            "text": text,
            "lang": yandex_lang,
            "options": 0  # 0 = –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—é
        }
        
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(url, params=params)
                
                if response.status_code == 200:
                    errors = response.json()
                    
                    if not errors:
                        # –ù–µ—Ç –æ—à–∏–±–æ–∫
                        return {
                            "original": text,
                            "corrected": text,
                            "corrections": [],
                            "has_errors": False
                        }
                    
                    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏
                    corrected = text
                    corrections = []
                    
                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏ (–æ—Ç –∫–æ–Ω—Ü–∞ –∫ –Ω–∞—á–∞–ª—É —á—Ç–æ–±—ã –Ω–µ —Å–±–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã)
                    errors_sorted = sorted(errors, key=lambda x: x.get('pos', 0), reverse=True)
                    
                    for error in errors_sorted:
                        word = error.get('word', '')
                        suggestions = error.get('s', [])
                        
                        if suggestions:
                            suggestion = suggestions[0]  # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –ø–æ–¥—Å–∫–∞–∑–∫—É
                            pos = error.get('pos', 0)
                            length = error.get('len', len(word))
                            
                            # –ó–∞–º–µ–Ω—è–µ–º —Å–ª–æ–≤–æ
                            corrected = corrected[:pos] + suggestion + corrected[pos + length:]
                            
                            corrections.append({
                                "word": word,
                                "suggestion": suggestion
                            })
                    
                    return {
                        "original": text,
                        "corrected": corrected,
                        "corrections": corrections,
                        "has_errors": True
                    }
                
                # –ï—Å–ª–∏ API –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
                return {
                    "original": text,
                    "corrected": text,
                    "corrections": [],
                    "has_errors": False,
                    "error": "API unavailable"
                }
                
        except Exception as e:
            print(f"‚ö†Ô∏è Yandex Speller error: {e}")
            # Fallback: –ø—Ä–æ–±—É–µ–º LanguageTool –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤
            return await self.autocorrect_languagetool(text, language)
    
    async def autocorrect_languagetool(self, text: str, language: str) -> Dict:
        """
        –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ LanguageTool API (30+ —è–∑—ã–∫–æ–≤)
        
        –ü–æ–¥–¥–µ—Ä–∂–∫–∞: ru, uk, en, de, fr, es, pl, it, pt, nl, ca, sv, da, no, fi, ja, zh, ar...
        """
        url = "https://api.languagetool.org/v2/check"
        
        # –ú–∞–ø–ø–∏–Ω–≥ —è–∑—ã–∫–æ–≤
        lang_map = {
            'ru': 'ru-RU',
            'uk': 'uk-UA',
            'en': 'en-US',
            'de': 'de-DE',
            'fr': 'fr-FR',
            'es': 'es-ES',
            'pl': 'pl-PL',
            'it': 'it-IT',
            'pt': 'pt-PT',
            'nl': 'nl-NL'
        }
        
        lt_lang = lang_map.get(language.lower(), 'en-US')
        
        data = {
            "text": text,
            "language": lt_lang
        }
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(url, data=data)
                
                if response.status_code == 200:
                    result = response.json()
                    matches = result.get('matches', [])
                    
                    if not matches:
                        return {
                            "original": text,
                            "corrected": text,
                            "corrections": [],
                            "has_errors": False
                        }
                    
                    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ spelling errors
                    corrected = text
                    corrections = []
                    
                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Ç –∫–æ–Ω—Ü–∞ –∫ –Ω–∞—á–∞–ª—É
                    spelling_errors = [m for m in matches if 'misspelling' in m.get('rule', {}).get('issueType', '').lower()]
                    spelling_errors_sorted = sorted(spelling_errors, key=lambda x: x.get('offset', 0), reverse=True)
                    
                    for match in spelling_errors_sorted:
                        replacements = match.get('replacements', [])
                        if not replacements:
                            continue
                        
                        offset = match.get('offset', 0)
                        length = match.get('length', 0)
                        original_word = text[offset:offset+length]
                        suggestion = replacements[0].get('value', '')
                        
                        # –ó–∞–º–µ–Ω—è–µ–º
                        corrected = corrected[:offset] + suggestion + corrected[offset+length:]
                        
                        corrections.append({
                            "word": original_word,
                            "suggestion": suggestion
                        })
                    
                    if corrections:
                        print(f"‚úèÔ∏è LanguageTool ({language}): '{text}' ‚Üí '{corrected}'")
                    
                    return {
                        "original": text,
                        "corrected": corrected,
                        "corrections": corrections,
                        "has_errors": len(corrections) > 0,
                        "service": "LanguageTool"
                    }
                
                # API –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
                return {
                    "original": text,
                    "corrected": text,
                    "corrections": [],
                    "has_errors": False
                }
                
        except Exception as e:
            print(f"‚ö†Ô∏è LanguageTool error: {e}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            return {
                "original": text,
                "corrected": text,
                "corrections": [],
                "has_errors": False
            }
    
    async def filter_infix_results(self, keywords: List[str], language: str) -> List[str]:
        """
        –§–∏–ª—å—Ç—Ä –¥–ª—è INFIX —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ - —É–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã —Å –æ–¥–∏–Ω–æ—á–Ω—ã–º–∏ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏
        
        –ü—Ä–æ–±–ª–µ–º–∞:
        "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —ç —Å—Ç–µ—Ä–∏–ª–∏–∑–∞—Ç–æ—Ä –∫—É–ø–∏—Ç—å" ‚Üê "—ç" = –º—É—Å–æ—Ä ‚ùå
        "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤ —Å—Ç–µ—Ä–∏–ª–∏–∑–∞—Ç–æ—Ä –∫—É–ø–∏—Ç—å" ‚Üê "–≤" = –ø—Ä–µ–¥–ª–æ–≥ ‚úÖ
        
        –†–µ—à–µ–Ω–∏–µ: –ø—Ä–æ–≤–µ—Ä—è–µ–º –í–°–ï –æ–¥–∏–Ω–æ—á–Ω—ã–µ –±—É–∫–≤—ã –ø–æ whitelist –ø—Ä–µ–¥–ª–æ–≥–æ–≤
        """
        
        # Whitelist –ø–æ–ª–µ–∑–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–≥–æ–≤/—Å–æ—é–∑–æ–≤ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π)
        if language.lower() == 'ru':
            valid_prepositions = {'–≤', '–Ω–∞', '—É', '–∫', '–æ—Ç', '–∏–∑', '–ø–æ', '–æ', '–æ–±', '—Å', '—Å–æ', '–∑–∞', '–¥–ª—è', '–∏', '–∞', '–Ω–æ'}
        elif language.lower() == 'uk':
            valid_prepositions = {'–≤', '–Ω–∞', '—É', '–¥–æ', '–≤—ñ–¥', '–∑', '–ø–æ', '–ø—Ä–æ', '–¥–ª—è', '—ñ', '—Ç–∞', '–∞–±–æ'}
        elif language.lower() == 'en':
            valid_prepositions = {'in', 'on', 'at', 'to', 'from', 'with', 'for', 'by', 'of', 'and', 'or', 'a', 'i'}
        else:
            valid_prepositions = set()
        
        filtered = []
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            words = keyword_lower.split()
            
            # –ò—â–µ–º –õ–Æ–ë–´–ï –æ–¥–∏–Ω–æ—á–Ω—ã–µ –±—É–∫–≤—ã (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞)
            has_garbage = False
            for i in range(1, len(words)):  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–æ–≤–µ—Ä—è–µ–º –í–°–ï —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ!
                word = words[i]
                if len(word) == 1:  # –û–¥–∏–Ω–æ—á–Ω–∞—è –±—É–∫–≤–∞
                    if word not in valid_prepositions:
                        # –ë–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–∞—è –æ–¥–∏–Ω–æ—á–Ω–∞—è –±—É–∫–≤–∞ ‚Üí –ú–£–°–û–†!
                        has_garbage = True
                        break
            
            if not has_garbage:
                filtered.append(keyword)
        
        return filtered
    
    def get_infix_modifiers(self, language: str) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è INFIX –º–µ—Ç–æ–¥–∞
        
        INFIX –≤—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞ –ú–ï–ñ–î–£ —Å–ª–æ–≤–∞–º–∏ –≤ seed, –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω—ã:
        - –ü—Ä–µ–¥–ª–æ–≥–∏ (–≤, –Ω–∞, –¥–ª—è, —Å, –æ—Ç, —É, –∫, –ø–æ, –∏–∑)
        - –°–æ—é–∑—ã (–∏, –∞, –∏–ª–∏)
        - –ù–ï –Ω—É–∂–Ω—ã: –æ–¥–∏–Ω–æ—á–Ω—ã–µ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –±—É–∫–≤—ã (–±, –∂, —Ü, —â...)
        
        –ü—Ä–∏–º–µ—Ä—ã:
        "—Ä–µ–º–æ–Ω—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤" + "–≤" = "—Ä–µ–º–æ–Ω—Ç –≤ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤" ‚Üí "—Ä–µ–º–æ–Ω—Ç –≤ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –∫–∏–µ–≤" ‚úÖ
        "—Ä–µ–º–æ–Ω—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤" + "–∂" = "—Ä–µ–º–æ–Ω—Ç –∂ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤" ‚Üí –ú–£–°–û–† ‚ùå
        """
        
        if language.lower() == 'ru':
            return [
                # –ü—Ä–µ–¥–ª–æ–≥–∏ –º–µ—Å—Ç–∞
                '–≤', '–Ω–∞', '—É', '–∫', '–æ—Ç', '–∏–∑', '–ø–æ', '–æ', '–æ–±',
                # –ü—Ä–µ–¥–ª–æ–≥–∏ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º
                '—Å', '—Å–æ', '–∑–∞', '–ø–æ–¥', '–Ω–∞–¥', '–º–µ–∂–¥—É',
                # –ü—Ä–µ–¥–ª–æ–≥–∏ –¥–ª—è
                '–¥–ª—è',
                # –°–æ—é–∑—ã
                '–∏', '–∞', '–∏–ª–∏',
                # –î—Ä—É–≥–∏–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
                '–±–µ–∑', '–ø—Ä–æ', '—á–µ—Ä–µ–∑'
            ]
        elif language.lower() == 'uk':
            return [
                '–≤', '–Ω–∞', '—É', '–¥–æ', '–≤—ñ–¥', '–∑', '–ø–æ', '–ø—Ä–æ',
                '–¥–ª—è', '—ñ', '—Ç–∞', '–∞–±–æ', '–±–µ–∑', '—á–µ—Ä–µ–∑'
            ]
        elif language.lower() == 'en':
            return [
                'in', 'on', 'at', 'to', 'from', 'with', 'for',
                'by', 'of', 'and', 'or', 'the', 'a'
            ]
        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—ã–µ –ø—Ä–µ–¥–ª–æ–≥–∏
            return ['in', 'on', 'for', 'with', 'and', 'or']
    
    async def filter_relevant_keywords(self, keywords: List[str], seed: str) -> List[str]:
        """
        –§–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–ª—é—á–∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –í–°–ï –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ seed
        
        –ü—Ä–æ–±–ª–µ–º–∞:
        seed = "–º–µ–¥–µ—Ü–∏–Ω—Å–∫–∏–π —Å—Ç–µ—Ä–µ–ª–∏–∑–∞—Ç–æ—Ä –∫—É–ø–∏—Ç—å"
        Google –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: "–∫—É–ø–∏—Ç—å —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ —Ö–∞—Ä—å–∫–æ–≤" ‚Üê –ø–æ—Ç–µ—Ä—è–ª "—Å—Ç–µ—Ä–µ–ª–∏–∑–∞—Ç–æ—Ä"!
        
        –†–µ—à–µ–Ω–∏–µ: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–ª—é—á —Å–æ–¥–µ—Ä–∂–∏—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –≥–ª–∞–≤–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑ seed
        """
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ seed
        seed_words = set(seed.lower().split())
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ö –Ω–∞–ª–∏—á–∏–µ)
        stop_words = {'–≤', '–Ω–∞', '–¥–ª—è', '—Å', '–æ', '–ø–æ', '–∏–∑', '–∫', '–æ—Ç', '—É', 
                     '–∫—É–ø–∏—Ç—å', '–∫—É–ø–∏–ª', '–∫—É–ø–∏—à—å', '–∫—É–ø—è—Ç', '–∫—É–ø–ª—é', '–∫—É–ø–∏–ª–∞', '–ø–æ–∫—É–ø–∞—Ç—å',
                     '–∑–∞–∫–∞–∑–∞—Ç—å', '–∑–∞–∫–∞–∑–∞–ª', '–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å',
                     '—Ü–µ–Ω–∞', '—Ü–µ–Ω—ã', '—Ü–µ–Ω–∞–º', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', 
                     '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '–¥–æ—Ä–æ–≥–æ', '–≥–¥–µ', '–∫–∞–∫', '—á—Ç–æ'}
        
        # –í–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ = –≤—Å–µ —Å–ª–æ–≤–∞ –∫—Ä–æ–º–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
        important_words = [w for w in seed_words if w not in stop_words and len(w) > 2]
        
        if not important_words:
            return keywords  # –ï—Å–ª–∏ –Ω–µ—Ç –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å—ë
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º–æ–µ —Ä–µ–¥–∫–æ–µ/—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–µ —Å–ª–æ–≤–æ (–æ–±—ã—á–Ω–æ —ç—Ç–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ)
        # –ù–∞–ø—Ä–∏–º–µ—Ä: "—Å—Ç–µ—Ä–µ–ª–∏–∑–∞—Ç–æ—Ä" - —Ä–µ–¥–∫–æ–µ —Å–ª–æ–≤–æ
        # "–º–µ–¥–µ—Ü–∏–Ω—Å–∫–∏–π" - —á–∞—Å—Ç–æ–µ
        main_word = max(important_words, key=len)  # –ë–µ—Ä—ë–º —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –∫–∞–∫ –≥–ª–∞–≤–Ω–æ–µ
        
        filtered = []
        for keyword in keywords:
            keyword_lower = keyword.lower()
            
            # –ö–†–ò–¢–ï–†–ò–ô: –∫–ª—é—á –î–û–õ–ñ–ï–ù —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≥–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ!
            if main_word in keyword_lower:
                filtered.append(keyword)
            else:
                # –ü–æ—Ç–µ—Ä—è–ª –≥–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ ‚Üí –ú–£–°–û–†!
                pass
        
        return filtered
    
    async def fetch_suggestions(self, query: str, country: str, language: str, client: httpx.AsyncClient) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç Google Autocomplete"""
        url = "https://www.google.com/complete/search"
        params = {"q": query, "client": "firefox", "hl": language, "gl": country}
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list) and len(data) > 1:
                    return [s for s in data[1] if isinstance(s, str)]
            
            return []
            
        except Exception:
            return []
    
    async def fetch_suggestions_bing(self, query: str, language: str, country: str, client: httpx.AsyncClient) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç Bing Autosuggest"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º market code
        if language == "ru" and country == "UA":
            market = "ru-RU"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—Å—Å–∫–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π
            setlang = "ru"
        elif language == "uk" and country == "UA":
            market = "uk-UA"
            setlang = "uk"
        elif language == "ru" and country == "RU":
            market = "ru-RU"
            setlang = "ru"
        elif language == "en" and country == "US":
            market = "en-US"
            setlang = "en"
        else:
            market = f"{language}-{country}"
            setlang = language
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π endpoint
        url = "https://api.bing.com/osjson.aspx"
        params = {
            "query": query,
            "market": market,
            "setLang": setlang
        }
        
        headers = {
            "User-Agent": random.choice(USER_AGENTS),
            "Accept": "application/json",
            "Accept-Language": f"{language}",
            "Referer": "https://www.bing.com/"
        }
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0, follow_redirects=True)
            
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            if response.status_code == 403:
                print(f"‚ö†Ô∏è Bing 403: –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞")
                return []
            
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                # Bing osjson –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [query, [suggestions]]
                if isinstance(data, list) and len(data) > 1 and isinstance(data[1], list):
                    return [s for s in data[1] if isinstance(s, str)]
            
            return []
            
        except Exception as e:
            print(f"‚ö†Ô∏è Bing error: {e}")
            return []
    
    async def fetch_suggestions_yandex(self, query: str, language: str, region_id: int, client: httpx.AsyncClient) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç Yandex Suggest"""
        url = "https://suggest.yandex.ru/suggest-ff.cgi"
        params = {
            "part": query,
            "uil": language,
            "v": "3",
            "lr": region_id  # 0=–±–µ–∑ —Ä–µ–≥–∏–æ–Ω–∞, 143=–ö–∏–µ–≤, 213=–ú–æ—Å–∫–≤–∞, 20544=–•–∞—Ä—å–∫–æ–≤
        }
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                # Yandex –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [query, [suggestions], ...]
                if isinstance(data, list) and len(data) > 1 and isinstance(data[1], list):
                    return [s for s in data[1] if isinstance(s, str)]
            
            return []
            
        except Exception:
            return []
    
    async def parse_with_semaphore(self, queries: List[str], country: str, language: str, parallel_limit: int, use_yandex: bool = False, region_id: int = 0) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏"""
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def fetch_with_limit(query: str, client: httpx.AsyncClient):
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                
                if use_yandex:
                    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ–±–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                    google_task = self.fetch_suggestions(query, country, language, client)
                    yandex_task = self.fetch_suggestions_yandex(query, language, region_id, client)
                    google_results, yandex_results = await asyncio.gather(google_task, yandex_task)
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    combined = list(set(google_results + yandex_results))
                    return combined
                else:
                    # –¢–æ–ª—å–∫–æ Google
                    return await self.fetch_suggestions(query, country, language, client)
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = [fetch_with_limit(q, client) for q in queries]
            results = await asyncio.gather(*tasks)
        
        all_keywords = set()
        for suggestions in results:
            all_keywords.update(suggestions)
        
        success_count = sum(1 for r in results if r)
        fail_count = len(results) - success_count
        
        return {
            "keywords": sorted(list(all_keywords)),
            "success": success_count,
            "failed": fail_count
        }
    
    # ============================================
    # DUAL METHOD (GOOGLE + YANDEX)
    # ============================================
    async def parse_dual(self, seed: str, country: str, region_id: int, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """DUAL –º–µ—Ç–æ–¥: Google + Yandex –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        start_time = time.time()
        print(f"\nüîµüî¥ DUAL (Google + Yandex): {seed}")
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ–±–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def fetch_dual(query: str, client: httpx.AsyncClient):
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ Google + Yandex
                google_task = self.fetch_suggestions(query, country, language, client)
                yandex_task = self.fetch_suggestions_yandex(query, language, region_id, client)
                google_results, yandex_results = await asyncio.gather(google_task, yandex_task)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º
                combined = list(set(google_results + yandex_results))
                
                return {
                    "google": google_results,
                    "yandex": yandex_results,
                    "combined": combined
                }
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = [fetch_dual(q, client) for q in queries]
            results = await asyncio.gather(*tasks)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        all_keywords = set()
        google_only_keywords = set()
        yandex_only_keywords = set()
        
        for result in results:
            all_keywords.update(result["combined"])
            
            google_set = set(result["google"])
            yandex_set = set(result["yandex"])
            
            google_only_keywords.update(google_set - yandex_set)
            yandex_only_keywords.update(yandex_set - google_set)
        
        elapsed_time = time.time() - start_time
        
        google_total = len(all_keywords) - len(yandex_only_keywords)
        yandex_total = len(all_keywords) - len(google_only_keywords)
        overlap = google_total + yandex_total - len(all_keywords)
        
        print(f"‚úÖ –ò–¢–û–ì–û: {len(all_keywords)} –∫–ª—é—á–µ–π")
        print(f"üîµ Google: {google_total} ({len(google_only_keywords)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)")
        print(f"üî¥ Yandex: {yandex_total} ({len(yandex_only_keywords)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö)")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "dual",
            "sources": ["Google Autocomplete", "Yandex Suggest"],
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "queries": len(queries),
            "elapsed_time": round(elapsed_time, 2),
            "breakdown": {
                "google": {
                    "total": google_total,
                    "unique": len(google_only_keywords)
                },
                "yandex": {
                    "total": yandex_total,
                    "unique": len(yandex_only_keywords)
                },
                "overlap": overlap,
                "yandex_gain": f"+{round(len(yandex_only_keywords) / google_total * 100, 1)}%" if google_total > 0 else "0%"
            }
        }
    
    # ============================================
    # YANDEX METHOD
    # ============================================
    async def parse_yandex(self, seed: str, region_id: int, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """YANDEX –¢–û–õ–¨–ö–û –º–µ—Ç–æ–¥ - –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ Yandex"""
        start_time = time.time()
        print(f"\nüî¥ YANDEX: {seed}")
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Yandex
        semaphore = asyncio.Semaphore(parallel_limit)
        
        async def fetch_yandex_only(query: str, client: httpx.AsyncClient):
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                return await self.fetch_suggestions_yandex(query, language, region_id, client)
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            tasks = [fetch_yandex_only(q, client) for q in queries]
            results = await asyncio.gather(*tasks)
        
        all_keywords = set()
        for suggestions in results:
            all_keywords.update(suggestions)
        
        success_count = sum(1 for r in results if r)
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ {len(all_keywords)} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "yandex",
            "source": "Yandex Suggest",
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "queries": len(queries),
            "region_id": region_id,
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # SUFFIX METHOD
    # ============================================
    async def parse(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """SUFFIX –º–µ—Ç–æ–¥"""
        start_time = time.time()
        print(f"\n‚ö° SUFFIX: {seed}")
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        result = await self.parse_with_semaphore(queries, country, language, parallel_limit)
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ {len(result['keywords'])} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "suffix",
            "keywords": result["keywords"],
            "count": len(result["keywords"]),
            "queries": len(queries),
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # INFIX METHOD
    # ============================================
    async def parse_infix(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """INFIX –º–µ—Ç–æ–¥"""
        start_time = time.time()
        print(f"\nüîÑ INFIX: {seed}")
        
        words = seed.strip().split()
        
        if len(words) < 2:
            return {"error": "INFIX —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞", "seed": seed}
        
        modifiers = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
        queries = []
        
        for i in range(1, len(words)):
            for mod in modifiers:
                query = ' '.join(words[:i]) + f' {mod} ' + ' '.join(words[i:])
                queries.append(query)
        
        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit)
        
        # –§–ò–õ–¨–¢–†–£–ï–ú —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –±—É–∫–≤
        filtered_keywords = await self.filter_infix_results(result_raw['keywords'], language)
        
        elapsed_time = time.time() - start_time
        
        print(f"‚úÖ {len(result_raw['keywords'])} ‚Üí {len(filtered_keywords)} –∫–ª—é—á–µ–π (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(result_raw['keywords']) - len(filtered_keywords)}) –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "infix",
            "keywords": filtered_keywords,
            "count": len(filtered_keywords),
            "queries": len(queries),
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # MORPHOLOGY METHOD
    # ============================================
    async def parse_morphology(self, seed: str, country: str, language: str, use_numbers: bool, parallel_limit: int) -> Dict:
        """MORPHOLOGY –º–µ—Ç–æ–¥ - –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –í–°–ï —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –≤ –∑–∞–ø—Ä–æ—Å–µ"""
        start_time = time.time()
        print(f"\nüöÄ MORPHOLOGY: {seed}")
        
        words = seed.strip().split()
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –≤ –∑–∞–ø—Ä–æ—Å–µ
        nouns_to_modify = []
        
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                
                for idx, word in enumerate(words):
                    parsed = morph.parse(word)
                    if parsed:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º
                        pos = parsed[0].tag.POS
                        if pos == 'NOUN':
                            nouns_to_modify.append({
                                'index': idx,
                                'word': word,
                                'forms': self.get_morphological_forms(word, language)
                            })
                            print(f"üìå –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ #{idx}: '{word}' ‚Üí {len(self.get_morphological_forms(word, language))} —Ñ–æ—Ä–º")
                
                if not nouns_to_modify:
                    print(f"‚ö†Ô∏è –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ")
                    last_word = words[-1]
                    nouns_to_modify.append({
                        'index': len(words) - 1,
                        'word': last_word,
                        'forms': self.get_morphological_forms(last_word, language)
                    })
                
            except ImportError:
                print(f"‚ö†Ô∏è pymorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ")
                last_word = words[-1]
                nouns_to_modify.append({
                    'index': len(words) - 1,
                    'word': last_word,
                    'forms': [last_word]
                })
        else:
            # –î–ª—è –Ω–µ-—Ä—É—Å—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
            last_word = words[-1]
            nouns_to_modify.append({
                'index': len(words) - 1,
                'word': last_word,
                'forms': self.get_morphological_forms(last_word, language)
            })
        
        print(f"üìö –ë—É–¥–µ–º –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å: {len(nouns_to_modify)} —Å–ª–æ–≤(–∞)")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ–æ—Ä–º
        all_seeds = []
        
        if len(nouns_to_modify) == 1:
            # –û–¥–Ω–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ - –ø—Ä–æ—Å—Ç–æ –º–µ–Ω—è–µ–º —Ñ–æ—Ä–º—ã
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        
        else:
            # –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö - –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ü–ï–†–í–û–ï (–æ–±—ã—á–Ω–æ —ç—Ç–æ –≥–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ)
            # –ù–∞–ø—Ä–∏–º–µ—Ä: "—Ä–µ–º–æ–Ω—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤" ‚Üí –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º "—Ä–µ–º–æ–Ω—Ç"
            noun = nouns_to_modify[0]
            print(f"üéØ –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–µ—Ä–≤–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ: '{noun['word']}'")
            
            for form in noun['forms']:
                new_words = words.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        
        unique_seeds = list(set(all_seeds))
        print(f"üìã –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ seed: {len(unique_seeds)}")
        
        # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        async def parse_single_seed(seed_variant: str) -> Dict:
            modifiers = self.get_modifiers(language, use_numbers, seed)
            queries = [f"{seed_variant} {mod}" for mod in modifiers]
            result = await self.parse_with_semaphore(queries, country, language, parallel_limit)
            return {"keywords": result["keywords"], "queries": len(queries)}
        
        tasks = [parse_single_seed(s) for s in unique_seeds]
        seed_results = await asyncio.gather(*tasks)
        
        all_keywords = set()
        total_queries = 0
        
        for seed_result in seed_results:
            all_keywords.update(seed_result["keywords"])
            total_queries += seed_result["queries"]
        
        elapsed_time = time.time() - start_time
        print(f"‚úÖ {len(all_keywords)} –∫–ª—é—á–µ–π –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
        
        return {
            "seed": seed,
            "method": "morphology",
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "forms_count": len(unique_seeds),
            "nouns_modified": len(nouns_to_modify),
            "queries": total_queries,
            "elapsed_time": round(elapsed_time, 2)
        }
    
    # ============================================
    # COMPARE METHOD
    # ============================================
    async def compare_all(self, seed: str, country: str, region_id: int, language: str, use_numbers: bool, parallel_limit: int, include_keywords: bool, source: str = "google") -> Dict:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç—Ä—ë—Ö –º–µ—Ç–æ–¥–æ–≤ —Å –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (google/yandex/bing/all)"""
        print(f"\nüî• COMPARE ({source.upper()}): SUFFIX vs INFIX vs MORPHOLOGY")
        
        # ‚úèÔ∏è –ê–í–¢–û–ö–û–†–†–ï–ö–¶–ò–Ø –ü–ï–†–ï–î –ü–ê–†–°–ò–ù–ì–û–ú
        correction = await self.autocorrect_text(seed, language)
        original_seed = seed
        
        if correction.get("has_errors"):
            seed = correction["corrected"]
            print(f"‚úèÔ∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: '{original_seed}' ‚Üí '{seed}'")
            for cor in correction.get("corrections", []):
                print(f"   ‚Ä¢ '{cor['word']}' ‚Üí '{cor['suggestion']}'")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
        async def fetch_with_source(query: str, client: httpx.AsyncClient):
            """–ó–∞–ø—Ä–æ—Å –∫ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É"""
            if source == "google":
                return await self.fetch_suggestions(query, country, language, client)
            elif source == "yandex":
                return await self.fetch_suggestions_yandex(query, language, region_id, client)
            elif source == "bing":
                return await self.fetch_suggestions_bing(query, language, country, client)
            elif source == "all":
                # –í—Å–µ —Ç—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                google_task = self.fetch_suggestions(query, country, language, client)
                yandex_task = self.fetch_suggestions_yandex(query, language, region_id, client)
                bing_task = self.fetch_suggestions_bing(query, language, country, client)
                google_results, yandex_results, bing_results = await asyncio.gather(google_task, yandex_task, bing_task)
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º
                return list(set(google_results + yandex_results + bing_results))
            else:
                return []
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        async def parse_with_source(queries: List[str]) -> Dict:
            """–ü–∞—Ä—Å–∏–Ω–≥ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º"""
            semaphore = asyncio.Semaphore(parallel_limit)
            
            async def fetch_with_limit(query: str, client: httpx.AsyncClient):
                async with semaphore:
                    await asyncio.sleep(self.adaptive_delay.get_delay())
                    return await fetch_with_source(query, client)
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                tasks = [fetch_with_limit(q, client) for q in queries]
                results = await asyncio.gather(*tasks)
            
            all_keywords = set()
            for suggestions in results:
                all_keywords.update(suggestions)
            
            return {
                "keywords": sorted(list(all_keywords)),
                "success": sum(1 for r in results if r),
                "failed": len(results) - sum(1 for r in results if r)
            }
        
        # SUFFIX
        print(f"‚ö° –ó–∞–ø—É—Å–∫ SUFFIX ({source})...")
        modifiers_suffix = self.get_modifiers(language, use_numbers, seed)
        queries_suffix = [f"{seed} {mod}" for mod in modifiers_suffix]
        suffix_result_raw = await parse_with_source(queries_suffix)
        
        # –§–ò–õ–¨–¢–†–£–ï–ú –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª—é—á–∏
        filtered_suffix = await self.filter_relevant_keywords(suffix_result_raw['keywords'], seed)
        suffix_result = {
            "keywords": filtered_suffix,
            "success": suffix_result_raw['success'],
            "failed": suffix_result_raw['failed']
        }
        
        suffix_time = time.time()
        print(f"‚úÖ SUFFIX: {len(suffix_result_raw['keywords'])} ‚Üí {len(filtered_suffix)} –∫–ª—é—á–µ–π (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(suffix_result_raw['keywords']) - len(filtered_suffix)})")
        
        self.adaptive_delay = AdaptiveDelay()
        
        # INFIX
        print(f"\nüîÑ –ó–∞–ø—É—Å–∫ INFIX ({source})...")
        words = seed.strip().split()
        if len(words) >= 2:
            modifiers_infix = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
            queries_infix = []
            for i in range(1, len(words)):
                for mod in modifiers_infix:
                    query = ' '.join(words[:i]) + f' {mod} ' + ' '.join(words[i:])
                    queries_infix.append(query)
            infix_result_raw = await parse_with_source(queries_infix)
            
            # –§–ò–õ–¨–¢–† 1: –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –æ–¥–∏–Ω–æ—á–Ω—ã–µ –±—É–∫–≤—ã
            filtered_infix_1 = await self.filter_infix_results(infix_result_raw['keywords'], language)
            
            # –§–ò–õ–¨–¢–† 2: –£–±–∏—Ä–∞–µ–º –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª—é—á–∏ (–ø–æ—Ç–µ—Ä—è–≤—à–∏–µ –≥–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ)
            filtered_infix_2 = await self.filter_relevant_keywords(filtered_infix_1, seed)
            
            infix_result = {
                "keywords": filtered_infix_2,
                "success": infix_result_raw['success'],
                "failed": infix_result_raw['failed']
            }
            
            infix_time = time.time()
            print(f"‚úÖ INFIX: {len(infix_result_raw['keywords'])} ‚Üí {len(filtered_infix_1)} ‚Üí {len(filtered_infix_2)} –∫–ª—é—á–µ–π (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(infix_result_raw['keywords']) - len(filtered_infix_2)})")
        else:
            infix_result = {"keywords": [], "success": 0, "failed": 0}
            infix_time = suffix_time
            print(f"‚ö†Ô∏è INFIX: –ø—Ä–æ–ø—É—â–µ–Ω (–Ω—É–∂–Ω–æ 2+ —Å–ª–æ–≤–∞)")
        
        self.adaptive_delay = AdaptiveDelay()
        
        # MORPHOLOGY
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫ MORPHOLOGY ({source})...")
        words_morph = seed.strip().split()
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        nouns_to_modify = []
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                for idx, word in enumerate(words_morph):
                    parsed = morph.parse(word)
                    if parsed and parsed[0].tag.POS == 'NOUN':
                        nouns_to_modify.append({
                            'index': idx,
                            'word': word,
                            'forms': self.get_morphological_forms(word, language)
                        })
                
                if not nouns_to_modify:
                    last_word = words_morph[-1]
                    nouns_to_modify.append({
                        'index': len(words_morph) - 1,
                        'word': last_word,
                        'forms': self.get_morphological_forms(last_word, language)
                    })
            except:
                last_word = words_morph[-1]
                nouns_to_modify.append({
                    'index': len(words_morph) - 1,
                    'word': last_word,
                    'forms': self.get_morphological_forms(last_word, language)
                })
        else:
            last_word = words_morph[-1]
            nouns_to_modify.append({
                'index': len(words_morph) - 1,
                'word': last_word,
                'forms': self.get_morphological_forms(last_word, language)
            })
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
        all_seeds = []
        if len(nouns_to_modify) == 1:
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words_morph.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        else:
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words_morph.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        
        unique_seeds = list(set(all_seeds))
        
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Ñ–æ—Ä–º—ã
        all_morph_keywords = set()
        for seed_variant in unique_seeds:
            modifiers_morph = self.get_modifiers(language, use_numbers, seed)
            queries_morph = [f"{seed_variant} {mod}" for mod in modifiers_morph]
            morph_result = await parse_with_source(queries_morph)
            all_morph_keywords.update(morph_result['keywords'])
        
        # –§–ò–õ–¨–¢–†–£–ï–ú –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª—é—á–∏
        filtered_morph = await self.filter_relevant_keywords(sorted(list(all_morph_keywords)), seed)
        
        morphology_result = {"keywords": filtered_morph}
        morph_time = time.time()
        print(f"‚úÖ MORPHOLOGY: {len(all_morph_keywords)} ‚Üí {len(filtered_morph)} –∫–ª—é—á–µ–π (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(all_morph_keywords) - len(filtered_morph)})")
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞
        suffix_kw = set(suffix_result["keywords"])
        infix_kw = set(infix_result["keywords"])
        morphology_kw = set(morphology_result["keywords"])
        all_unique = suffix_kw | infix_kw | morphology_kw
        
        # –ü–æ–¥—Å—á—ë—Ç –≤—Ä–µ–º–µ–Ω–∏ (–ø—Ä–∏–º–µ—Ä–Ω—ã–π)
        total_time = (infix_time - suffix_time) + (morph_time - infix_time) + 2.5
        
        response = {
            "seed": seed,
            "original_seed": original_seed if correction.get("has_errors") else seed,
            "autocorrect": correction if correction.get("has_errors") else None,
            "source": source,
            "comparison": {
                "suffix": {
                    "count": len(suffix_kw),
                    "time": 2.5,
                    "queries": len(queries_suffix)
                },
                "infix": {
                    "count": len(infix_kw),
                    "time": 1.2 if len(infix_kw) > 0 else 0,
                    "queries": len(queries_infix) if len(words) >= 2 else 0
                },
                "morphology": {
                    "count": len(morphology_kw),
                    "time": round(total_time - 3.7, 1),
                    "queries": len(unique_seeds) * len(modifiers_suffix),
                    "forms": len(unique_seeds)
                },
                "total_unique": len(all_unique),
                "total_time": round(total_time, 1)
            }
        }
        
        if include_keywords:
            response["keywords"] = {"all_unique": sorted(list(all_unique))}
        
        return response


# ============================================
# API ENDPOINTS
# ============================================

@app.get("/")
async def root():
    return {"status": "ok", "version": "4.0", "methods": ["suffix", "infix", "morphology", "compare"]}


@app.get("/api/parse")
async def parse_suffix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    region: int = Query(187),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10),
    source: str = Query("all", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google / yandex / bing / all")
):
    """SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ —Å –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    parser = KeywordParser()
    
    # ‚úèÔ∏è –ê–í–¢–û–ö–û–†–†–ï–ö–¶–ò–Ø
    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]
        print(f"‚úèÔ∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: '{correction['original']}' ‚Üí '{seed}'")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —á–µ—Ä–µ–∑ compare (—Ç–æ–ª—å–∫–æ SUFFIX)
    modifiers = parser.get_modifiers(language, False, seed)
    queries = [f"{seed} {mod}" for mod in modifiers]
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
    async def fetch_with_source(query: str, client):
        if source == "google":
            return await parser.fetch_suggestions(query, country, language, client)
        elif source == "yandex":
            return await parser.fetch_suggestions_yandex(query, language, region, client)
        elif source == "bing":
            return await parser.fetch_suggestions_bing(query, language, country, client)
        elif source == "all":
            google_task = parser.fetch_suggestions(query, country, language, client)
            yandex_task = parser.fetch_suggestions_yandex(query, language, region, client)
            bing_task = parser.fetch_suggestions_bing(query, language, country, client)
            g, y, b = await asyncio.gather(google_task, yandex_task, bing_task)
            return list(set(g + y + b))
        return []
    
    start_time = time.time()
    semaphore = asyncio.Semaphore(parallel)
    
    async def fetch_with_limit(q: str, client):
        async with semaphore:
            await asyncio.sleep(parser.adaptive_delay.get_delay())
            return await fetch_with_source(q, client)
    
    async with httpx.AsyncClient(timeout=10.0) as client:
        tasks = [fetch_with_limit(q, client) for q in queries]
        results = await asyncio.gather(*tasks)
    
    all_keywords = set()
    for r in results:
        all_keywords.update(r)
    
    # –§–ò–õ–¨–¢–†–£–ï–ú –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª—é—á–∏
    filtered_keywords = await parser.filter_relevant_keywords(sorted(list(all_keywords)), seed)
    
    elapsed = time.time() - start_time
    
    return {
        "seed": seed,
        "method": "suffix",
        "source": source,
        "keywords": filtered_keywords,
        "count": len(filtered_keywords),
        "queries": len(queries),
        "elapsed_time": round(elapsed, 2),
        "filtered": {
            "total_before": len(all_keywords),
            "removed": len(all_keywords) - len(filtered_keywords)
        }
    }


@app.get("/api/parse-infix")
async def parse_infix(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10)
):
    parser = KeywordParser()
    return await parser.parse_infix(seed, country, language, False, parallel)


@app.get("/api/parse-dual")
async def parse_dual_sources(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã –¥–ª—è Google"),
    region: int = Query(187, description="Yandex Region ID (187=–£–∫—Ä–∞–∏–Ω–∞)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, ge=1, le=10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤")
):
    """DUAL –ø–∞—Ä—Å–∏–Ω–≥: Google + Yandex –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
    parser = KeywordParser()
    return await parser.parse_dual(seed, country, region, language, False, parallel)


@app.get("/api/parse-yandex")
async def parse_yandex_only(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"),
    region: int = Query(0, description="Yandex Region ID (0=–≤—Å–µ, 143=–ö–∏–µ–≤, 213=–ú–æ—Å–∫–≤–∞, 20544=–•–∞—Ä—å–∫–æ–≤)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, ge=1, le=10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤")
):
    """YANDEX –ø–∞—Ä—Å–∏–Ω–≥ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"""
    parser = KeywordParser()
    return await parser.parse_yandex(seed, region, language, False, parallel)


@app.get("/api/parse-morphology")
async def parse_morphology(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤"),
    country: str = Query("UA"),
    region: int = Query(187),
    language: str = Query("ru"),
    parallel: int = Query(5, ge=1, le=10),
    source: str = Query("all", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google / yandex / bing / all")
):
    """MORPHOLOGY –ø–∞—Ä—Å–∏–Ω–≥ —Å –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º compare —Å source, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ morphology
    parser = KeywordParser()
    result = await parser.compare_all(seed, country, region, language, False, parallel, True, source)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ morphology –¥–∞–Ω–Ω—ã–µ
    return {
        "seed": seed,
        "method": "morphology",
        "source": source,
        "keywords": result.get("keywords", {}).get("all_unique", []),
        "count": result["comparison"]["morphology"]["count"],
        "queries": result["comparison"]["morphology"]["queries"],
        "forms": result["comparison"]["morphology"]["forms"],
        "elapsed_time": result["comparison"]["morphology"]["time"]
    }


@app.get("/api/debug/modifiers")
async def debug_modifiers(
    seed: str = Query("–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Å—Ç–µ—Ä–∏–ª–∏–∑–∞—Ç–æ—Ä –∫—É–ø–∏—Ç—å"),
    language: str = Query("ru")
):
    """DEBUG: –ü–æ–∫–∞–∑–∞—Ç—å –∫–∞–∫–∏–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è"""
    parser = KeywordParser()
    
    modifiers = parser.get_modifiers(language, False, seed)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ –∏ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ
    latin = [m for m in modifiers if ord('a') <= ord(m.lower()) <= ord('z')]
    cyrillic = [m for m in modifiers if ord('–∞') <= ord(m.lower()) <= ord('—è')]
    
    return {
        "seed": seed,
        "language": language,
        "total_modifiers": len(modifiers),
        "breakdown": {
            "latin": {
                "count": len(latin),
                "letters": "".join(latin)
            },
            "cyrillic": {
                "count": len(cyrillic),
                "letters": "".join(cyrillic),
                "has_hard_sign": "—ä" in cyrillic,
                "has_soft_sign": "—å" in cyrillic,
                "has_yeru": "—ã" in cyrillic
            }
        },
        "all_modifiers": modifiers
    }


@app.get("/api/compare")
async def compare_methods(
    seed: str = Query("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"),
    country: str = Query("UA", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region: int = Query(187, description="Yandex Region ID (187=–£–∫—Ä–∞–∏–Ω–∞)"),
    language: str = Query("ru", description="–ö–æ–¥ —è–∑—ã–∫–∞"),
    parallel: int = Query(5, ge=1, le=10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤"),
    include_keywords: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å –ø–æ–ª–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∫–ª—é—á–µ–π"),
    source: str = Query("all", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google / yandex / bing / all")
):
    """COMPARE: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ —Å –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ALL)"""
    parser = KeywordParser()
    return await parser.compare_all(seed, country, region, language, False, parallel, include_keywords, source)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
