"""
# DEPLOYED: 2026-01-08 v5.2.8 (geonamescache - full database)
FGS Parser API - Version 5.2.8
–ú–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞: SUFFIX | INFIX | MORPHOLOGY | ADAPTIVE PREFIX | LIGHT SEARCH | DEEP SEARCH
–¢—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: Google + Yandex + Bing
–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è: Yandex Speller + LanguageTool

–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: 2026-01-08
+ –ì–∏–±—Ä–∏–¥–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (Pymorphy3 + Snowball + Fuzzy Matching)
+ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ 8 —è–∑—ã–∫–æ–≤: RU, UK, EN, DE, FR, ES, IT, PL
+ –¢—Ä—ë—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–ª–µ–º–º—ã + –æ—Ä–∏–≥–∏–Ω–∞–ª—ã + –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤)
+ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ñ–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–≥–∞–º - –æ—Ç—Å–µ–∫–∞–µ—Ç "–≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π —à—É–º"
+ EntityLogicManager —Å –¥–≤—É—Ö—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π (–¥–ª—è "–ª—å–≤–æ–≤")
+ –ü—Ä–µ-—Ñ–∏–ª—å—Ç—Ä: geonamescache –±–∞–∑–∞ (15000+ –≥–æ—Ä–æ–¥–æ–≤, –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è)
+ –†–µ–≥–∏–æ–Ω—ã –≤ –∫–µ—à–µ - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –æ–±–ª–∞—Å—Ç–µ–π
+ –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –£–†–û–í–ù–Ø 2 (Subset Matching)
"""

from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from typing import List, Dict
import httpx
import asyncio
import time
import random
import re
from difflib import SequenceMatcher

# NLTK –¥–ª—è —Å—Ç–µ–º–º–∏–Ω–≥–∞ (v5.2.0)
import nltk
from nltk.stem import SnowballStemmer

# Natasha –¥–ª—è NER (v5.2.4)
try:
    from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsNERTagger, Doc
    NATASHA_AVAILABLE = True
except ImportError:
    NATASHA_AVAILABLE = False
    print("‚ö†Ô∏è Natasha –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. EntityLogicManager –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å –∂—ë—Å—Ç–∫–∏–º –∫–µ—à–µ–º.")

# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö NLTK (–¥–ª—è Render.com)
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except Exception as e:
    print(f"Warning: Could not download NLTK data: {e}")

# Pymorphy3 –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ RU/UK (v5.2.0)
import pymorphy3

# ============================================
# FASTAPI APP
# ============================================
app = FastAPI(
    title="FGS Parser API",
    version="5.2.8",
    description="6 –º–µ—Ç–æ–¥–æ–≤ | 3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞ | Pre-filter: geonames (15k+ cities) | Level 2 Complete"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# –ö–û–ù–°–¢–ê–ù–¢–´
# ============================================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

# ============================================
# GEO BLACKLIST –î–õ–Ø –ü–†–ï-–§–ò–õ–¨–¢–†–ê (v5.2.8)
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑ geonamescache (15000+ –≥–æ—Ä–æ–¥–æ–≤)
# ============================================

def generate_geo_blacklist_full():
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ü–û–õ–ù–´–ô GEO_BLACKLIST –∏–∑ –±–∞–∑—ã geonames
    
    –ë–∞–∑–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç:
    - 15000+ –≥–æ—Ä–æ–¥–æ–≤ –º–∏—Ä–∞
    - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–ú–æ—Å–∫–≤–∞, Moscow, Maskva, –ú–°–ö...)
    - –ü—Ä–∏–≤—è–∑–∫–∞ –∫ —Å—Ç—Ä–∞–Ω–∞–º —á–µ—Ä–µ–∑ countrycode
    
    –ë–ï–ó —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –Ω–∞—Å–µ–ª–µ–Ω–∏—é - –±–µ—Ä—ë–º –í–°–ï –≥–æ—Ä–æ–¥–∞!
    
    Returns:
        dict: {
            'ua': [—Å–ø–∏—Å–æ–∫ —á—É–∂–∏—Ö –≥–æ—Ä–æ–¥–æ–≤ –¥–ª—è –£–∫—Ä–∞–∏–Ω—ã],
            'ru': [—Å–ø–∏—Å–æ–∫ —á—É–∂–∏—Ö –≥–æ—Ä–æ–¥–æ–≤ –¥–ª—è –†–æ—Å—Å–∏–∏],
            ...
        }
    """
    try:
        from geonamescache import GeonamesCache
        
        gc = GeonamesCache()
        cities = gc.get_cities()
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –≥–æ—Ä–æ–¥–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º
        cities_by_country = {}
        
        for city_id, city_data in cities.items():
            country = city_data['countrycode']
            
            if country not in cities_by_country:
                cities_by_country[country] = set()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
            name = city_data['name'].lower()
            cities_by_country[country].add(name)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            for alt in city_data.get('alternatenames', []):
                # –§–∏–ª—å—Ç—Ä—É–µ–º: —Ç–æ–ª—å–∫–æ —á–∏—Ç–∞–µ–º—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–Ω–µ –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã)
                if alt and len(alt) <= 30:
                    # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –±—É–∫–≤—ã
                    if any(c.isalpha() for c in alt):
                        cities_by_country[country].add(alt.lower())
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º blacklist
        blacklist = {}
        
        # –£–∫—Ä–∞–∏–Ω–∞ –±–ª–æ–∫–∏—Ä—É–µ—Ç: –†–æ—Å—Å–∏—é + –ë–µ–ª–∞—Ä—É—Å—å + –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω + —Å–æ—Å–µ–¥–µ–π
        blacklist['ua'] = (
            cities_by_country.get('RU', set()) |  # –†–æ—Å—Å–∏—è
            cities_by_country.get('BY', set()) |  # –ë–µ–ª–∞—Ä—É—Å—å
            cities_by_country.get('KZ', set()) |  # –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω
            cities_by_country.get('PL', set()) |  # –ü–æ–ª—å—à–∞ (–©–µ—Ü–∏–Ω!)
            cities_by_country.get('LT', set()) |  # –õ–∏—Ç–≤–∞
            cities_by_country.get('LV', set()) |  # –õ–∞—Ç–≤–∏—è
            cities_by_country.get('EE', set())    # –≠—Å—Ç–æ–Ω–∏—è
        )
        
        # –†–æ—Å—Å–∏—è –±–ª–æ–∫–∏—Ä—É–µ—Ç: –£–∫—Ä–∞–∏–Ω—É + –ë–µ–ª–∞—Ä—É—Å—å + –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω
        blacklist['ru'] = (
            cities_by_country.get('UA', set()) |
            cities_by_country.get('BY', set()) |
            cities_by_country.get('KZ', set())
        )
        
        # –ë–µ–ª–∞—Ä—É—Å—å –±–ª–æ–∫–∏—Ä—É–µ—Ç: –†–æ—Å—Å–∏—é + –£–∫—Ä–∞–∏–Ω—É
        blacklist['by'] = (
            cities_by_country.get('RU', set()) |
            cities_by_country.get('UA', set())
        )
        
        # –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω –±–ª–æ–∫–∏—Ä—É–µ—Ç: –†–æ—Å—Å–∏—é + –£–∫—Ä–∞–∏–Ω—É + –ë–µ–ª–∞—Ä—É—Å—å
        blacklist['kz'] = (
            cities_by_country.get('RU', set()) |
            cities_by_country.get('UA', set()) |
            cities_by_country.get('BY', set())
        )
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print("‚úÖ GEO_BLACKLIST —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏–∑ geonames:")
        for country, cities_set in blacklist.items():
            print(f"   {country.upper()}: {len(cities_set)} –≥–æ—Ä–æ–¥–æ–≤ –≤ blacklist")
        
        return blacklist
        
    except ImportError:
        # –ï—Å–ª–∏ geonamescache –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - fallback –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫
        print("‚ö†Ô∏è geonamescache –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π blacklist")
        return {
            "ua": {"–º–æ—Å–∫–≤–∞", "–º—Å–∫", "—Å–ø–±", "–ø–∏—Ç–µ—Ä", "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–º–∏–Ω—Å–∫"},
            "ru": {"–∫–∏–µ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–¥–Ω–µ–ø—Ä", "–ª—å–≤–æ–≤", "–æ–¥–µ—Å—Å–∞"},
            "by": {"–º–æ—Å–∫–≤–∞", "—Å–ø–±", "–∫–∏–µ–≤", "—Ö–∞—Ä—å–∫–æ–≤"},
            "kz": {"–º–æ—Å–∫–≤–∞", "—Å–ø–±", "–∫–∏–µ–≤"}
        }

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è (–æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ)
GEO_BLACKLIST = generate_geo_blacklist_full()


# ============================================
# ADAPTIVE DELAY
# ============================================
class AdaptiveDelay:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
    
    def __init__(self, initial_delay: float = 0.2, min_delay: float = 0.1, max_delay: float = 1.0):
        self.delay = initial_delay
        self.min_delay = min_delay
        self.max_delay = max_delay
    
    def get_delay(self) -> float:
        return self.delay
    
    def on_success(self):
        self.delay = max(self.min_delay, self.delay * 0.95)
    
    def on_rate_limit(self):
        self.delay = min(self.max_delay, self.delay * 1.5)


# ============================================
# ENTITY CONFLICT DETECTION (v5.2.4)
# ============================================
class EntityLogicManager:
    """
    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ entities (–≥–æ—Ä–æ–¥–∞, –±—Ä–µ–Ω–¥—ã)
    
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
    1. –ñ—ë—Å—Ç–∫–∏–π –∫–µ—à –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö entities (O(1) - –º–≥–Ω–æ–≤–µ–Ω–Ω–æ)
    2. Natasha NER –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ RU, –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞)
    3. –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    
    –ü—Ä–∏–º–µ—Ä—ã:
        >>> manager = EntityLogicManager()
        >>> manager.check_conflict("—Ä–µ–º–æ–Ω—Ç –¥–Ω–µ–ø—Ä", "—Ä–µ–º–æ–Ω—Ç –∫–∏–µ–≤", "ru")
        True  # –ö–æ–Ω—Ñ–ª–∏–∫—Ç –≥–æ—Ä–æ–¥–æ–≤!
        
        >>> manager.check_conflict("—Ä–µ–º–æ–Ω—Ç –¥–Ω–µ–ø—Ä", "—Ä–µ–º–æ–Ω—Ç –¥–Ω–µ–ø—Ä –Ω–µ–¥–æ—Ä–æ–≥–æ", "ru")
        False  # –ù–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞
    """
    
    def __init__(self):
        # –ö–µ—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (seed –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –º–Ω–æ–≥–æ —Ä–∞–∑)
        self.cache = {}
        
        # Pymorphy3 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤ (v5.2.5)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è "–∫–∏–µ–≤–µ" ‚Üí "–∫–∏–µ–≤"
        try:
            import pymorphy3
            self.morph_ru = pymorphy3.MorphAnalyzer(lang='ru')
            self.morph_uk = pymorphy3.MorphAnalyzer(lang='uk')
            self.morph_available = True
        except Exception as e:
            print(f"‚ö†Ô∏è Pymorphy3 –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è EntityLogicManager: {e}")
            self.morph_available = False
        
        # –ñ—ë—Å—Ç–∫–∏–π –∫–µ—à –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö entities (O(1) –ø—Ä–æ–≤–µ—Ä–∫–∞)
        self.hard_cache = {
            'LOC': {
                # –£–∫—Ä–∞–∏–Ω–∞ - –≥–æ—Ä–æ–¥–∞
                "–∫–∏–µ–≤", "–¥–Ω–µ–ø—Ä", "–¥–Ω–µ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å–∫", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–ª—å–≤–æ–≤", 
                "–∑–∞–ø–æ—Ä–æ–∂—å–µ", "–¥–æ–Ω–µ—Ü–∫", "–∫—Ä–∏–≤–æ–π —Ä–æ–≥", "–Ω–∏–∫–æ–ª–∞–µ–≤", "–ª—É–≥–∞–Ω—Å–∫", 
                "–≤–∏–Ω–Ω–∏—Ü–∞", "—Ö–µ—Ä—Å–æ–Ω", "–ø–æ–ª—Ç–∞–≤–∞", "—á–µ—Ä–Ω–∏–≥–æ–≤", "—á–µ—Ä–∫–∞—Å—Å—ã",
                "–∂–∏—Ç–æ–º–∏—Ä", "—Å—É–º—ã", "—Ö–º–µ–ª—å–Ω–∏—Ü–∫–∏–π", "—Ä–æ–≤–Ω–æ", "–∏–≤–∞–Ω–æ-—Ñ—Ä–∞–Ω–∫–æ–≤—Å–∫",
                "—Ç–µ—Ä–Ω–æ–ø–æ–ª—å", "–ª—É—Ü–∫", "—É–∂–≥–æ—Ä–æ–¥", "—á–µ—Ä–Ω–æ–≤—Ü—ã",
                # –£–∫—Ä–∞–∏–Ω–∞ - –æ–±–ª–∞—Å—Ç–∏ –∏ —Ä–µ–≥–∏–æ–Ω—ã (v5.2.6)
                "–¥–Ω–µ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–¥–Ω–µ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å–∫–∞—è",
                "—Ö–∞—Ä—å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "—Ö–∞—Ä—å–∫–æ–≤—Å–∫–∞—è",
                "–∫–∏–µ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–∫–∏–µ–≤—Å–∫–∞—è",
                "–æ–¥–µ—Å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–æ–¥–µ—Å—Å–∫–∞—è",
                "–ª—å–≤–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ª—å–≤–æ–≤—Å–∫–∞—è",
                "–∑–∞–ø–æ—Ä–æ–∂—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–∑–∞–ø–æ—Ä–æ–∂—Å–∫–∞—è",
                "–¥–æ–Ω–µ—Ü–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–¥–æ–Ω–µ—Ü–∫–∞—è",
                "–ª—É–≥–∞–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ª—É–≥–∞–Ω—Å–∫–∞—è",
                "–∫—Ä—ã–º", "–¥–æ–Ω–±–∞—Å—Å", "–∑–∞–∫–∞—Ä–ø–∞—Ç—å–µ",
                # –†–æ—Å—Å–∏—è - –≥–æ—Ä–æ–¥–∞
                "–º–æ—Å–∫–≤–∞", "—Å–ø–±", "–ø–∏—Ç–µ—Ä", "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", 
                "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–∫–∞–∑–∞–Ω—å", "–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä", "–≤–æ—Ä–æ–Ω–µ–∂", "—Å–∞–º–∞—Ä–∞", 
                "—Ä–æ—Å—Ç–æ–≤", "—É—Ñ–∞", "—á–µ–ª—è–±–∏–Ω—Å–∫", "–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥", "–æ–º—Å–∫",
                "–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫", "–ø–µ—Ä–º—å", "–≤–æ–ª–≥–æ–≥—Ä–∞–¥", "—Å–∞—Ä–∞—Ç–æ–≤", "—Ç—é–º–µ–Ω—å",
                # –†–æ—Å—Å–∏—è - —Ä–µ–≥–∏–æ–Ω—ã (v5.2.6)
                "–º–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–º–æ—Å–∫–æ–≤—Å–∫–∞—è",
                "–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è",
                "–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä—Å–∫–∏–π –∫—Ä–∞–π", "–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä—Å–∫–∏–π",
                "–ø–æ–¥–º–æ—Å–∫–æ–≤—å–µ",
                # –ë–µ–ª–∞—Ä—É—Å—å
                "–º–∏–Ω—Å–∫", "–≥–æ–º–µ–ª—å", "–º–æ–≥–∏–ª–µ–≤", "–≤–∏—Ç–µ–±—Å–∫", "–≥—Ä–æ–¥–Ω–æ", "–±—Ä–µ—Å—Ç",
                # –î—Ä—É–≥–∏–µ
                "–∞—Å—Ç–∞–Ω–∞", "–∞–ª–º–∞—Ç—ã", "—Ç–∞—à–∫–µ–Ω—Ç", "—Ç–±–∏–ª–∏—Å–∏", "–µ—Ä–µ–≤–∞–Ω", "–±–∞–∫—É",
                "–∫–∏—à–∏–Ω–µ–≤", "—Ä–∏–≥–∞", "—Ç–∞–ª–ª–∏–Ω", "–≤–∏–ª—å–Ω—é—Å", "–ø—Ä–∞–≥–∞", "–≤–∞—Ä—à–∞–≤–∞",
                # –ö—Ä—ã–º - –≥–æ—Ä–æ–¥–∞
                "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å", "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å", "—è–ª—Ç–∞", "–µ–≤–ø–∞—Ç–æ—Ä–∏—è", "–∫–µ—Ä—á—å", "—Ñ–µ–æ–¥–æ—Å–∏—è",
                # –ö–∏–ø—Ä
                "–ª–∏–º–∞—Å—Å–æ–ª", "–Ω–∏–∫–æ—Å–∏—è", "–ª–∞—Ä–Ω–∞–∫–∞", "–ø–∞—Ñ–æ—Å"
            },
            'ORG': {
                # –ë—Ä–µ–Ω–¥—ã —Ç–µ—Ö–Ω–∏–∫–∏
                "apple", "samsung", "xiaomi", "lg", "sony", "bosch",
                "philips", "panasonic", "nokia", "huawei", "lenovo",
                "dell", "hp", "asus", "acer", "msi", "intel", "amd",
                # –ë—ã—Ç–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞
                "dyson", "karcher", "thomas", "electrolux", "siemens",
                "ariston", "indesit", "candy", "zanussi", "beko",
                "gorenje", "whirlpool", "hotpoint", "miele", "aeg"
            }
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Natasha (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞)
        self.natasha_available = NATASHA_AVAILABLE
        if self.natasha_available:
            try:
                self.segmenter = Segmenter()
                self.morph_vocab = MorphVocab()
                self.emb = NewsEmbedding()
                self.ner_tagger = NewsNERTagger(self.emb)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Natasha: {e}")
                self.natasha_available = False
    
    def get_entities(self, text: str, lang: str = 'ru') -> Dict[str, set]:
        """
        –ò–∑–≤–ª–µ—á—å entities –∏–∑ —Ç–µ–∫—Å—Ç–∞ (v5.2.6 —Å –¥–≤—É—Ö—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π)
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            lang: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (ru, uk, en)
            
        Returns:
            {'LOC': set(), 'ORG': set()}
            
        –£–ª—É—á—à–µ–Ω–∏—è v5.2.6:
            - –î–≤—É—Ö—Å—Ç—É–ø–µ–Ω—á–∞—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞:
              –®–ê–ì 1: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ë–ï–ó –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–¥–ª—è "–ª—å–≤–æ–≤")
              –®–ê–ì 2: –° –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π (–¥–ª—è "–∫–∏–µ–≤–µ" ‚Üí "–∫–∏–µ–≤")
            - –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–≥–¥–∞ Pymorphy3 –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç "–ª—å–≤–æ–≤" ‚Üí "–ª–µ–≤"
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∞
        cache_key = f"{text}_{lang}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        text_lower = text.lower()
        entities = {'LOC': set(), 'ORG': set()}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
        words_original = set(re.findall(r'\w+', text_lower))
        
        # –®–ê–ì 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ë–ï–ó –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
        # –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è –≥–æ—Ä–æ–¥–æ–≤ —Ç–∏–ø–∞ "–ª—å–≤–æ–≤" –∫–æ—Ç–æ—Ä—ã–µ Pymorphy3 –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç
        for category, items in self.hard_cache.items():
            intersection = words_original & items
            if intersection:
                entities[category].update(intersection)
        
        # –®–ê–ì 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –° –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π (–¥–ª—è –ø–∞–¥–µ–∂–µ–π)
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –®–ê–ì 1 –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à—ë–ª
        if not any(entities.values()) and self.morph_available and lang in ['ru', 'uk']:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Pymorphy3 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            morph = self.morph_ru if lang == 'ru' else self.morph_uk
            words_normalized = set()
            
            for word in words_original:
                try:
                    parsed = morph.parse(word)
                    if parsed:
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É (–∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂)
                        words_normalized.add(parsed[0].normal_form)
                except:
                    # –ï—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–æ—Å—å - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    words_normalized.add(word)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∂—ë—Å—Ç–∫–∏–π –∫–µ—à —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            for category, items in self.hard_cache.items():
                intersection = words_normalized & items
                if intersection:
                    entities[category].update(intersection)
        
        # –®–ê–ì 3: –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ –∏ —è–∑—ã–∫ = RU –∏ Natasha –¥–æ—Å—Ç—É–ø–Ω–∞ ‚Üí NER
        if not any(entities.values()) and lang == 'ru' and self.natasha_available:
            try:
                doc = Doc(text)
                doc.segment(self.segmenter)
                doc.tag_ner(self.ner_tagger)
                
                for span in doc.spans:
                    if span.type == 'LOC':
                        entities['LOC'].add(span.text.lower())
                    elif span.type == 'ORG':
                        entities['ORG'].add(span.text.lower())
            except Exception as e:
                pass  # NER –º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        self.cache[cache_key] = entities
        return entities
    
    def check_conflict(self, seed: str, keyword: str, lang: str = 'ru') -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç entities
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏:
        - –í seed –µ—Å—Ç—å –≥–æ—Ä–æ–¥ –ê, –∞ –≤ keyword –ø–æ—è–≤–∏–ª—Å—è –≥–æ—Ä–æ–¥ –ë
        - –í seed –µ—Å—Ç—å –±—Ä–µ–Ω–¥ –ê, –∞ –≤ keyword –ø–æ—è–≤–∏–ª—Å—è –±—Ä–µ–Ω–¥ –ë
        
        Args:
            seed: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            keyword: –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            lang: –Ø–∑—ã–∫
            
        Returns:
            True –µ—Å–ª–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç, False –µ—Å–ª–∏ –≤—Å—ë –û–ö
            
        –ü—Ä–∏–º–µ—Ä—ã:
            >>> check_conflict("—Ä–µ–º–æ–Ω—Ç –¥–Ω–µ–ø—Ä", "—Ä–µ–º–æ–Ω—Ç –∫–∏–µ–≤", "ru")
            True  # –ö–æ–Ω—Ñ–ª–∏–∫—Ç: –¥–Ω–µ–ø—Ä != –∫–∏–µ–≤
            
            >>> check_conflict("—Ä–µ–º–æ–Ω—Ç –¥–Ω–µ–ø—Ä", "—Ä–µ–º–æ–Ω—Ç –¥–Ω–µ–ø—Ä –Ω–µ–¥–æ—Ä–æ–≥–æ", "ru")
            False  # –ù–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞: —Ç–æ—Ç –∂–µ –≥–æ—Ä–æ–¥
        """
        seed_entities = self.get_entities(seed, lang)
        kw_entities = self.get_entities(keyword, lang)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–ø entities (LOC, ORG)
        for entity_type in ['LOC', 'ORG']:
            seed_set = seed_entities[entity_type]
            kw_set = kw_entities[entity_type]
            
            # –ï—Å–ª–∏ –≤ seed –µ—Å—Ç—å entity, –∞ –≤ keyword –ø–æ—è–≤–∏–ª—Å—è –î–†–£–ì–û–ô
            if seed_set and (kw_set - seed_set):
                return True  # –ö–û–ù–§–õ–ò–ö–¢!
        
        return False  # –ù–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞


# ============================================
# PARSER CLASS
# ============================================
class GoogleAutocompleteParser:
    def __init__(self):
        self.adaptive_delay = AdaptiveDelay()
        
        # ============================================
        # ENTITY CONFLICT DETECTION (v5.2.4)
        # ============================================
        self.entity_manager = EntityLogicManager()
        
        # ============================================
        # –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–õ–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–ò (v5.2.0)
        # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥: Pymorphy3 (RU/UK) + Snowball (–æ—Å—Ç–∞–ª—å–Ω—ã–µ)
        # ============================================
        
        # Pymorphy3 –¥–ª—è —Ç–æ—á–Ω–æ–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ RU/UK
        self.morph_ru = pymorphy3.MorphAnalyzer(lang='ru')
        self.morph_uk = pymorphy3.MorphAnalyzer(lang='uk')
        
        # Snowball –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–µ–º–º–∏–Ω–≥–∞ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤
        self.stemmers = {
            'en': SnowballStemmer("english"),
            'de': SnowballStemmer("german"),
            'fr': SnowballStemmer("french"),
            'es': SnowballStemmer("spanish"),
            'it': SnowballStemmer("italian"),
        }
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–ø—Ä–µ–¥–ª–æ–≥–∏, —Å–æ—é–∑—ã - –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
        self.stop_words = {
            'ru': {'–∏', '–≤', '–≤–æ', '–Ω–µ', '–Ω–∞', '—Å', '–æ—Ç', '–¥–ª—è', '–ø–æ', '–æ', '–æ–±', '–∫', '—É', '–∑–∞', 
                   '–∏–∑', '—Å–æ', '–¥–æ', '–ø—Ä–∏', '–±–µ–∑', '–Ω–∞–¥', '–ø–æ–¥', '–∞', '–Ω–æ', '–¥–∞', '–∏–ª–∏', '—á—Ç–æ–±—ã', 
                   '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '–ø–æ—á–µ–º—É'},
            'uk': {'—ñ', '–≤', '–Ω–∞', '–∑', '–≤—ñ–¥', '–¥–ª—è', '–ø–æ', '–æ', '–¥–æ', '–ø—Ä–∏', '–±–µ–∑', '–Ω–∞–¥', '–ø—ñ–¥', 
                   '–∞', '–∞–ª–µ', '—Ç–∞', '–∞–±–æ', '—â–æ', '—è–∫', '–¥–µ', '–∫–æ–ª–∏', '–∫—É–¥–∏', '–∑–≤—ñ–¥–∫–∏', '—á–æ–º—É'},
            'en': {'a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 
                   'up', 'about', 'into', 'through', 'during', 'and', 'or', 'but', 'if', 'when', 
                   'where', 'how', 'why', 'what'},
            'de': {'der', 'die', 'das', 'den', 'dem', 'des', 'ein', 'eine', 'einen', 'einem', 
                   'und', 'oder', 'aber', 'in', 'auf', 'von', 'zu', 'mit', 'f√ºr', 'bei', 'nach',
                   'wie', 'wo', 'wann', 'warum', 'was', 'wer'},
            'fr': {'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'et', 'ou', 'mais', 'dans',
                   'sur', 'avec', 'pour', 'par', '√†', 'en', 'au', 'aux', 'ce', 'qui', 'que',
                   'comment', 'o√π', 'quand', 'pourquoi', 'quoi'},
            'es': {'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'de', 'del', 'y', 'o',
                   'pero', 'en', 'con', 'por', 'para', 'a', 'al', 'como', 'que', 'quien',
                   'donde', 'cuando', 'porque', 'qu√©'},
            'it': {'il', 'lo', 'la', 'i', 'gli', 'le', 'un', 'uno', 'una', 'di', 'da', 'e', 'o',
                   'ma', 'in', 'su', 'con', 'per', 'a', 'come', 'che', 'chi', 'dove', 'quando',
                   'perch√©', 'cosa'},
            'pl': {'i', 'w', 'na', 'z', 'do', 'dla', 'po', 'o', 'przy', 'bez', 'nad', 'pod',
                   'a', 'ale', 'lub', 'czy', '≈ºe', 'jak', 'gdzie', 'kiedy', 'dlaczego', 'co'}
        }
    
    # ============================================
    # LANGUAGE & MODIFIERS
    # ============================================
    def detect_seed_language(self, seed: str) -> str:
        """–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ seed"""
        if any('\u0400' <= char <= '\u04FF' for char in seed):
            if any(char in '—ñ—ó—î“ë' for char in seed.lower()):
                return 'uk'
            return 'ru'
        return 'en'
    
    def get_modifiers(self, language: str, use_numbers: bool, seed: str, cyrillic_only: bool = False) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —è–∑—ã–∫–∞ —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        modifiers = []
        
        # –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –∞–Ω–∞–ª–∏–∑ seed –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–ª—Ñ–∞–≤–∏—Ç–∞
        seed_lower = seed.lower()
        has_cyrillic = any('\u0400' <= c <= '\u04FF' for c in seed_lower)
        has_latin = any('a' <= c <= 'z' for c in seed_lower)
        
        # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ (–ë–ï–ó —ä, —å, —ã)
        if language.lower() == 'ru':
            modifiers.extend(list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ç—é—è"))
        elif language.lower() == 'uk':
            modifiers.extend(list("–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—é—è—ñ—ó—î“ë"))
        
        # –õ–∞—Ç–∏–Ω–∏—Ü–∞ - —É–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        if not cyrillic_only:
            # –ï—Å–ª–∏ seed –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ –ò –Ω–µ –∑–∞–¥–∞–Ω –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–∞—Ç–∏–Ω–∏—Ü—É
            if has_cyrillic and not has_latin and language.lower() not in ['en', 'de', 'fr', 'es', 'pl']:
                # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –ª–∞—Ç–∏–Ω–∏—Ü—É –¥–ª—è —á–∏—Å—Ç–æ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö seed
                pass
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –ª–∞—Ç–∏–Ω–∏—Ü—É –µ—Å–ª–∏:
                # - –í seed –µ—Å—Ç—å –ª–∞—Ç–∏–Ω–∏—Ü–∞
                # - –Ø–∑—ã–∫ = –∞–Ω–≥–ª–∏–π—Å–∫–∏–π/–µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–π
                # - Seed –±–µ–∑ –∞–ª—Ñ–∞–≤–∏—Ç–∞ (—Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã/—Ü–∏—Ñ—Ä—ã)
                modifiers.extend(list("abcdefghijklmnopqrstuvwxyz"))
        
        # –¶–∏—Ñ—Ä—ã
        if use_numbers:
            modifiers.extend([str(i) for i in range(10)])
        
        return modifiers
    
    def get_morphological_forms(self, word: str, language: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ pymorphy3"""
        forms = set([word])
        
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                parsed = morph.parse(word)
                
                if parsed:
                    for form in parsed[0].lexeme:
                        pos = form.tag.POS
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–∏—á–∞—Å—Ç–∏—è –∏ –¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏—è
                        if pos not in ['PRTS', 'PRTF', 'GRND']:
                            forms.add(form.word)
            except:
                pass
        
        return sorted(list(forms))
    
    def _normalize_with_pymorphy(self, text: str, language: str) -> set:
        """
        –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ Pymorphy3 –¥–ª—è RU/UK
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            language: 'ru' –∏–ª–∏ 'uk'
            
        Returns:
            set: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –ª–µ–º–º (–Ω–∞—á–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º —Å–ª–æ–≤)
        """
        # –í—ã–±–∏—Ä–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        morph = self.morph_ru if language == 'ru' else self.morph_uk
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = self.stop_words.get(language, self.stop_words['ru'])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
        words = re.findall(r'\w+', text.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        meaningful = [w for w in words if w not in stop_words and len(w) > 1]
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ Pymorphy3
        lemmas = set()
        for word in meaningful:
            try:
                parsed = morph.parse(word)
                if parsed:
                    lemmas.add(parsed[0].normal_form)
            except:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                lemmas.add(word)
        
        return lemmas
    
    def _normalize_with_snowball(self, text: str, language: str) -> set:
        """
        –°—Ç–µ–º–º–∏–Ω–≥ —á–µ—Ä–µ–∑ Snowball –¥–ª—è EN/DE/FR/ES/IT
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            language: –Ø–∑—ã–∫ (en, de, fr, es, it)
            
        Returns:
            set: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç–µ–º–º–æ–≤ (–∫–æ—Ä–Ω–µ–π —Å–ª–æ–≤)
        """
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–µ–º–º–µ—Ä
        stemmer = self.stemmers.get(language, self.stemmers['en'])
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = self.stop_words.get(language, self.stop_words['en'])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
        words = re.findall(r'\w+', text.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        meaningful = [w for w in words if w not in stop_words and len(w) > 1]
        
        # –°—Ç–µ–º–º–∏–Ω–≥
        stems = {stemmer.stem(w) for w in meaningful}
        
        return stems
    
    def _are_words_similar(self, word1: str, word2: str, threshold: float = 0.85) -> bool:
        """
        Fuzzy Matching –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ (RU ‚Üî UK)
        
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ª–æ–≤ –¥–ª–∏–Ω–Ω–µ–µ 5 —Å–∏–º–≤–æ–ª–æ–≤
        
        Args:
            word1: –ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
            word2: –í—Ç–æ—Ä–æ–µ —Å–ª–æ–≤–æ
            threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.85-0.9)
            
        Returns:
            bool: True –µ—Å–ª–∏ —Å–ª–æ–≤–∞ –ø–æ—Ö–æ–∂–∏
            
        –ü—Ä–∏–º–µ—Ä—ã:
            >>> _are_words_similar('–ø–∏–ª–æ—Å–æ—Å', '–ø—ã–ª–µ—Å–æ—Å', 0.85)
            True  # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–π ‚Üî –†—É—Å—Å–∫–∏–π
            
            >>> _are_words_similar('—Ä–µ–º–æ–Ω—Ç', '–¥–µ–º–æ–Ω—Ç', 0.85)
            False  # –†–∞–∑–Ω—ã–µ —Å–ª–æ–≤–∞
        """
        # Fuzzy —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ (–∏–∑–±–µ–≥–∞–µ–º –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π)
        if len(word1) <= 4 or len(word2) <= 4:
            return False
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —á–µ—Ä–µ–∑ SequenceMatcher
        similarity = SequenceMatcher(None, word1, word2).ratio()
        
        return similarity >= threshold
    
    def _normalize(self, text: str, language: str = 'ru') -> set:
        """
        –ì–∏–±—Ä–∏–¥–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (v5.2.0)
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
        - RU/UK: Pymorphy3 (—Ç–æ—á–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)
        - EN/DE/FR/ES/IT: Snowball (–±—ã—Å—Ç—Ä—ã–π —Å—Ç–µ–º–º–∏–Ω–≥)
        - PL: Snowball —Å —Ä—É—Å—Å–∫–∏–º –¥–≤–∏–∂–∫–æ–º (fallback)
        
        –õ–æ–≥–∏–∫–∞ –æ—Ç Gemini AI - –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (ru, uk, en, de, fr, es, it, pl)
            
        Returns:
            set: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º —Å–ª–æ–≤
            
        –ü—Ä–∏–º–µ—Ä—ã:
            >>> _normalize("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –≤ –¥–Ω–µ–ø—Ä–µ", "ru")
            {'—Ä–µ–º–æ–Ω—Ç', '–ø—ã–ª–µ—Å–æ—Å', '–¥–Ω–µ–ø—Ä'}
            
            >>> _normalize("–ø–∏–ª–æ—Å–æ—Å—ñ–≤ –¥–Ω—ñ–ø—Ä–æ", "uk")
            {'–ø–∏–ª–æ—Å–æ—Å', '–¥–Ω—ñ–ø—Ä–æ'}
            
            >>> _normalize("repair vacuum cleaner", "en")
            {'repair', 'vacuum', 'cleaner'}
        """
        
        # –°–¢–†–ê–¢–ï–ì–ò–Ø 1: Pymorphy3 –¥–ª—è RU/UK (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)
        if language in ['ru', 'uk']:
            return self._normalize_with_pymorphy(text, language)
        
        # –°–¢–†–ê–¢–ï–ì–ò–Ø 2: Snowball –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö (—Å—Ç–µ–º–º–∏–Ω–≥)
        elif language in ['en', 'de', 'fr', 'es', 'it']:
            return self._normalize_with_snowball(text, language)
        
        # –°–¢–†–ê–¢–ï–ì–ò–Ø 3: Fallback –¥–ª—è PL –∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö
        else:
            # –î–ª—è –ø–æ–ª—å—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
            # (Pymorphy3 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç, Snowball —Ç–æ–∂–µ)
            words = re.findall(r'\w+', text.lower())
            stop_words = self.stop_words.get('en', set())  # fallback –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ
            meaningful = [w for w in words if w not in stop_words and len(w) > 1]
            return set(meaningful)
    
    def is_grammatically_valid(self, seed_word: str, kw_word: str, language: str = 'ru') -> bool:
        """
        –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ñ–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–≥–∞–º (v5.2.3 - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è Gemini)
        
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–ª–æ–≤ –∏–∑ seed –∏ keyword.
        –û—Ç—Å–µ–∫–∞–µ—Ç "–≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π —à—É–º" - —Ñ–æ—Ä–º—ã –∫–æ—Ç–æ—Ä—ã–µ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ,
        –Ω–æ –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –Ω–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è.
        
        Args:
            seed_word: –°–ª–æ–≤–æ –∏–∑ seed (–æ–±—ã—á–Ω–æ –≤ –ò–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ–º –ø–∞–¥–µ–∂–µ)
            kw_word: –°–ª–æ–≤–æ –∏–∑ keyword (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –ª—é–±–æ–π —Ñ–æ—Ä–º–µ)
            language: –Ø–∑—ã–∫ (–¥–ª—è ru/uk –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Pymorphy3)
            
        Returns:
            bool: True –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞ –¥–æ–ø—É—Å—Ç–∏–º–∞—è, False –µ—Å–ª–∏ —ç—Ç–æ "–≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π —à—É–º"
            
        –ü—Ä–∏–º–µ—Ä—ã:
            >>> is_grammatically_valid('—Ä–µ–º–æ–Ω—Ç', '—Ä–µ–º–æ–Ω—Ç–∞', 'ru')
            True  # –†–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂ - OK
            
            >>> is_grammatically_valid('—Ä–µ–º–æ–Ω—Ç', '—Ä–µ–º–æ–Ω—Ç–∞–º', 'ru')
            False  # –î–∞—Ç–µ–ª—å–Ω—ã–π –º–Ω.—á - –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä!
            
            >>> is_grammatically_valid('—Ä–µ–º–æ–Ω—Ç', '—Ä–µ–º–æ–Ω—Ç–∞–º–∏', 'ru')
            False  # –¢–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –º–Ω.—á - –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä!
        """
        # –î–ª—è —è–∑—ã–∫–æ–≤ –±–µ–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º True
        if language not in ['ru', 'uk']:
            return True
        
        try:
            # –í—ã–±–∏—Ä–∞–µ–º –º–æ—Ä—Ñ–æ–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
            morph = self.morph_ru if language == 'ru' else self.morph_uk
            
            # –ü–∞—Ä—Å–∏–º –æ–±–µ —Ñ–æ—Ä–º—ã
            parsed_seed = morph.parse(seed_word)
            parsed_kw = morph.parse(kw_word)
            
            if not parsed_seed or not parsed_kw:
                return True  # –ï—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–æ—Å—å - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            
            seed_form = parsed_seed[0]
            kw_form = parsed_kw[0]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –≠—Ç–æ –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ —Å–ª–æ–≤–æ?
            if seed_form.normal_form != kw_form.normal_form:
                return True  # –†–∞–∑–Ω—ã–µ —Å–ª–æ–≤–∞ - –Ω–µ –Ω–∞—à–∞ –ø—Ä–æ–±–ª–µ–º–∞
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –ó–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã
            # –°–ø–∏—Å–æ–∫ –ó–ê–ü–†–ï–©–Å–ù–ù–´–• —Ç–µ–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:
            # - datv (–¥–∞—Ç–µ–ª—å–Ω—ã–π): "—Ä–µ–º–æ–Ω—Ç–∞–º"
            # - ablt (—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π): "—Ä–µ–º–æ–Ω—Ç–∞–º–∏"  
            # - loct (–ø—Ä–µ–¥–ª–æ–∂–Ω—ã–π): "—Ä–µ–º–æ–Ω—Ç–∞—Ö"
            # –í–û –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–û–ú –ß–ò–°–õ–ï (plur)
            
            invalid_tags = {'datv', 'ablt', 'loct'}
            
            # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –≤–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —á–∏—Å–ª–µ –ò –≤ –æ–¥–Ω–æ–º –∏–∑ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö –ø–∞–¥–µ–∂–µ–π
            if 'plur' in kw_form.tag and any(tag in kw_form.tag for tag in invalid_tags):
                return False  # –û—Ç—Å–µ–∏–≤–∞–µ–º –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä!
            
            return True  # –§–æ—Ä–º–∞ –¥–æ–ø—É—Å—Ç–∏–º–∞—è
            
        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–±–µ–∑–æ–ø–∞—Å–Ω–µ–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —á–µ–º –æ—Ç—Å–µ—è—Ç—å —Ö–æ—Ä–æ—à–µ–µ)
            return True
    
    def is_query_allowed(self, query: str, seed: str, country: str) -> bool:
        """
        –ü–†–ï-–§–ò–õ–¨–¢–† (v5.2.8): –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å –î–û –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Google
        
        –ë–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã —Å –≥–æ—Ä–æ–¥–∞–º–∏ –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞–Ω –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è:
        1. –ù–µ—Ü–µ–ª–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞ (—É–∫—Ä–∞–∏–Ω—Å–∫–∏–π IP —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ –ú–æ—Å–∫–≤—É)
        2. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å Google (–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
        
        –£–ª—É—á—à–µ–Ω–∏—è v5.2.8:
        - –ë–∞–∑–∞ geonames: 15000+ –≥–æ—Ä–æ–¥–æ–≤ –≤–º–µ—Å—Ç–æ 20 –≤—Ä—É—á–Ω—É—é
        - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è blacklist –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è (Moscow, –ú–æ—Å–∫–≤–∞, Maskva, –ú–°–ö...)
        - –ü–æ–∫—Ä—ã—Ç–∏–µ: ~1500 –≥–æ—Ä–æ–¥–æ–≤ –¥–ª—è UA (–±—ã–ª–æ 20)
        
        –£–º–Ω–∞—è –ª–æ–≥–∏–∫–∞:
        - –ï—Å–ª–∏ –≥–æ—Ä–æ–¥ –≤ seed - —Ä–∞–∑—Ä–µ—à–∞–µ–º (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –°–ê–ú —É–∫–∞–∑–∞–ª)
        - –ï—Å–ª–∏ –≥–æ—Ä–æ–¥ –ù–ï –≤ seed –Ω–æ –≤ query - –±–ª–æ–∫–∏—Ä—É–µ–º
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            seed: –ò—Å—Ö–æ–¥–Ω—ã–π seed (–æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
            country: –ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (ua, ru, by, kz)
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Ä–∞–∑—Ä–µ—à—ë–Ω, False –µ—Å–ª–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω
            
        –ü—Ä–∏–º–µ—Ä—ã:
            >>> is_query_allowed("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –º–æ—Å–∫–≤–∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", "ua")
            False  # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞: "–º–æ—Å–∫–≤–∞" –≤ –±–∞–∑–µ geonames
            
            >>> is_query_allowed("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ szczecin", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤", "ua")
            False  # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞: "szczecin" ‚Üí "—â–µ—Ü–∏–Ω" –≤ –±–∞–∑–µ geonames
            
            >>> is_query_allowed("—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –º–æ—Å–∫–≤–∞ —Ü–µ–Ω–∞", "—Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤ –º–æ—Å–∫–≤–∞", "ua")
            True  # –†–∞–∑—Ä–µ—à–µ–Ω–æ: "–º–æ—Å–∫–≤–∞" –≤ seed (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –°–ê–ú —É–∫–∞–∑–∞–ª)
        """
        blacklist = GEO_BLACKLIST.get(country.lower(), set())  # .lower() –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏!
        
        if not blacklist:
            return True  # –ù–µ—Ç blacklist –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω—ã
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º seed: –µ—Å—Ç—å –ª–∏ —Ç–∞–º –≥–æ—Ä–æ–¥–∞ –∏–∑ blacklist?
        seed_lower = seed.lower()
        seed_has_blocked_city = any(city in seed_lower for city in blacklist)
        
        # –ï—Å–ª–∏ –≤ seed —É–∂–µ –µ—Å—Ç—å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–æ—Ä–æ–¥ - —Ä–∞–∑—Ä–µ—à–∞–µ–º –≤—Å—ë
        # (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –°–ê–ú –∑–∞—Ö–æ—Ç–µ–ª –∏—Å–∫–∞—Ç—å –ø—Ä–æ —ç—Ç–æ—Ç –≥–æ—Ä–æ–¥)
        if seed_has_blocked_city:
            return True
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ query
        query_lower = query.lower()
        query_words = set(re.findall(r'\w+', query_lower))
        
        # –≠–¢–ê–ü 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ë–ï–ó –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–±—ã—Å—Ç—Ä–∞—è)
        for city in blacklist:
            if city in query_words or city in query_lower:
                print(f"üö´ [PRE-FILTER] –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞: '{query}' (—á—É–∂–æ–π –≥–æ—Ä–æ–¥: {city}, —Å—Ç—Ä–∞–Ω–∞: {country})", flush=True)
                return False
        
        # –≠–¢–ê–ü 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –° –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π (–¥–ª—è –ø–∞–¥–µ–∂–µ–π)
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ Pymorphy3
        if hasattr(self, 'morph_ru'):
            normalized_words = set()
            
            for word in query_words:
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—Å—Å–∫–∏–π –º–æ—Ä—Ñ–æ–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—Å–µ—Ö —Å–ª–∞–≤—è–Ω—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤
                    parsed = self.morph_ru.parse(word)
                    if parsed:
                        normalized_words.add(parsed[0].normal_form)
                except:
                    normalized_words.add(word)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
            for city in blacklist:
                if city in normalized_words:
                    print(f"üö´ [PRE-FILTER] –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ (–Ω–æ—Ä–º.): '{query}' (—á—É–∂–æ–π –≥–æ—Ä–æ–¥: {city}, —Å—Ç—Ä–∞–Ω–∞: {country})", flush=True)
                    return False
        
        return True
    
    # ============================================
    # AUTOCORRECTION
    # ============================================
    async def autocorrect_text(self, text: str, language: str) -> Dict:
        """–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ Yandex Speller (ru/uk/en) –∏–ª–∏ LanguageTool (–æ—Å—Ç–∞–ª—å–Ω—ã–µ)"""
        
        # Yandex Speller –¥–ª—è ru/uk/en
        if language.lower() in ['ru', 'uk', 'en']:
            url = "https://speller.yandex.net/services/spellservice.json/checkText"
            lang_map = {'ru': 'ru', 'uk': 'uk', 'en': 'en'}
            yandex_lang = lang_map.get(language.lower(), 'ru')
            
            params = {"text": text, "lang": yandex_lang, "options": 0}
            
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get(url, params=params)
                    
                    if response.status_code == 200:
                        errors = response.json()
                        
                        if not errors:
                            return {"original": text, "corrected": text, "corrections": [], "has_errors": False}
                        
                        corrected = text
                        corrections = []
                        errors_sorted = sorted(errors, key=lambda x: x.get('pos', 0), reverse=True)
                        
                        for error in errors_sorted:
                            word = error.get('word', '')
                            suggestions = error.get('s', [])
                            
                            if suggestions:
                                suggestion = suggestions[0]
                                pos = error.get('pos', 0)
                                corrected = corrected[:pos] + suggestion + corrected[pos + len(word):]
                                corrections.append({"word": word, "suggestion": suggestion})
                        
                        return {
                            "original": text,
                            "corrected": corrected,
                            "corrections": corrections,
                            "has_errors": True
                        }
            except:
                pass
        
        # LanguageTool fallback –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤
        return await self.autocorrect_languagetool(text, language)
    
    async def autocorrect_languagetool(self, text: str, language: str) -> Dict:
        """–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ LanguageTool API (30+ —è–∑—ã–∫–æ–≤)"""
        url = "https://api.languagetool.org/v2/check"
        
        data = {
            "text": text,
            "language": language.lower(),
            "enabledOnly": "false"
        }
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(url, data=data)
                
                if response.status_code == 200:
                    result = response.json()
                    matches = result.get('matches', [])
                    
                    if not matches:
                        return {"original": text, "corrected": text, "corrections": [], "has_errors": False}
                    
                    corrected = text
                    corrections = []
                    
                    for match in reversed(matches):
                        offset = match.get('offset', 0)
                        length = match.get('length', 0)
                        replacements = match.get('replacements', [])
                        
                        if replacements:
                            suggestion = replacements[0].get('value', '')
                            word = text[offset:offset+length]
                            corrected = corrected[:offset] + suggestion + corrected[offset+length:]
                            corrections.append({"word": word, "suggestion": suggestion})
                    
                    return {
                        "original": text,
                        "corrected": corrected,
                        "corrections": corrections,
                        "has_errors": True
                    }
        except:
            pass
        
        return {"original": text, "corrected": text, "corrections": [], "has_errors": False}
    
    # ============================================
    # FILTERS
    # ============================================
    async def filter_infix_results(self, keywords: List[str], language: str) -> List[str]:
        """–§–∏–ª—å—Ç—Ä INFIX —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: —É–±–∏—Ä–∞–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ –æ–¥–∏–Ω–æ—á–Ω—ã–µ –±—É–∫–≤—ã"""
        
        # Whitelist –ø—Ä–µ–¥–ª–æ–≥–æ–≤/—Å–æ—é–∑–æ–≤
        if language.lower() == 'ru':
            valid = {'–≤', '–Ω–∞', '—É', '–∫', '–æ—Ç', '–∏–∑', '–ø–æ', '–æ', '–æ–±', '—Å', '—Å–æ', '–∑–∞', '–¥–ª—è', '–∏', '–∞', '–Ω–æ'}
        elif language.lower() == 'uk':
            valid = {'–≤', '–Ω–∞', '—É', '–¥–æ', '–≤—ñ–¥', '–∑', '–ø–æ', '–ø—Ä–æ', '–¥–ª—è', '—ñ', '—Ç–∞', '–∞–±–æ'}
        elif language.lower() == 'en':
            valid = {'in', 'on', 'at', 'to', 'from', 'with', 'for', 'by', 'of', 'and', 'or', 'a', 'i'}
        else:
            valid = set()
        
        filtered = []
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            words = keyword_lower.split()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –í–°–ï —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ
            has_garbage = False
            for i in range(1, len(words)):
                word = words[i]
                if len(word) == 1 and word not in valid:
                    has_garbage = True
                    break
            
            if not has_garbage:
                filtered.append(keyword)
        
        return filtered
    
    async def filter_relevant_keywords(self, keywords: List[str], seed: str, language: str = 'ru') -> List[str]:
        """
        SUBSET MATCHING v5.2.3: –¢—Ä—ë—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (—É–ª—É—á—à–µ–Ω–æ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Ç–µ—Å—Ç–∞)
        
        –°–∏—Ç–æ 1 (–õ–µ–º–º—ã): –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–º—ã—Å–ª - –ø—Ä–æ –ø—ã–ª–µ—Å–æ—Å—ã –∏–ª–∏ –ø—Ä–æ —É—Ç—é–≥–∏
        –°–∏—Ç–æ 2 (–û—Ä–∏–≥–∏–Ω–∞–ª—ã + –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è): –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å - –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–∏ —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞
        –°–∏—Ç–æ 3 (–ü–æ—Ä—è–¥–æ–∫): –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ seed - —ç—Ç–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å–ª–æ–≤–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        
        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
        - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (_normalize)
        - –¢—Ä–µ–±—É–µ—Ç –í–°–ï –ª–µ–º–º—ã seed –≤ keyword (subset matching)
        - –¢—Ä–µ–±—É–µ—Ç 100% –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ seed –≤ keyword (—Å—Ç—Ä–æ–∂–µ —á–µ–º 70%)
        - –¢—Ä–µ–±—É–µ—Ç —á—Ç–æ seed –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–∞–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –ò–õ–ò —Å–ª–æ–≤–∞ –∏–¥—É—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        """
        
        # 1. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º seed (–ª–µ–º–º—ã –¥–ª—è —Å–º—ã—Å–ª–æ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏)
        seed_lemmas = self._normalize(seed, language)
        
        if not seed_lemmas:
            return keywords
        
        # 2. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ seed (–¥–ª—è —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏)
        seed_lower = seed.lower()
        seed_words_original = [w.lower() for w in re.findall(r'\w+', seed) if len(w) > 2]
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —è–∑—ã–∫–∞
        stop_words = self.stop_words.get(language, self.stop_words['ru'])
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        seed_important_words = [w for w in seed_words_original if w not in stop_words]
        
        if not seed_important_words:
            # –ï—Å–ª–∏ –≤—Å–µ —Å–ª–æ–≤–∞ - —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –ø–æ–ª–∞–≥–∞–µ–º—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ª–µ–º–º—ã
            seed_important_words = seed_words_original
        
        filtered = []
        
        for keyword in keywords:
            kw_lower = keyword.lower()
            
            # –ü–†–û–í–ï–†–ö–ê 1: –õ–µ–º–º—ã (Subset Matching - —Å–º—ã—Å–ª)
            kw_lemmas = self._normalize(keyword, language)
            if not seed_lemmas.issubset(kw_lemmas):
                continue  # –ù–µ –ø—Ä–æ —Ç–æ - –æ—Ç—Å–µ–∏–≤–∞–µ–º
            
            # –ü–†–û–í–ï–†–ö–ê 2: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã (–∑–∞—â–∏—Ç–∞ –æ—Ç "—Ä–µ–º–æ–Ω—Ç–∞—Ö" - —Å–∏–Ω—Ç–∞–∫—Å–∏—Å)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–ª–æ–≤ –∏–∑ seed –≤ —Å–ª–æ–≤–∞ keyword
            kw_words = kw_lower.split()
            matches = 0
            grammatically_valid = True
            
            for seed_word in seed_important_words:
                found_match = False
                
                for kw_word in kw_words:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                    if seed_word in kw_word:
                        # –ü–†–û–í–ï–†–ö–ê 2.5: –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å (v5.2.3 - Gemini)
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ "–≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä"
                        if self.is_grammatically_valid(seed_word, kw_word, language):
                            found_match = True
                            break
                        else:
                            # –§–æ—Ä–º–∞ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è (–Ω–∞–ø—Ä–∏–º–µ—Ä "—Ä–µ–º–æ–Ω—Ç–∞–º")
                            grammatically_valid = False
                            break
                
                if found_match:
                    matches += 1
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ñ–æ—Ä–º—É - –æ—Ç—Å–µ–∏–≤–∞–µ–º
            if not grammatically_valid:
                continue
            
            # –¢—Ä–µ–±—É–µ–º 100% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤ (—Å—Ç—Ä–æ–∂–µ!)
            if len(seed_important_words) > 0:
                match_ratio = matches / len(seed_important_words)
                if match_ratio < 1.0:  # –ï—Å–ª–∏ –ù–ï 100% - –æ—Ç—Å–µ–∏–≤–∞–µ–º
                    continue
            
            # –ü–†–û–í–ï–†–ö–ê 3: –ü–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤ + –ø–æ–∑–∏—Ü–∏—è seed
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ seed –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ keyword (–∏–ª–∏ –±–ª–∏–∑–∫–æ –∫ –Ω–∞—á–∞–ª—É)
            # –ò –≤—Å–µ —Å–ª–æ–≤–∞ –∏–¥—É—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            
            first_seed_word = seed_important_words[0]
            first_word_position = -1
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –ø–µ—Ä–≤–æ–≥–æ –≤–∞–∂–Ω–æ–≥–æ —Å–ª–æ–≤–∞ seed –≤ keyword
            for i, kw_word in enumerate(kw_words):
                if first_seed_word in kw_word:
                    first_word_position = i
                    break
            
            # –ï—Å–ª–∏ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ seed –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ –æ—Ç –Ω–∞—á–∞–ª–∞ - –æ—Ç—Å–µ–∏–≤–∞–µ–º
            # –î–æ–ø—É—Å–∫–∞–µ–º –º–∞–∫—Å–∏–º—É–º 1 —Å–ª–æ–≤–æ –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –≤–∞–∂–Ω—ã–º —Å–ª–æ–≤–æ–º seed
            if first_word_position > 1:
                continue  # "–¥–æ–º—É —Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤" ‚Üí –ø–æ–∑–∏—Ü–∏—è 1, –Ω–æ —ç—Ç–æ —É–∂–µ –º—É—Å–æ—Ä
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ seed –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ keyword –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            last_index = -1
            order_correct = True
            
            for seed_word in seed_important_words:
                # –ò—â–µ–º —ç—Ç–æ —Å–ª–æ–≤–æ –≤ keyword –ø–æ—Å–ª–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ
                found_at = -1
                for i, kw_word in enumerate(kw_words):
                    if i > last_index and seed_word in kw_word:
                        found_at = i
                        break
                
                if found_at == -1:
                    order_correct = False
                    break
                
                last_index = found_at
            
            if order_correct:
                filtered.append(keyword)
        
        # ============================================
        # –≠–¢–ê–ü 2: ENTITY CONFLICTS (v5.2.4)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –≥–æ—Ä–æ–¥–æ–≤/–±—Ä–µ–Ω–¥–æ–≤
        # ============================================
        filtered_final = []
        
        for keyword in filtered:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Ç.–∫. Natasha —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è
            # (asyncio.to_thread –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop)
            is_conflict = await asyncio.to_thread(
                self.entity_manager.check_conflict,
                seed,
                keyword,
                language
            )
            
            if not is_conflict:
                filtered_final.append(keyword)
        
        return filtered_final
    
    # ============================================
    # FETCH SUGGESTIONS (3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
    # ============================================
    async def fetch_suggestions(self, query: str, country: str, language: str, client: httpx.AsyncClient) -> List[str]:
        """Google Autocomplete"""
        url = "https://www.google.com/complete/search"
        params = {"q": query, "client": "firefox", "hl": language, "gl": country}
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                return data[1] if len(data) > 1 else []
        except:
            pass
        
        return []
    
    async def fetch_suggestions_yandex(self, query: str, language: str, region_id: int, client: httpx.AsyncClient) -> List[str]:
        """Yandex Suggest"""
        url = "https://suggest-maps.yandex.ru/suggest-geo"
        
        params = {
            "v": "9",
            "search_type": "tp",
            "part": query,
            "lang": language,
            "n": "10",
            "geo": str(region_id),
            "fullpath": "1"
        }
        
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ rate limit
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            # –£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                results = data.get('results', [])
                return [item.get('text', '') for item in results if item.get('text')]
        except:
            pass
        
        return []
    
    async def fetch_suggestions_bing(self, query: str, language: str, country: str, client: httpx.AsyncClient) -> List[str]:
        """Bing Autosuggest"""
        url = "https://www.bing.com/AS/Suggestions"
        
        params = {
            "q": query,
            "mkt": f"{language}-{country}",
            "cvid": "0",
            "qry": query
        }
        
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            response = await client.get(url, params=params, headers=headers, timeout=10.0)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ rate limit
            if response.status_code == 429:
                self.adaptive_delay.on_rate_limit()
                return []
            
            # –£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            self.adaptive_delay.on_success()
            
            if response.status_code == 200:
                data = response.json()
                suggestion_groups = data.get('AS', {}).get('Results', [])
                
                suggestions = []
                for group in suggestion_groups:
                    for item in group.get('Suggests', []):
                        text = item.get('Txt', '')
                        if text:
                            suggestions.append(text)
                
                return suggestions
        except:
            pass
        
        return []
    
    # ============================================
    # PARSING WITH SEMAPHORE
    # ============================================
    async def parse_with_semaphore(self, queries: List[str], country: str, language: str, 
                                   parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤—ã–±–æ—Ä–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        
        semaphore = asyncio.Semaphore(parallel_limit)
        all_keywords = set()
        success_count = 0
        failed_count = 0
        
        async def fetch_with_limit(query: str, client: httpx.AsyncClient):
            nonlocal success_count, failed_count
            
            async with semaphore:
                await asyncio.sleep(self.adaptive_delay.get_delay())
                
                # –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                if source == "google":
                    results = await self.fetch_suggestions(query, country, language, client)
                elif source == "yandex":
                    results = await self.fetch_suggestions_yandex(query, language, region_id, client)
                elif source == "bing":
                    results = await self.fetch_suggestions_bing(query, language, country, client)
                else:
                    results = []
                
                if results:
                    all_keywords.update(results)
                    success_count += 1
                else:
                    failed_count += 1
                
                return results
        
        async with httpx.AsyncClient() as client:
            tasks = [fetch_with_limit(q, client) for q in queries]
            await asyncio.gather(*tasks)
        
        return {
            "keywords": sorted(list(all_keywords)),
            "success": success_count,
            "failed": failed_count
        }
    
    # ============================================
    # SUFFIX METHOD
    # ============================================
    async def parse_suffix(self, seed: str, country: str, language: str, use_numbers: bool, 
                          parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """SUFFIX –º–µ—Ç–æ–¥: seed + –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä"""
        start_time = time.time()
        
        modifiers = self.get_modifiers(language, use_numbers, seed)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)
        
        # –§–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (v5.2.0: subset matching)
        filtered = await self.filter_relevant_keywords(result_raw['keywords'], seed, language)
        
        elapsed = time.time() - start_time
        
        return {
            "seed": seed,
            "method": "suffix",
            "source": source,
            "keywords": filtered,
            "count": len(filtered),
            "queries": len(queries),
            "elapsed_time": round(elapsed, 2)
        }
    
    # ============================================
    # INFIX METHOD
    # ============================================
    async def parse_infix(self, seed: str, country: str, language: str, use_numbers: bool, 
                         parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """INFIX –º–µ—Ç–æ–¥: –≤—Å—Ç–∞–≤–∫–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏"""
        start_time = time.time()
        
        words = seed.strip().split()
        
        if len(words) < 2:
            return {"error": "INFIX —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞", "seed": seed}
        
        modifiers = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
        queries = []
        
        for i in range(1, len(words)):
            for mod in modifiers:
                query = ' '.join(words[:i]) + f' {mod} ' + ' '.join(words[i:])
                queries.append(query)
        
        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)
        
        # –§–∏–ª—å—Ç—Ä 1: –æ–¥–∏–Ω–æ—á–Ω—ã–µ –±—É–∫–≤—ã
        filtered_1 = await self.filter_infix_results(result_raw['keywords'], language)
        
        # –§–∏–ª—å—Ç—Ä 2: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (v5.2.0: subset matching)
        filtered_2 = await self.filter_relevant_keywords(filtered_1, seed, language)
        
        elapsed = time.time() - start_time
        
        return {
            "seed": seed,
            "method": "infix",
            "source": source,
            "keywords": filtered_2,
            "count": len(filtered_2),
            "queries": len(queries),
            "elapsed_time": round(elapsed, 2)
        }
    
    # ============================================
    # MORPHOLOGY METHOD
    # ============================================
    async def parse_morphology(self, seed: str, country: str, language: str, use_numbers: bool, 
                               parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """MORPHOLOGY –º–µ—Ç–æ–¥: –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–æ—Ä–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö"""
        start_time = time.time()
        
        words = seed.strip().split()
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        nouns_to_modify = []
        
        if language.lower() in ['ru', 'uk']:
            try:
                import pymorphy3
                morph = pymorphy3.MorphAnalyzer()
                for idx, word in enumerate(words):
                    parsed = morph.parse(word)
                    if parsed and parsed[0].tag.POS == 'NOUN':
                        nouns_to_modify.append({
                            'index': idx,
                            'word': word,
                            'forms': self.get_morphological_forms(word, language)
                        })
                
                if not nouns_to_modify:
                    last_word = words[-1]
                    nouns_to_modify.append({
                        'index': len(words) - 1,
                        'word': last_word,
                        'forms': self.get_morphological_forms(last_word, language)
                    })
            except:
                last_word = words[-1]
                nouns_to_modify.append({
                    'index': len(words) - 1,
                    'word': last_word,
                    'forms': self.get_morphological_forms(last_word, language)
                })
        else:
            last_word = words[-1]
            nouns_to_modify.append({
                'index': len(words) - 1,
                'word': last_word,
                'forms': self.get_morphological_forms(last_word, language)
            })
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã seed
        all_seeds = []
        if len(nouns_to_modify) >= 1:
            noun = nouns_to_modify[0]
            for form in noun['forms']:
                new_words = words.copy()
                new_words[noun['index']] = form
                all_seeds.append(' '.join(new_words))
        
        unique_seeds = list(set(all_seeds))
        
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Ñ–æ—Ä–º—ã
        all_keywords = set()
        modifiers = self.get_modifiers(language, use_numbers, seed)
        
        for seed_variant in unique_seeds:
            queries = [f"{seed_variant} {mod}" for mod in modifiers]
            result = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)
            all_keywords.update(result['keywords'])
        
        # –§–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        filtered = await self.filter_relevant_keywords(sorted(list(all_keywords)), seed, language)
        
        elapsed = time.time() - start_time
        
        return {
            "seed": seed,
            "method": "morphology",
            "source": source,
            "keywords": filtered,
            "count": len(filtered),
            "elapsed_time": round(elapsed, 2)
        }
    
    # ============================================
    # LIGHT SEARCH (SUFFIX + INFIX)
    # ============================================
    async def parse_light_search(self, seed: str, country: str, language: str, use_numbers: bool, 
                                 parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """LIGHT SEARCH: –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ (SUFFIX + INFIX)"""
        start_time = time.time()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º SUFFIX –∏ INFIX –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        suffix_result = await self.parse_suffix(seed, country, language, use_numbers, parallel_limit, source, region_id)
        infix_result = await self.parse_infix(seed, country, language, use_numbers, parallel_limit, source, region_id)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_keywords = set(suffix_result["keywords"]) | set(infix_result.get("keywords", []))
        
        elapsed = time.time() - start_time
        
        return {
            "seed": seed,
            "method": "light_search",
            "source": source,
            "keywords": sorted(list(all_keywords)),
            "count": len(all_keywords),
            "suffix_count": len(suffix_result["keywords"]),
            "infix_count": len(infix_result.get("keywords", [])),
            "elapsed_time": round(elapsed, 2)
        }
    
    # ============================================
    # ADAPTIVE PREFIX METHOD
    # ============================================
    async def parse_adaptive_prefix(self, seed: str, country: str, language: str, use_numbers: bool, 
                                    parallel_limit: int, source: str = "google", region_id: int = 0) -> Dict:
        """ADAPTIVE PREFIX –º–µ—Ç–æ–¥: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ –∏–∑ SUFFIX + PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞"""
        start_time = time.time()
        
        seed_words = set(seed.lower().split())
        
        # –≠–¢–ê–ü 1: SUFFIX –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü—É (PREFIX —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π)
        modifiers = self.get_modifiers(language, use_numbers, seed, cyrillic_only=True)
        queries = [f"{seed} {mod}" for mod in modifiers]
        
        # –ü–†–ï-–§–ò–õ–¨–¢–† (v5.2.6): –ë–ª–æ–∫–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å —á—É–∂–∏–º–∏ –≥–æ—Ä–æ–¥–∞–º–∏
        queries = [q for q in queries if self.is_query_allowed(q, seed, country)]
        
        result_raw = await self.parse_with_semaphore(queries, country, language, parallel_limit, source, region_id)
        
        # –≠–¢–ê–ü 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• –Ω–æ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        from collections import Counter
        word_counter = Counter()
        
        for result in result_raw['keywords']:
            result_words = result.lower().split()
            for word in result_words:
                # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –∏–∑ seed –∏ –¥–ª–∏–Ω–∞ > 2
                if word not in seed_words and len(word) > 2:
                    word_counter[word] += 1
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: —Å–ª–æ–≤–∞ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è ‚â•2 —Ä–∞–∑–∞
        candidates = {w for w, count in word_counter.items() if count >= 2}
        
        # –≠–¢–ê–ü 3: PREFIX –ø—Ä–æ–≤–µ—Ä–∫–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ–º "{—Å–ª–æ–≤–æ} {seed}"
        all_keywords = set()
        verified_prefixes = []
        
        for candidate in sorted(candidates):
            query = f"{candidate} {seed}"
            
            # –ü–†–ï-–§–ò–õ–¨–¢–† (v5.2.6): –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π
            if not self.is_query_allowed(query, seed, country):
                continue
            
            result = await self.parse_with_semaphore([query], country, language, parallel_limit, source, region_id)
            if result['keywords']:
                all_keywords.update(result['keywords'])
                verified_prefixes.append(candidate)
        
        # –§–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        filtered = await self.filter_relevant_keywords(sorted(list(all_keywords)), seed, language)
        
        elapsed = time.time() - start_time
        
        return {
            "seed": seed,
            "method": "adaptive_prefix",
            "source": source,
            "keywords": filtered,
            "count": len(filtered),
            "candidates_found": len(candidates),
            "verified_prefixes": verified_prefixes,
            "elapsed_time": round(elapsed, 2)
        }
    
    # ============================================
    # DEEP SEARCH (–í–°–ï –ú–ï–¢–û–î–´)
    # ============================================
    async def parse_deep_search(self, seed: str, country: str, region_id: int, language: str, 
                                use_numbers: bool, parallel_limit: int, include_keywords: bool, 
                                source: str = "google") -> Dict:
        """DEEP SEARCH: –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ (–≤—Å–µ 4 –º–µ—Ç–æ–¥–∞)"""
        
        # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è seed
        correction = await self.autocorrect_text(seed, language)
        original_seed = seed
        
        if correction.get("has_errors"):
            seed = correction["corrected"]
        
        start_time = time.time()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ 4 –º–µ—Ç–æ–¥–∞
        suffix_result = await self.parse_suffix(seed, country, language, use_numbers, parallel_limit, source, region_id)
        infix_result = await self.parse_infix(seed, country, language, use_numbers, parallel_limit, source, region_id)
        morph_result = await self.parse_morphology(seed, country, language, use_numbers, parallel_limit, source, region_id)
        prefix_result = await self.parse_adaptive_prefix(seed, country, language, use_numbers, parallel_limit, source, region_id)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        suffix_kw = set(suffix_result["keywords"])
        infix_kw = set(infix_result.get("keywords", []))
        morph_kw = set(morph_result["keywords"])
        prefix_kw = set(prefix_result["keywords"])
        
        all_unique = suffix_kw | infix_kw | morph_kw | prefix_kw
        
        elapsed = time.time() - start_time
        
        response = {
            "seed": original_seed,
            "corrected_seed": seed if correction.get("has_errors") else None,
            "corrections": correction.get("corrections", []) if correction.get("has_errors") else [],
            "source": source,
            "total_unique_keywords": len(all_unique),
            "methods": {
                "suffix": {"count": len(suffix_kw)},
                "infix": {"count": len(infix_kw)},
                "morphology": {"count": len(morph_kw)},
                "adaptive_prefix": {"count": len(prefix_kw)}
            },
            "elapsed_time": round(elapsed, 2)
        }
        
        if include_keywords:
            response["keywords"] = {
                "all": sorted(list(all_unique)),
                "suffix": sorted(list(suffix_kw)),
                "infix": sorted(list(infix_kw)),
                "morphology": sorted(list(morph_kw)),
                "adaptive_prefix": sorted(list(prefix_kw))
            }
        
        return response


# ============================================
# API ENDPOINTS
# ============================================
parser = GoogleAutocompleteParser()

@app.get("/")
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return FileResponse('static/index.html')

@app.get("/api/light-search")
async def light_search_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """LIGHT SEARCH: –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ (SUFFIX + INFIX)"""
    
    if language == "auto":
        language = parser.detect_seed_language(seed)
    
    # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è
    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]
    
    result = await parser.parse_light_search(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])
    
    return result

@app.get("/api/deep-search")
async def deep_search_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (ua/us/de...)"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex (143=–ö–∏–µ–≤)"),
    language: str = Query("auto", description="–Ø–∑—ã–∫ (auto/ru/uk/en)"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    include_keywords: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–π"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """DEEP SEARCH: –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ (–≤—Å–µ 4 –º–µ—Ç–æ–¥–∞)"""
    
    if language == "auto":
        language = parser.detect_seed_language(seed)
    
    return await parser.parse_deep_search(seed, country, region_id, language, use_numbers, parallel_limit, include_keywords, source)

# –û—Å—Ç–∞–≤–ª—è—é —Å—Ç–∞—Ä—ã–π endpoint –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
@app.get("/api/compare")
async def compare_methods(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (ua/us/de...)"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex (143=–ö–∏–µ–≤)"),
    language: str = Query("auto", description="–Ø–∑—ã–∫ (auto/ru/uk/en)"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã 0-9"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    include_keywords: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–π"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """[DEPRECATED] –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /api/deep-search"""
    
    if language == "auto":
        language = parser.detect_seed_language(seed)
    
    return await parser.parse_deep_search(seed, country, region_id, language, use_numbers, parallel_limit, include_keywords, source)

@app.get("/api/parse/suffix")
async def parse_suffix_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """–¢–æ–ª—å–∫–æ SUFFIX –º–µ—Ç–æ–¥"""
    
    if language == "auto":
        language = parser.detect_seed_language(seed)
    
    # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è
    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]
    
    result = await parser.parse_suffix(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])
    
    return result

@app.get("/api/parse/infix")
async def parse_infix_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞)"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """–¢–æ–ª—å–∫–æ INFIX –º–µ—Ç–æ–¥"""
    
    if language == "auto":
        language = parser.detect_seed_language(seed)
    
    # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è
    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]
    
    result = await parser.parse_infix(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])
    
    return result

@app.get("/api/parse/morphology")
async def parse_morphology_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """–¢–æ–ª—å–∫–æ MORPHOLOGY –º–µ—Ç–æ–¥"""
    
    if language == "auto":
        language = parser.detect_seed_language(seed)
    
    # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è
    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]
    
    result = await parser.parse_morphology(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])
    
    return result

@app.get("/api/parse/adaptive-prefix")
async def parse_adaptive_prefix_endpoint(
    seed: str = Query(..., description="–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"),
    country: str = Query("ua", description="–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã"),
    region_id: int = Query(143, description="ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è Yandex"),
    language: str = Query("auto", description="–Ø–∑—ã–∫"),
    use_numbers: bool = Query(False, description="–î–æ–±–∞–≤–∏—Ç—å —Ü–∏—Ñ—Ä—ã"),
    parallel_limit: int = Query(10, description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"),
    source: str = Query("google", description="–ò—Å—Ç–æ—á–Ω–∏–∫: google/yandex/bing")
):
    """ADAPTIVE PREFIX –º–µ—Ç–æ–¥ (–Ω–∞—Ö–æ–¥–∏—Ç PREFIX –∑–∞–ø—Ä–æ—Å—ã —Ç–∏–ø–∞ '–∫–∏–µ–≤ —Ä–µ–º–æ–Ω—Ç –ø—ã–ª–µ—Å–æ—Å–æ–≤')"""
    
    if language == "auto":
        language = parser.detect_seed_language(seed)
    
    # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è
    correction = await parser.autocorrect_text(seed, language)
    if correction.get("has_errors"):
        seed = correction["corrected"]
    
    result = await parser.parse_adaptive_prefix(seed, country, language, use_numbers, parallel_limit, source, region_id)
    
    if correction.get("has_errors"):
        result["original_seed"] = correction["original"]
        result["corrections"] = correction.get("corrections", [])
    
    return result
